{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "189bf2d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<!DOCTYPE html>\n",
      "\n",
      "<html dir=\"ltr\" lang=\"en\">\n",
      "<head>\n",
      "<meta charset=\"utf-8\"/>\n",
      "<meta content=\"IE=edge\" http-equiv=\"X-UA-Compatible\"/>\n",
      "<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n",
      "<meta content=\"5fPGCLllnWrvFxH9QWI0l1TadV7byeEvfPcyK2VkS_s\" name=\"google-site-verification\">\n",
      "<meta content=\"Rp5zp04IKW-s1IbpTOGB7Z6XY60oloZD5C3kTM-AiY4\" name=\"google-site-verification\">\n",
      "<meta content=\"umenay8zh4kswbi568zqp19bqb-jvngusibub1ygib0x3jne9rig0fnmtofm8abb7lkzgltqp5yhm68s5qz4iqqkm39xl2o-p5foixd-1xfq4yig07ugcd1sp5kmyvpe\" name=\"norton-safeweb-site-verification\">\n",
      "<title>SONYC Urban Sound Tagging (SONYC-UST): a multilabel dataset from an urban acoustic sensor network | Zenodo</title>\n",
      "<link href=\"/static/favicon.ico\" rel=\"shortcut icon\"/>\n",
      "<link href=\"/static/apple-touch-icon-144-precomposed.png\" rel=\"apple-touch-icon-precomposed\" sizes=\"144x144\"/>\n",
      "<link href=\"/static/apple-touch-icon-114-precomposed.png\" rel=\"apple-touch-icon-precomposed\" sizes=\"114x114\"/>\n",
      "<link href=\"/static/apple-touch-icon-72-precomposed.png\" rel=\"apple-touch-icon-precomposed\" sizes=\"72x72\"/>\n",
      "<link href=\"/static/apple-touch-icon-57-precomposed.png\" rel=\"apple-touch-icon-precomposed\" sizes=\"57x57\"/>\n",
      "<link href=\"https://fonts.googleapis.com/css?family=Roboto:300,400,500,700,100,italic\" rel=\"stylesheet\"/>\n",
      "<link href=\"/static/gen/zenodo.5fb6164a.css\" rel=\"stylesheet\"/>\n",
      "<!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->\n",
      "<!--[if lt IE 9]>\n",
      "  <script src=\"https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js\"></script>\n",
      "  <script src=\"https://oss.maxcdn.com/respond/1.4.2/respond.min.js\"></script>\n",
      "<![endif]-->\n",
      "<meta content='SONYC Urban Sound Tagging (SONYC-UST): a multilabel dataset from an urban acoustic sensor network Version 2.3, September 2020   Created by Mark Cartwright (1,2,3), Jason Cramer (1), Ana Elisa Mendez Mendez (1), Yu Wang (1), Ho-Hsiang Wu (1), Vincent Lostanlen (1,2,4), Magdalena Fuentes (1), Graham Dove (2), Charlie Mydlarz (1,2), Justin Salamon (5), Oded Nov (6), Juan Pablo Bello (1,2,3) Music and Audio Research Lab, New York University Center for Urban Science and Progress, New York University Department of Computer Science and Engineering, New York University Cornell Lab of Ornithology Adobe Research Department of Technology Management and Innovation, New York University   Publication If using this data in an academic work, please reference the DOI and version, as well as cite the following paper, which presented the data collection procedure and the first version of the dataset: Cartwright, M., Cramer, J., Mendez, A.E.M., Wang, Y., Wu, H., Lostanlen, V., Fuentes, M., Dove, G., Mydlarz, C., Salamon, J., Nov, O., Bello, J.P. SONYC-UST-V2: An Urban Sound Tagging Dataset with Spatiotemporal Context. In Proceedings of the Workshop on Detection and Classification of Acoustic Scenes and Events (DCASE), 2020. [pdf]   Description SONYC Urban Sound Tagging (SONYC-UST) is a dataset for the development and evaluation of machine listening systems for realistic urban noise monitoring. The audio was recorded from the SONYC acoustic sensor network. Volunteers on the  Zooniverse citizen science platform tagged the presence of 23 classes that were chosen in consultation with the New York City Department of Environmental Protection. These 23 fine-grained classes can be grouped into 8 coarse-grained classes. The recordings are split into three sets: training, validation, and test. The training and validation sets are disjoint with respect to the sensor from which each recording came, and the test set is displaced in time. For increased reliability, three volunteers annotated each recording. In addition, members of the SONYC team subsequently created a subset of verified, ground-truth tags using a two-stage annotation procedure in which two annotators independently tagged and then collectively resolved any disagreements. This subset of recordings with verified annotations intersects with all three recording splits. All of the recordings in the test set have these verified annotations.  In v2 version of this dataset, we have also included coarse spatiotemporal context information to aid in tag prediction when time and location is known. For more details on the motivation and creation of this dataset see the DCASE 2020 Urban Sound Tagging with Spatiotemporal Context Task website.   Audio data The provided audio has been acquired using the SONYC acoustic sensor network for urban noise pollution monitoring. Over 60 different sensors have been deployed in New York City, and these sensors have collectively gathered the equivalent of over 50 years of audio data, of which we provide a small subset. The data was sampled by selecting the nearest neighbors on VGGish features of recordings known to have classes of interest. All recordings are 10 seconds and were recorded with identical microphones at identical gain settings. To maintain privacy, we quantized the spatial information to the level of a city block, and we quantized the temporal information to the level of an hour. We also limited the occurrence of recordings with positive human voice annotations to one per hour per sensor.   Label taxonomy The label taxonomy is as follows: engine 1: small-sounding-engine 2: medium-sounding-engine 3: large-sounding-engine X: engine-of-uncertain-size machinery-impact 1: rock-drill 2: jackhammer 3: hoe-ram 4: pile-driver X: other-unknown-impact-machinery non-machinery-impact 1: non-machinery-impact powered-saw 1: chainsaw 2: small-medium-rotating-saw 3: large-rotating-saw X: other-unknown-powered-saw alert-signal 1: car-horn 2: car-alarm 3: siren 4: reverse-beeper X: other-unknown-alert-signal music 1: stationary-music 2: mobile-music 3: ice-cream-truck X: music-from-uncertain-source human-voice 1: person-or-small-group-talking 2: person-or-small-group-shouting 3: large-crowd 4: amplified-speech X: other-unknown-human-voice dog 1: dog-barking-whining The classes preceded by an X code indicate when an annotator was able to identify the coarse class, but couldn’t identify the fine class because either they were uncertain which fine class it was or the fine class was not included in the taxonomy. dcase-ust-taxonomy.yaml contains this taxonomy in an easily machine-readable form.   Data splits This release contains a training subset (13538 recordings from 35 sensors), and validation subset (4308 recordings from 9 sensors), and a test subset (669 recordings from 48 sensors). The training and validation subsets are disjoint with respect to the sensor from which each recording came. The sensors in the test set will not disjoint from the training and validation subsets, but the test recordings are displaced in time, occurring after any of the recordings in the training and validation subset. The subset of recordings with verified annotations (1380 recordings) intersects with all three recording splits.  All of the recordings in the test set have these verified annotations.   Annotation data The annotation data are contained in annotations.csv, and encompass the training, validation, and test subsets. Each row in the file represents one multi-label annotation of a recording—it could be the annotation of a single citizen science volunteer, a single SONYC team member, or the agreed-upon ground truth by the SONYC team (see the annotator_id column description for more information).  Note that since the SONYC team members annotated each class group separately, there may be multiple annotation rows by a single SONYC team annotator for a particular audio recording.     Columns split The data split. (train, validate, test) sensor_id The ID of the sensor the recording is from. audio_filename The filename of the audio recording annotator_id The anonymous ID of the annotator. If this value is positive, it is a citizen science volunteer from the Zooniverse platform. If it is negative, it is a SONYC team member. If it is 0, then it is the ground truth agreed-upon by the SONYC team. year The year the recording is from. week The week of the year the recording is from. day The day of the week the recording is from, with Monday as the start (i.e. 0=Monday). hour The hour of the day the recording is from borough The NYC borough in which the sensor is located (1=Manhattan, 3=Brooklyn, 4=Queens). This corresponds to the first digit in the 10-digit NYC parcel number system known as Borough, Block, Lot (BBL). block The NYC block in which the sensor is located. This corresponds to digits 2—6 digit in the 10-digit NYC parcel number system known as Borough, Block, Lot (BBL). latitude The latitude coordinate of the block in which the sensor is located. longitude The longitude coordinate of the block in which the sensor is located. &lt;coarse_id&gt;-&lt;fine_id&gt;_&lt;fine_name&gt;_presence Columns of this form indicate the presence of fine-level class. 1 if present, 0 if not present. If -1, then the class was not labeled in this annotation because the annotation was performed by a SONYC team member who only annotated one coarse group of classes at a time when annotating the verified subset. &lt;coarse_id&gt;_&lt;coarse_name&gt;_presence Columns of this form indicate the presence of a coarse-level class. 1 if present, 0 if not present. If -1, then the class was not labeled in this annotation because the annotation was performed by a SONYC team member who only annotated one coarse group of classes at a time when annotating the verified subset. These columns are computed from the fine-level class presence columns and are presented here for convenience when training on only coarse-level classes. &lt;coarse_id&gt;-&lt;fine_id&gt;_&lt;fine_name&gt;_proximity Columns of this form indicate the proximity of a fine-level class. After indicating the presence of a fine-level class, citizen science annotators were asked to indicate the proximity of the sound event to the sensor. Only the citizen science volunteers performed this task, and therefore this data is not included in the verified annotations. This column may take on one of the following four values: (near, far, notsure, -1). If -1, then the proximity was not annotated because either the annotation was not performed by a citizen science volunteer, or the citizen science volunteer did not indicate the presence of the class.   Conditions of use Dataset created by Mark Cartwright, Jason Cramer, Ana Elisa Mendez Mendez, Yu Wang, Ho-Hsiang Wu, Vincent Lostanlen, Magdalena Fuentes, Graham Dove, Charlie Mydlarz, Justin Salamon, Oded Nov, and Juan Pablo Bello The SONYC-UST dataset is offered free of charge under the terms of the Creative Commons Attribution 4.0 International (CC BY 4.0) license: https://creativecommons.org/licenses/by/4.0/ The dataset and its contents are made available on an “as is” basis and without warranties of any kind, including without limitation satisfactory quality and conformity, merchantability, fitness for a particular purpose, accuracy or completeness, or absence of errors. Subject to any liability that may not be excluded or limited by law, New York University is not liable for, and expressly excludes all liability for, loss or damage however and whenever caused to anyone by any use of the SONYC-UST dataset or any part of it.   Feedback Please help us improve SONYC-UST by sending your feedback to: Mark Cartwright: mcartwright@gmail.com In case of a problem, please include as many details as possible.   Acknowledgments We would like to thank all the Zooniverse volunteers who continue to contribute to our project. This work is supported by National Science Foundation award 1544753.   Change log 2.3 Added the ground truth annotations for the test set, and regrouped the audio files for upload to Zenodo. 2.2 Added the audio for the test set (audio-eval.tar.gz). 2.1 The DCASE 2020 development dataset. 14778 new recordings added along with coarse spatiotemporal context information. 1.0 Data is the same as v0.4. Publication added to README. 0.4 Fixed error in annotations. Previously, the coarse class \"machinery-impact\" was accidentally indicated as present whenever \"non-machinery-impact\" was present regardless of the presence of \"machinery-impact\". This error has been fixed. 0.3 Test set annotations added 0.2 Test set audio files added' name=\"description\"/>\n",
      "<meta content=\"SONYC Urban Sound Tagging (SONYC-UST): a multilabel dataset from an urban acoustic sensor network\" name=\"citation_title\"/>\n",
      "<meta content=\"Mark Cartwright\" name=\"citation_author\"/>\n",
      "<meta content=\"Jason Cramer\" name=\"citation_author\"/>\n",
      "<meta content=\"Ana Elisa Mendez Mendez\" name=\"citation_author\"/>\n",
      "<meta content=\"Yu Wang\" name=\"citation_author\"/>\n",
      "<meta content=\"Ho-Hsiang Wu\" name=\"citation_author\"/>\n",
      "<meta content=\"Vincent Lostanlen\" name=\"citation_author\"/>\n",
      "<meta content=\"Magdalena Fuentes\" name=\"citation_author\"/>\n",
      "<meta content=\"Graham Dove\" name=\"citation_author\"/>\n",
      "<meta content=\"Charlie Mydlarz\" name=\"citation_author\"/>\n",
      "<meta content=\"Justin Salamon\" name=\"citation_author\"/>\n",
      "<meta content=\"Oded Nov\" name=\"citation_author\"/>\n",
      "<meta content=\"Juan Pablo Bello\" name=\"citation_author\"/>\n",
      "<meta content=\"2020/09/14\" name=\"citation_publication_date\"/>\n",
      "<meta content=\"10.5281/zenodo.3966543\" name=\"citation_doi\"/>\n",
      "<meta content=\"urban sound\" name=\"citation_keywords\"/>\n",
      "<meta content=\"noise pollution\" name=\"citation_keywords\"/>\n",
      "<meta content=\"machine listening\" name=\"citation_keywords\"/>\n",
      "<meta content=\"computer audition\" name=\"citation_keywords\"/>\n",
      "<meta content=\"longterm spatiotemporal context\" name=\"citation_keywords\"/>\n",
      "<meta content=\"sound tagging\" name=\"citation_keywords\"/>\n",
      "<meta content=\"https://zenodo.org/record/3966543\" name=\"citation_abstract_html_url\"/>\n",
      "<meta content=\"SONYC Urban Sound Tagging (SONYC-UST): a multilabel dataset from an urban acoustic sensor network\" property=\"og:title\"/>\n",
      "<meta content='SONYC Urban Sound Tagging (SONYC-UST): a multilabel dataset from an urban acoustic sensor network Version 2.3, September 2020   Created by Mark Cartwright (1,2,3), Jason Cramer (1), Ana Elisa Mendez Mendez (1), Yu Wang (1), Ho-Hsiang Wu (1), Vincent Lostanlen (1,2,4), Magdalena Fuentes (1), Graham Dove (2), Charlie Mydlarz (1,2), Justin Salamon (5), Oded Nov (6), Juan Pablo Bello (1,2,3) Music and Audio Research Lab, New York University Center for Urban Science and Progress, New York University Department of Computer Science and Engineering, New York University Cornell Lab of Ornithology Adobe Research Department of Technology Management and Innovation, New York University   Publication If using this data in an academic work, please reference the DOI and version, as well as cite the following paper, which presented the data collection procedure and the first version of the dataset: Cartwright, M., Cramer, J., Mendez, A.E.M., Wang, Y., Wu, H., Lostanlen, V., Fuentes, M., Dove, G., Mydlarz, C., Salamon, J., Nov, O., Bello, J.P. SONYC-UST-V2: An Urban Sound Tagging Dataset with Spatiotemporal Context. In Proceedings of the Workshop on Detection and Classification of Acoustic Scenes and Events (DCASE), 2020. [pdf]   Description SONYC Urban Sound Tagging (SONYC-UST) is a dataset for the development and evaluation of machine listening systems for realistic urban noise monitoring. The audio was recorded from the SONYC acoustic sensor network. Volunteers on the  Zooniverse citizen science platform tagged the presence of 23 classes that were chosen in consultation with the New York City Department of Environmental Protection. These 23 fine-grained classes can be grouped into 8 coarse-grained classes. The recordings are split into three sets: training, validation, and test. The training and validation sets are disjoint with respect to the sensor from which each recording came, and the test set is displaced in time. For increased reliability, three volunteers annotated each recording. In addition, members of the SONYC team subsequently created a subset of verified, ground-truth tags using a two-stage annotation procedure in which two annotators independently tagged and then collectively resolved any disagreements. This subset of recordings with verified annotations intersects with all three recording splits. All of the recordings in the test set have these verified annotations.  In v2 version of this dataset, we have also included coarse spatiotemporal context information to aid in tag prediction when time and location is known. For more details on the motivation and creation of this dataset see the DCASE 2020 Urban Sound Tagging with Spatiotemporal Context Task website.   Audio data The provided audio has been acquired using the SONYC acoustic sensor network for urban noise pollution monitoring. Over 60 different sensors have been deployed in New York City, and these sensors have collectively gathered the equivalent of over 50 years of audio data, of which we provide a small subset. The data was sampled by selecting the nearest neighbors on VGGish features of recordings known to have classes of interest. All recordings are 10 seconds and were recorded with identical microphones at identical gain settings. To maintain privacy, we quantized the spatial information to the level of a city block, and we quantized the temporal information to the level of an hour. We also limited the occurrence of recordings with positive human voice annotations to one per hour per sensor.   Label taxonomy The label taxonomy is as follows: engine 1: small-sounding-engine 2: medium-sounding-engine 3: large-sounding-engine X: engine-of-uncertain-size machinery-impact 1: rock-drill 2: jackhammer 3: hoe-ram 4: pile-driver X: other-unknown-impact-machinery non-machinery-impact 1: non-machinery-impact powered-saw 1: chainsaw 2: small-medium-rotating-saw 3: large-rotating-saw X: other-unknown-powered-saw alert-signal 1: car-horn 2: car-alarm 3: siren 4: reverse-beeper X: other-unknown-alert-signal music 1: stationary-music 2: mobile-music 3: ice-cream-truck X: music-from-uncertain-source human-voice 1: person-or-small-group-talking 2: person-or-small-group-shouting 3: large-crowd 4: amplified-speech X: other-unknown-human-voice dog 1: dog-barking-whining The classes preceded by an X code indicate when an annotator was able to identify the coarse class, but couldn’t identify the fine class because either they were uncertain which fine class it was or the fine class was not included in the taxonomy. dcase-ust-taxonomy.yaml contains this taxonomy in an easily machine-readable form.   Data splits This release contains a training subset (13538 recordings from 35 sensors), and validation subset (4308 recordings from 9 sensors), and a test subset (669 recordings from 48 sensors). The training and validation subsets are disjoint with respect to the sensor from which each recording came. The sensors in the test set will not disjoint from the training and validation subsets, but the test recordings are displaced in time, occurring after any of the recordings in the training and validation subset. The subset of recordings with verified annotations (1380 recordings) intersects with all three recording splits.  All of the recordings in the test set have these verified annotations.   Annotation data The annotation data are contained in annotations.csv, and encompass the training, validation, and test subsets. Each row in the file represents one multi-label annotation of a recording—it could be the annotation of a single citizen science volunteer, a single SONYC team member, or the agreed-upon ground truth by the SONYC team (see the annotator_id column description for more information).  Note that since the SONYC team members annotated each class group separately, there may be multiple annotation rows by a single SONYC team annotator for a particular audio recording.     Columns split The data split. (train, validate, test) sensor_id The ID of the sensor the recording is from. audio_filename The filename of the audio recording annotator_id The anonymous ID of the annotator. If this value is positive, it is a citizen science volunteer from the Zooniverse platform. If it is negative, it is a SONYC team member. If it is 0, then it is the ground truth agreed-upon by the SONYC team. year The year the recording is from. week The week of the year the recording is from. day The day of the week the recording is from, with Monday as the start (i.e. 0=Monday). hour The hour of the day the recording is from borough The NYC borough in which the sensor is located (1=Manhattan, 3=Brooklyn, 4=Queens). This corresponds to the first digit in the 10-digit NYC parcel number system known as Borough, Block, Lot (BBL). block The NYC block in which the sensor is located. This corresponds to digits 2—6 digit in the 10-digit NYC parcel number system known as Borough, Block, Lot (BBL). latitude The latitude coordinate of the block in which the sensor is located. longitude The longitude coordinate of the block in which the sensor is located. &lt;coarse_id&gt;-&lt;fine_id&gt;_&lt;fine_name&gt;_presence Columns of this form indicate the presence of fine-level class. 1 if present, 0 if not present. If -1, then the class was not labeled in this annotation because the annotation was performed by a SONYC team member who only annotated one coarse group of classes at a time when annotating the verified subset. &lt;coarse_id&gt;_&lt;coarse_name&gt;_presence Columns of this form indicate the presence of a coarse-level class. 1 if present, 0 if not present. If -1, then the class was not labeled in this annotation because the annotation was performed by a SONYC team member who only annotated one coarse group of classes at a time when annotating the verified subset. These columns are computed from the fine-level class presence columns and are presented here for convenience when training on only coarse-level classes. &lt;coarse_id&gt;-&lt;fine_id&gt;_&lt;fine_name&gt;_proximity Columns of this form indicate the proximity of a fine-level class. After indicating the presence of a fine-level class, citizen science annotators were asked to indicate the proximity of the sound event to the sensor. Only the citizen science volunteers performed this task, and therefore this data is not included in the verified annotations. This column may take on one of the following four values: (near, far, notsure, -1). If -1, then the proximity was not annotated because either the annotation was not performed by a citizen science volunteer, or the citizen science volunteer did not indicate the presence of the class.   Conditions of use Dataset created by Mark Cartwright, Jason Cramer, Ana Elisa Mendez Mendez, Yu Wang, Ho-Hsiang Wu, Vincent Lostanlen, Magdalena Fuentes, Graham Dove, Charlie Mydlarz, Justin Salamon, Oded Nov, and Juan Pablo Bello The SONYC-UST dataset is offered free of charge under the terms of the Creative Commons Attribution 4.0 International (CC BY 4.0) license: https://creativecommons.org/licenses/by/4.0/ The dataset and its contents are made available on an “as is” basis and without warranties of any kind, including without limitation satisfactory quality and conformity, merchantability, fitness for a particular purpose, accuracy or completeness, or absence of errors. Subject to any liability that may not be excluded or limited by law, New York University is not liable for, and expressly excludes all liability for, loss or damage however and whenever caused to anyone by any use of the SONYC-UST dataset or any part of it.   Feedback Please help us improve SONYC-UST by sending your feedback to: Mark Cartwright: mcartwright@gmail.com In case of a problem, please include as many details as possible.   Acknowledgments We would like to thank all the Zooniverse volunteers who continue to contribute to our project. This work is supported by National Science Foundation award 1544753.   Change log 2.3 Added the ground truth annotations for the test set, and regrouped the audio files for upload to Zenodo. 2.2 Added the audio for the test set (audio-eval.tar.gz). 2.1 The DCASE 2020 development dataset. 14778 new recordings added along with coarse spatiotemporal context information. 1.0 Data is the same as v0.4. Publication added to README. 0.4 Fixed error in annotations. Previously, the coarse class \"machinery-impact\" was accidentally indicated as present whenever \"non-machinery-impact\" was present regardless of the presence of \"machinery-impact\". This error has been fixed. 0.3 Test set annotations added 0.2 Test set audio files added' property=\"og:description\"/>\n",
      "<meta content=\"https://zenodo.org/record/3966543\" property=\"og:url\"/>\n",
      "<meta content=\"Zenodo\" property=\"og:site_name\"/>\n",
      "<meta content=\"summary\" name=\"twitter:card\"/>\n",
      "<meta content=\"@zenodo_org\" name=\"twitter:site\"/>\n",
      "<meta content=\"SONYC Urban Sound Tagging (SONYC-UST): a multilabel dataset from an urban acoustic sensor network\" name=\"twitter:title\"/>\n",
      "<meta content='SONYC Urban Sound Tagging (SONYC-UST): a multilabel dataset from an urban acoustic sensor network Version 2.3, September 2020   Created by Mark Cartwright (1,2,3), Jason Cramer (1), Ana Elisa Mendez Mendez (1), Yu Wang (1), Ho-Hsiang Wu (1), Vincent Lostanlen (1,2,4), Magdalena Fuentes (1), Graham Dove (2), Charlie Mydlarz (1,2), Justin Salamon (5), Oded Nov (6), Juan Pablo Bello (1,2,3) Music and Audio Research Lab, New York University Center for Urban Science and Progress, New York University Department of Computer Science and Engineering, New York University Cornell Lab of Ornithology Adobe Research Department of Technology Management and Innovation, New York University   Publication If using this data in an academic work, please reference the DOI and version, as well as cite the following paper, which presented the data collection procedure and the first version of the dataset: Cartwright, M., Cramer, J., Mendez, A.E.M., Wang, Y., Wu, H., Lostanlen, V., Fuentes, M., Dove, G., Mydlarz, C., Salamon, J., Nov, O., Bello, J.P. SONYC-UST-V2: An Urban Sound Tagging Dataset with Spatiotemporal Context. In Proceedings of the Workshop on Detection and Classification of Acoustic Scenes and Events (DCASE), 2020. [pdf]   Description SONYC Urban Sound Tagging (SONYC-UST) is a dataset for the development and evaluation of machine listening systems for realistic urban noise monitoring. The audio was recorded from the SONYC acoustic sensor network. Volunteers on the  Zooniverse citizen science platform tagged the presence of 23 classes that were chosen in consultation with the New York City Department of Environmental Protection. These 23 fine-grained classes can be grouped into 8 coarse-grained classes. The recordings are split into three sets: training, validation, and test. The training and validation sets are disjoint with respect to the sensor from which each recording came, and the test set is displaced in time. For increased reliability, three volunteers annotated each recording. In addition, members of the SONYC team subsequently created a subset of verified, ground-truth tags using a two-stage annotation procedure in which two annotators independently tagged and then collectively resolved any disagreements. This subset of recordings with verified annotations intersects with all three recording splits. All of the recordings in the test set have these verified annotations.  In v2 version of this dataset, we have also included coarse spatiotemporal context information to aid in tag prediction when time and location is known. For more details on the motivation and creation of this dataset see the DCASE 2020 Urban Sound Tagging with Spatiotemporal Context Task website.   Audio data The provided audio has been acquired using the SONYC acoustic sensor network for urban noise pollution monitoring. Over 60 different sensors have been deployed in New York City, and these sensors have collectively gathered the equivalent of over 50 years of audio data, of which we provide a small subset. The data was sampled by selecting the nearest neighbors on VGGish features of recordings known to have classes of interest. All recordings are 10 seconds and were recorded with identical microphones at identical gain settings. To maintain privacy, we quantized the spatial information to the level of a city block, and we quantized the temporal information to the level of an hour. We also limited the occurrence of recordings with positive human voice annotations to one per hour per sensor.   Label taxonomy The label taxonomy is as follows: engine 1: small-sounding-engine 2: medium-sounding-engine 3: large-sounding-engine X: engine-of-uncertain-size machinery-impact 1: rock-drill 2: jackhammer 3: hoe-ram 4: pile-driver X: other-unknown-impact-machinery non-machinery-impact 1: non-machinery-impact powered-saw 1: chainsaw 2: small-medium-rotating-saw 3: large-rotating-saw X: other-unknown-powered-saw alert-signal 1: car-horn 2: car-alarm 3: siren 4: reverse-beeper X: other-unknown-alert-signal music 1: stationary-music 2: mobile-music 3: ice-cream-truck X: music-from-uncertain-source human-voice 1: person-or-small-group-talking 2: person-or-small-group-shouting 3: large-crowd 4: amplified-speech X: other-unknown-human-voice dog 1: dog-barking-whining The classes preceded by an X code indicate when an annotator was able to identify the coarse class, but couldn’t identify the fine class because either they were uncertain which fine class it was or the fine class was not included in the taxonomy. dcase-ust-taxonomy.yaml contains this taxonomy in an easily machine-readable form.   Data splits This release contains a training subset (13538 recordings from 35 sensors), and validation subset (4308 recordings from 9 sensors), and a test subset (669 recordings from 48 sensors). The training and validation subsets are disjoint with respect to the sensor from which each recording came. The sensors in the test set will not disjoint from the training and validation subsets, but the test recordings are displaced in time, occurring after any of the recordings in the training and validation subset. The subset of recordings with verified annotations (1380 recordings) intersects with all three recording splits.  All of the recordings in the test set have these verified annotations.   Annotation data The annotation data are contained in annotations.csv, and encompass the training, validation, and test subsets. Each row in the file represents one multi-label annotation of a recording—it could be the annotation of a single citizen science volunteer, a single SONYC team member, or the agreed-upon ground truth by the SONYC team (see the annotator_id column description for more information).  Note that since the SONYC team members annotated each class group separately, there may be multiple annotation rows by a single SONYC team annotator for a particular audio recording.     Columns split The data split. (train, validate, test) sensor_id The ID of the sensor the recording is from. audio_filename The filename of the audio recording annotator_id The anonymous ID of the annotator. If this value is positive, it is a citizen science volunteer from the Zooniverse platform. If it is negative, it is a SONYC team member. If it is 0, then it is the ground truth agreed-upon by the SONYC team. year The year the recording is from. week The week of the year the recording is from. day The day of the week the recording is from, with Monday as the start (i.e. 0=Monday). hour The hour of the day the recording is from borough The NYC borough in which the sensor is located (1=Manhattan, 3=Brooklyn, 4=Queens). This corresponds to the first digit in the 10-digit NYC parcel number system known as Borough, Block, Lot (BBL). block The NYC block in which the sensor is located. This corresponds to digits 2—6 digit in the 10-digit NYC parcel number system known as Borough, Block, Lot (BBL). latitude The latitude coordinate of the block in which the sensor is located. longitude The longitude coordinate of the block in which the sensor is located. &lt;coarse_id&gt;-&lt;fine_id&gt;_&lt;fine_name&gt;_presence Columns of this form indicate the presence of fine-level class. 1 if present, 0 if not present. If -1, then the class was not labeled in this annotation because the annotation was performed by a SONYC team member who only annotated one coarse group of classes at a time when annotating the verified subset. &lt;coarse_id&gt;_&lt;coarse_name&gt;_presence Columns of this form indicate the presence of a coarse-level class. 1 if present, 0 if not present. If -1, then the class was not labeled in this annotation because the annotation was performed by a SONYC team member who only annotated one coarse group of classes at a time when annotating the verified subset. These columns are computed from the fine-level class presence columns and are presented here for convenience when training on only coarse-level classes. &lt;coarse_id&gt;-&lt;fine_id&gt;_&lt;fine_name&gt;_proximity Columns of this form indicate the proximity of a fine-level class. After indicating the presence of a fine-level class, citizen science annotators were asked to indicate the proximity of the sound event to the sensor. Only the citizen science volunteers performed this task, and therefore this data is not included in the verified annotations. This column may take on one of the following four values: (near, far, notsure, -1). If -1, then the proximity was not annotated because either the annotation was not performed by a citizen science volunteer, or the citizen science volunteer did not indicate the presence of the class.   Conditions of use Dataset created by Mark Cartwright, Jason Cramer, Ana Elisa Mendez Mendez, Yu Wang, Ho-Hsiang Wu, Vincent Lostanlen, Magdalena Fuentes, Graham Dove, Charlie Mydlarz, Justin Salamon, Oded Nov, and Juan Pablo Bello The SONYC-UST dataset is offered free of charge under the terms of the Creative Commons Attribution 4.0 International (CC BY 4.0) license: https://creativecommons.org/licenses/by/4.0/ The dataset and its contents are made available on an “as is” basis and without warranties of any kind, including without limitation satisfactory quality and conformity, merchantability, fitness for a particular purpose, accuracy or completeness, or absence of errors. Subject to any liability that may not be excluded or limited by law, New York University is not liable for, and expressly excludes all liability for, loss or damage however and whenever caused to anyone by any use of the SONYC-UST dataset or any part of it.   Feedback Please help us improve SONYC-UST by sending your feedback to: Mark Cartwright: mcartwright@gmail.com In case of a problem, please include as many details as possible.   Acknowledgments We would like to thank all the Zooniverse volunteers who continue to contribute to our project. This work is supported by National Science Foundation award 1544753.   Change log 2.3 Added the ground truth annotations for the test set, and regrouped the audio files for upload to Zenodo. 2.2 Added the audio for the test set (audio-eval.tar.gz). 2.1 The DCASE 2020 development dataset. 14778 new recordings added along with coarse spatiotemporal context information. 1.0 Data is the same as v0.4. Publication added to README. 0.4 Fixed error in annotations. Previously, the coarse class \"machinery-impact\" was accidentally indicated as present whenever \"non-machinery-impact\" was present regardless of the presence of \"machinery-impact\". This error has been fixed. 0.3 Test set annotations added 0.2 Test set audio files added' name=\"twitter:description\"/>\n",
      "<link href=\"https://zenodo.org/record/3966543\" rel=\"canonical\"/>\n",
      "<link href=\"https://zenodo.org/record/3966543/files/annotations.csv\" rel=\"alternate\" type=\"text/csv\"/>\n",
      "<link href=\"https://zenodo.org/record/3966543/files/audio-0.tar.gz\" rel=\"alternate\" type=\"application/gzip\"/>\n",
      "<link href=\"https://zenodo.org/record/3966543/files/audio-10.tar.gz\" rel=\"alternate\" type=\"application/gzip\"/>\n",
      "<link href=\"https://zenodo.org/record/3966543/files/audio-11.tar.gz\" rel=\"alternate\" type=\"application/gzip\"/>\n",
      "<link href=\"https://zenodo.org/record/3966543/files/audio-12.tar.gz\" rel=\"alternate\" type=\"application/gzip\"/>\n",
      "<link href=\"https://zenodo.org/record/3966543/files/audio-13.tar.gz\" rel=\"alternate\" type=\"application/gzip\"/>\n",
      "<link href=\"https://zenodo.org/record/3966543/files/audio-14.tar.gz\" rel=\"alternate\" type=\"application/gzip\"/>\n",
      "<link href=\"https://zenodo.org/record/3966543/files/audio-15.tar.gz\" rel=\"alternate\" type=\"application/gzip\"/>\n",
      "<link href=\"https://zenodo.org/record/3966543/files/audio-16.tar.gz\" rel=\"alternate\" type=\"application/gzip\"/>\n",
      "<link href=\"https://zenodo.org/record/3966543/files/audio-17.tar.gz\" rel=\"alternate\" type=\"application/gzip\"/>\n",
      "<link href=\"https://zenodo.org/record/3966543/files/audio-18.tar.gz\" rel=\"alternate\" type=\"application/gzip\"/>\n",
      "<link href=\"https://zenodo.org/record/3966543/files/audio-1.tar.gz\" rel=\"alternate\" type=\"application/gzip\"/>\n",
      "<link href=\"https://zenodo.org/record/3966543/files/audio-2.tar.gz\" rel=\"alternate\" type=\"application/gzip\"/>\n",
      "<link href=\"https://zenodo.org/record/3966543/files/audio-3.tar.gz\" rel=\"alternate\" type=\"application/gzip\"/>\n",
      "<link href=\"https://zenodo.org/record/3966543/files/audio-4.tar.gz\" rel=\"alternate\" type=\"application/gzip\"/>\n",
      "<link href=\"https://zenodo.org/record/3966543/files/audio-5.tar.gz\" rel=\"alternate\" type=\"application/gzip\"/>\n",
      "<link href=\"https://zenodo.org/record/3966543/files/audio-6.tar.gz\" rel=\"alternate\" type=\"application/gzip\"/>\n",
      "<link href=\"https://zenodo.org/record/3966543/files/audio-7.tar.gz\" rel=\"alternate\" type=\"application/gzip\"/>\n",
      "<link href=\"https://zenodo.org/record/3966543/files/audio-8.tar.gz\" rel=\"alternate\" type=\"application/gzip\"/>\n",
      "<link href=\"https://zenodo.org/record/3966543/files/audio-9.tar.gz\" rel=\"alternate\" type=\"application/gzip\"/>\n",
      "<link href=\"https://zenodo.org/record/3966543/files/dcase-ust-taxonomy.yaml\" rel=\"alternate\" type=\"application/octet-stream\"/>\n",
      "<link href=\"https://zenodo.org/record/3966543/files/README.md\" rel=\"alternate\" type=\"application/octet-stream\"/>\n",
      "<link href=\"https://zenodo.org/record/3966543/files/unpack_audio.sh\" rel=\"alternate\" type=\"application/x-sh\"/>\n",
      "</meta></meta></meta></head>\n",
      "<body data-spy=\"scroll\" data-target=\".scrollspy-target\" itemscope=\"\" itemtype=\"http://schema.org/WebPage\" ng-csp=\"\">\n",
      "<!--[if lt IE 8]>\n",
      "      <p class=\"browserupgrade\">You are using an <strong>outdated</strong> browser. Please <a href=\"http://browsehappy.com/\">upgrade your browser</a> to improve your experience.</p>\n",
      "    <![endif]-->\n",
      "<header>\n",
      "<nav class=\"navbar navbar-default navbar-static-top\">\n",
      "<div class=\"container\">\n",
      "<div class=\"navbar-header\">\n",
      "<button aria-controls=\"navbar\" aria-expanded=\"false\" class=\"navbar-toggle collapsed\" data-target=\"#navbar\" data-toggle=\"collapse\" type=\"button\">\n",
      "<span class=\"sr-only\">Toggle navigation</span>\n",
      "<span class=\"icon-bar\"></span>\n",
      "<span class=\"icon-bar\"></span>\n",
      "<span class=\"icon-bar\"></span>\n",
      "</button>\n",
      "<a href=\"/\"><img alt=\"Zenodo\" class=\"navbar-brand\" src=\"/static/img/zenodo.svg\"/></a>\n",
      "</div>\n",
      "<div class=\"navbar-collapse collapse\" id=\"navbar\">\n",
      "<form action=\"/search\" class=\"navbar-form navbar-left navbar-search\" role=\"search\">\n",
      "<div class=\"form-group\">\n",
      "<div class=\"input-group\">\n",
      "<input class=\"form-control search\" name=\"q\" placeholder=\"Search\" type=\"text\"/>\n",
      "<div class=\"input-group-btn\">\n",
      "<button class=\"btn btn-default\" type=\"submit\"><i class=\"fa fa-search\"></i></button>\n",
      "</div>\n",
      "</div>\n",
      "</div>\n",
      "</form>\n",
      "<ul class=\"nav navbar-nav\">\n",
      "<li><a href=\"/deposit\">Upload</a></li>\n",
      "<li><a href=\"/communities/\">Communities</a></li>\n",
      "</ul>\n",
      "<form class=\"navbar-form navbar-right\">\n",
      "<a class=\"btn btn-default\" href=\"/login/?next=%2Frecord%2F3966543%2F\"><i class=\"fa fa-sign-in\"></i> Log in</a>\n",
      "<a class=\"btn btn-default btn-warning\" href=\"/signup/\" type=\"submit\"><i class=\"fa fa-edit\"></i> Sign up</a>\n",
      "</form>\n",
      "</div>\n",
      "</div>\n",
      "</nav>\n",
      "</header>\n",
      "<div class=\"container record-detail\">\n",
      "<div class=\"row\">\n",
      "<div class=\"col-sm-8 col-md-8 col-left\">\n",
      "<p>\n",
      "<time data-toggle=\"tooltip\" datetime=\"September 14, 2020\" title=\"Publication date\">September 14, 2020</time>\n",
      "<span class=\"pull-right\">\n",
      "<span class=\"label label-default\">Dataset</span>\n",
      "<span class=\"label label-success\">\n",
      "Open Access\n",
      "</span>\n",
      "</span>\n",
      "</p>\n",
      "<h1>SONYC Urban Sound Tagging (SONYC-UST): a multilabel dataset from an urban acoustic sensor network</h1>\n",
      "<p>\n",
      "<a href=\"https://orcid.org/0000-0002-5908-390X\"><img class=\"inline-orcid\" src=\"/static/img/orcid.png\"/></a>\n",
      "<span class=\"text-muted\" data-toggle=\"tooltip\" title=\"New York University\">Mark Cartwright</span>; \n",
      "<a href=\"https://orcid.org/0000-0001-5288-9399\"><img class=\"inline-orcid\" src=\"/static/img/orcid.png\"/></a>\n",
      "<span class=\"text-muted\" data-toggle=\"tooltip\" title=\"New York University\">Jason Cramer</span>; \n",
      "<a href=\"https://orcid.org/0000-0002-4861-5616\"><img class=\"inline-orcid\" src=\"/static/img/orcid.png\"/></a>\n",
      "<span class=\"text-muted\" data-toggle=\"tooltip\" title=\"New York University\">Ana Elisa Mendez Mendez</span>; \n",
      "<a href=\"https://orcid.org/0000-0002-1615-5141\"><img class=\"inline-orcid\" src=\"/static/img/orcid.png\"/></a>\n",
      "<span class=\"text-muted\" data-toggle=\"tooltip\" title=\"New York University\">Yu Wang</span>; \n",
      "<a href=\"https://orcid.org/0000-0002-1102-074X\"><img class=\"inline-orcid\" src=\"/static/img/orcid.png\"/></a>\n",
      "<span class=\"text-muted\" data-toggle=\"tooltip\" title=\"New York University\">Ho-Hsiang Wu</span>; \n",
      "<a href=\"https://orcid.org/0000-0003-0580-1651\"><img class=\"inline-orcid\" src=\"/static/img/orcid.png\"/></a>\n",
      "<span class=\"text-muted\" data-toggle=\"tooltip\" title=\"Cornel University\">Vincent Lostanlen</span>; \n",
      "<a href=\"https://orcid.org/0000-0003-4506-6639\"><img class=\"inline-orcid\" src=\"/static/img/orcid.png\"/></a>\n",
      "<span class=\"text-muted\" data-toggle=\"tooltip\" title=\"New York University\">Magdalena Fuentes</span>; \n",
      "<a href=\"https://orcid.org/0000-0002-3551-0209\"><img class=\"inline-orcid\" src=\"/static/img/orcid.png\"/></a>\n",
      "<span class=\"text-muted\" data-toggle=\"tooltip\" title=\"New York University\">Graham Dove</span>; \n",
      "<a href=\"https://orcid.org/0000-0001-7061-0638\"><img class=\"inline-orcid\" src=\"/static/img/orcid.png\"/></a>\n",
      "<span class=\"text-muted\" data-toggle=\"tooltip\" title=\"New York University\">Charlie Mydlarz</span>; \n",
      "<a href=\"https://orcid.org/0000-0001-6345-4593\"><img class=\"inline-orcid\" src=\"/static/img/orcid.png\"/></a>\n",
      "<span class=\"text-muted\" data-toggle=\"tooltip\" title=\"New York University\">Justin Salamon</span>; \n",
      "<a href=\"https://orcid.org/0000-0001-6410-2995\"><img class=\"inline-orcid\" src=\"/static/img/orcid.png\"/></a>\n",
      "<span class=\"text-muted\" data-toggle=\"tooltip\" title=\"New York University\">Oded Nov</span>; \n",
      "<a href=\"https://orcid.org/0000-0001-8561-5204\"><img class=\"inline-orcid\" src=\"/static/img/orcid.png\"/></a>\n",
      "<span class=\"text-muted\" data-toggle=\"tooltip\" title=\"New York University\">Juan Pablo Bello</span></p>\n",
      "<div class=\"record-description\"><p><strong>SONYC Urban Sound Tagging (SONYC-UST): a multilabel dataset from an urban acoustic sensor network</strong></p>\n",
      "<p>Version 2.3, September 2020</p>\n",
      "<p> </p>\n",
      "<p><strong>Created by</strong></p>\n",
      "<p>Mark Cartwright (1,2,3), Jason Cramer (1), Ana Elisa Mendez Mendez (1), Yu Wang (1), Ho-Hsiang Wu (1), Vincent Lostanlen (1,2,4), Magdalena Fuentes (1), Graham Dove (2), Charlie Mydlarz (1,2), Justin Salamon (5), Oded Nov (6), Juan Pablo Bello (1,2,3)</p>\n",
      "<ol>\n",
      "<li>Music and Audio Research Lab, New York University</li>\n",
      "<li>Center for Urban Science and Progress, New York University</li>\n",
      "<li>Department of Computer Science and Engineering, New York University</li>\n",
      "<li>Cornell Lab of Ornithology</li>\n",
      "<li>Adobe Research</li>\n",
      "<li>Department of Technology Management and Innovation, New York University</li>\n",
      "</ol>\n",
      "<p> </p>\n",
      "<p><strong>Publication</strong></p>\n",
      "<p>If using this data in an academic work, please reference the DOI and version, as well as cite the following paper, which presented the data collection procedure and the first version of the dataset:</p>\n",
      "<p>Cartwright, M., Cramer, J., Mendez, A.E.M., Wang, Y., Wu, H., Lostanlen, V., Fuentes, M., Dove, G., Mydlarz, C., Salamon, J., Nov, O., Bello, J.P. SONYC-UST-V2: An Urban Sound Tagging Dataset with Spatiotemporal Context. In <em>Proceedings of the Workshop on Detection and Classification of Acoustic Scenes and Events (DCASE)</em>, 2020.<br/>\n",
      "<a href=\"https://arxiv.org/abs/2009.05188\">[pdf]</a></p>\n",
      "<p> </p>\n",
      "<p><strong>Description</strong></p>\n",
      "<p>SONYC Urban Sound Tagging (SONYC-UST) is a dataset for the development and evaluation of machine listening systems for realistic urban noise monitoring. The audio was recorded from the <a href=\"https://wp.nyu.edu/sonyc\">SONYC</a> acoustic sensor network. Volunteers on the  <a href=\"https://zooniverse.org\">Zooniverse</a> citizen science platform tagged the presence of 23 classes that were chosen in consultation with the New York City Department of Environmental Protection. These 23 fine-grained classes can be grouped into 8 coarse-grained classes. The recordings are split into three sets: training, validation, and test. The training and validation sets are disjoint with respect to the sensor from which each recording came, and the test set is displaced in time. For increased reliability, three volunteers annotated each recording. In addition, members of the SONYC team subsequently created a subset of verified, ground-truth tags using a two-stage annotation procedure in which two annotators independently tagged and then collectively resolved any disagreements. This subset of recordings with verified annotations intersects with all three recording splits. All of the recordings in the test set have these verified annotations.  In v2 version of this dataset, we have also included coarse spatiotemporal context information to aid in tag prediction when time and location is known. For more details on the motivation and creation of this dataset see the <a href=\"http://dcase.community/challenge2020/task-urban-sound-tagging-with-spatiotemporal-context\">DCASE 2020 Urban Sound Tagging with Spatiotemporal Context Task website</a>.</p>\n",
      "<p> </p>\n",
      "<p><strong>Audio data</strong></p>\n",
      "<p>The provided audio has been acquired using the SONYC acoustic sensor network for urban noise pollution monitoring. Over 60 different sensors have been deployed in New York City, and these sensors have collectively gathered the equivalent of over 50 years of audio data, of which we provide a small subset. The data was sampled by selecting the nearest neighbors on VGGish features of recordings known to have classes of interest. All recordings are 10 seconds and were recorded with identical microphones at identical gain settings. To maintain privacy, we quantized the spatial information to the level of a city block, and we quantized the temporal information to the level of an hour. We also limited the occurrence of recordings with positive human voice annotations to one per hour per sensor.</p>\n",
      "<p> </p>\n",
      "<p><strong>Label taxonomy</strong></p>\n",
      "<p>The label taxonomy is as follows:</p>\n",
      "<ol>\n",
      "<li>engine<br/>\n",
      "\t1: small-sounding-engine<br/>\n",
      "\t2: medium-sounding-engine<br/>\n",
      "\t3: large-sounding-engine<br/>\n",
      "\tX: engine-of-uncertain-size</li>\n",
      "<li>machinery-impact<br/>\n",
      "\t1: rock-drill<br/>\n",
      "\t2: jackhammer<br/>\n",
      "\t3: hoe-ram<br/>\n",
      "\t4: pile-driver<br/>\n",
      "\tX: other-unknown-impact-machinery</li>\n",
      "<li>non-machinery-impact<br/>\n",
      "\t1: non-machinery-impact</li>\n",
      "<li>powered-saw<br/>\n",
      "\t1: chainsaw<br/>\n",
      "\t2: small-medium-rotating-saw<br/>\n",
      "\t3: large-rotating-saw<br/>\n",
      "\tX: other-unknown-powered-saw</li>\n",
      "<li>alert-signal<br/>\n",
      "\t1: car-horn<br/>\n",
      "\t2: car-alarm<br/>\n",
      "\t3: siren<br/>\n",
      "\t4: reverse-beeper<br/>\n",
      "\tX: other-unknown-alert-signal</li>\n",
      "<li>music<br/>\n",
      "\t1: stationary-music<br/>\n",
      "\t2: mobile-music<br/>\n",
      "\t3: ice-cream-truck<br/>\n",
      "\tX: music-from-uncertain-source</li>\n",
      "<li>human-voice<br/>\n",
      "\t1: person-or-small-group-talking<br/>\n",
      "\t2: person-or-small-group-shouting<br/>\n",
      "\t3: large-crowd<br/>\n",
      "\t4: amplified-speech<br/>\n",
      "\tX: other-unknown-human-voice</li>\n",
      "<li>dog<br/>\n",
      "\t1: dog-barking-whining</li>\n",
      "</ol>\n",
      "<p>The classes preceded by an <code>X</code> code indicate when an annotator was able to identify the coarse class, but couldn’t identify the fine class because either they were uncertain which fine class it was or the fine class was not included in the taxonomy. <code>dcase-ust-taxonomy.yaml</code> contains this taxonomy in an easily machine-readable form.</p>\n",
      "<p> </p>\n",
      "<p><strong>Data splits</strong></p>\n",
      "<p>This release contains a training subset (13538 recordings from 35 sensors), and validation subset (4308 recordings from 9 sensors), and a test subset (669 recordings from 48 sensors). The training and validation subsets are disjoint with respect to the sensor from which each recording came. The sensors in the test set will not disjoint from the training and validation subsets, but the test recordings are displaced in time, occurring after any of the recordings in the training and validation subset. The subset of recordings with verified annotations (1380 recordings) intersects with all three recording splits.  All of the recordings in the test set have these verified annotations.</p>\n",
      "<p> </p>\n",
      "<p><strong>Annotation data</strong></p>\n",
      "<p>The annotation data are contained in <code>annotations.csv</code>, and encompass the training, validation, and test subsets. Each row in the file represents one multi-label annotation of a recording—it could be the annotation of a single citizen science volunteer, a single SONYC team member, or the agreed-upon ground truth by the SONYC team (see the <em>annotator_id</em> column description for more information).  Note that since the SONYC team members annotated each class group separately, there may be multiple annotation rows by a single SONYC team annotator for a particular audio recording.</p>\n",
      "<p> </p>\n",
      "<p> </p>\n",
      "<p><strong>Columns</strong></p>\n",
      "<p><em>split</em></p>\n",
      "<p>The data split. (<em>train</em>, <em>validate, test</em>)</p>\n",
      "<p><em>sensor_id</em></p>\n",
      "<p>The ID of the sensor the recording is from.</p>\n",
      "<p><em>audio_filename</em></p>\n",
      "<p>The filename of the audio recording</p>\n",
      "<p><em>annotator_id</em></p>\n",
      "<p>The anonymous ID of the annotator. If this value is positive, it is a citizen science volunteer from the Zooniverse platform. If it is negative, it is a SONYC team member. If it is <code>0</code>, then it is the ground truth agreed-upon by the SONYC team.</p>\n",
      "<p><em>year</em></p>\n",
      "<p>The year the recording is from.</p>\n",
      "<p><em>week</em></p>\n",
      "<p>The week of the year the recording is from.</p>\n",
      "<p><em>day</em></p>\n",
      "<p>The day of the week the recording is from, with Monday as the start (i.e. <code>0</code>=Monday).</p>\n",
      "<p><em>hour</em></p>\n",
      "<p>The hour of the day the recording is from</p>\n",
      "<p><em>borough</em><br/>\n",
      "The NYC borough in which the sensor is located (<code>1</code>=Manhattan, <code>3</code>=Brooklyn, <code>4</code>=Queens). This corresponds to the first digit in the 10-digit NYC parcel number system known as Borough, Block, Lot (BBL).</p>\n",
      "<p><em>block</em></p>\n",
      "<p>The NYC block in which the sensor is located. This corresponds to digits 2—6 digit in the 10-digit NYC parcel number system known as Borough, Block, Lot (BBL).</p>\n",
      "<p><em>latitude</em></p>\n",
      "<p>The latitude coordinate of the <strong>block</strong> in which the sensor is located.</p>\n",
      "<p><em>longitude</em></p>\n",
      "<p>The longitude coordinate of the <strong>block</strong> in which the sensor is located.</p>\n",
      "<p><em>&lt;coarse_id&gt;-&lt;fine_id&gt;_&lt;fine_name&gt;_presence</em></p>\n",
      "<p>Columns of this form indicate the presence of fine-level class. <code>1</code> if present, <code>0</code> if not present. If <code>-1</code>, then the class was not labeled in this annotation because the annotation was performed by a SONYC team member who only annotated one coarse group of classes at a time when annotating the verified subset.</p>\n",
      "<p><em>&lt;coarse_id&gt;_&lt;coarse_name&gt;_presence</em></p>\n",
      "<p>Columns of this form indicate the presence of a coarse-level class. <code>1</code> if present, <code>0</code> if not present. If <code>-1</code>, then the class was not labeled in this annotation because the annotation was performed by a SONYC team member who only annotated one coarse group of classes at a time when annotating the verified subset. These columns are computed from the fine-level class presence columns and are presented here for convenience when training on only coarse-level classes.</p>\n",
      "<p><em>&lt;coarse_id&gt;-&lt;fine_id&gt;_&lt;fine_name&gt;_proximity</em></p>\n",
      "<p>Columns of this form indicate the proximity of a fine-level class. After indicating the presence of a fine-level class, citizen science annotators were asked to indicate the proximity of the sound event to the sensor. Only the citizen science volunteers performed this task, and therefore this data is not included in the verified annotations. This column may take on one of the following four values: (<code>near</code>, <code>far</code>, <code>notsure</code>, <code>-1</code>). If <code>-1</code>, then the proximity was not annotated because either the annotation was not performed by a citizen science volunteer, or the citizen science volunteer did not indicate the presence of the class.</p>\n",
      "<p> </p>\n",
      "<p><strong>Conditions of use</strong></p>\n",
      "<p>Dataset created by Mark Cartwright, Jason Cramer, Ana Elisa Mendez Mendez, Yu Wang, Ho-Hsiang Wu, Vincent Lostanlen, Magdalena Fuentes, Graham Dove, Charlie Mydlarz, Justin Salamon, Oded Nov, and Juan Pablo Bello</p>\n",
      "<p>The SONYC-UST dataset is offered free of charge under the terms of the Creative Commons Attribution 4.0 International (CC BY 4.0) license:<br/>\n",
      "<a href=\"https://creativecommons.org/licenses/by/4.0/\">https://creativecommons.org/licenses/by/4.0/</a></p>\n",
      "<p>The dataset and its contents are made available on an “as is” basis and without warranties of any kind, including without limitation satisfactory quality and conformity, merchantability, fitness for a particular purpose, accuracy or completeness, or absence of errors. Subject to any liability that may not be excluded or limited by law, New York University is not liable for, and expressly excludes all liability for, loss or damage however and whenever caused to anyone by any use of the SONYC-UST dataset or any part of it.</p>\n",
      "<p> </p>\n",
      "<p><strong>Feedback</strong></p>\n",
      "<p>Please help us improve SONYC-UST by sending your feedback to:</p>\n",
      "<ul>\n",
      "<li>Mark Cartwright: <a href=\"mailto:mcartwright@gmail.com\">mcartwright@gmail.com</a></li>\n",
      "</ul>\n",
      "<p>In case of a problem, please include as many details as possible.</p>\n",
      "<p> </p>\n",
      "<p><strong>Acknowledgments</strong></p>\n",
      "<p>We would like to thank all the Zooniverse volunteers who continue to contribute to our project. This work is supported by <a href=\"https://www.nsf.gov/awardsearch/showAward?AWD_ID=1544753\">National Science Foundation award 1544753</a>.</p>\n",
      "<p> </p>\n",
      "<p><strong>Change log</strong></p>\n",
      "<ul>\n",
      "<li>2.3 Added the ground truth annotations for the test set, and regrouped the audio files for upload to Zenodo.</li>\n",
      "<li>2.2 Added the audio for the test set (audio-eval.tar.gz).</li>\n",
      "<li>2.1 The DCASE 2020 development dataset. 14778 new recordings added along with coarse spatiotemporal context information.</li>\n",
      "<li>1.0 Data is the same as v0.4. Publication added to README.</li>\n",
      "<li>0.4 Fixed error in annotations. Previously, the coarse class \"machinery-impact\" was accidentally indicated as present whenever \"non-machinery-impact\" was present regardless of the presence of \"machinery-impact\". This error has been fixed.</li>\n",
      "<li>0.3 Test set annotations added</li>\n",
      "<li>0.2 Test set audio files added</li>\n",
      "</ul></div>\n",
      "<div class=\"alert alert-warning record-notes\">\n",
      "        This work is supported by National Science Foundation award 1544753.\n",
      "      </div>\n",
      "<div class=\"panel panel-default\" id=\"preview\">\n",
      "<div class=\"panel-heading\">\n",
      "<a class=\"panel-toggle\" data-toggle=\"collapse\" href=\"#collapseOne\">\n",
      "        Preview\n",
      "        <span class=\"pull-right show-on-collapsed\"><i class=\"fa fa-chevron-right\"></i></span>\n",
      "<span class=\"pull-right hide-on-collapsed\"><i class=\"fa fa-chevron-down\"></i></span>\n",
      "</a>\n",
      "</div>\n",
      "<div class=\"collapse in\" id=\"collapseOne\">\n",
      "<iframe class=\"preview-iframe\" height=\"400\" id=\"preview-iframe\" src=\"/record/3966543/preview/README.md\" width=\"100%\"></iframe>\n",
      "</div>\n",
      "</div><div class=\"panel panel-default files-box\" id=\"files\">\n",
      "<div class=\"panel-heading\">\n",
      "<a class=\"panel-toggle\" data-toggle=\"collapse\" href=\"#collapseTwo\">\n",
      "      Files\n",
      "      <span class=\"pull-right show-on-collapsed\"><i class=\"fa fa-chevron-right\"></i></span>\n",
      "<span class=\"pull-right hide-on-collapsed\"><i class=\"fa fa-chevron-down\"></i></span>\n",
      "</a>\n",
      "<small class=\"text-muted\"> (13.3 GB)</small>\n",
      "</div>\n",
      "<div class=\"collapse in\" id=\"collapseTwo\">\n",
      "<table class=\"table table-striped\">\n",
      "<thead>\n",
      "<tr class=\"\">\n",
      "<th>Name</th>\n",
      "<th>Size</th>\n",
      "</tr>\n",
      "</thead>\n",
      "<tbody>\n",
      "<tr>\n",
      "<td>\n",
      "<a class=\"filename\" href=\"/record/3966543/files/annotations.csv?download=1\">annotations.csv</a>\n",
      "<br><small class=\"text-muted nowrap\">md5:70b507b15bb4cfcce4870925302f276b <i class=\"fa fa-question-circle text-muted\" data-placement=\"top\" data-toggle=\"tooltip\" title=\"This is the file fingerprint (MD5 checksum), which can be used to verify the file integrity.\" tooltip=\"\"></i></small>\n",
      "</br></td>\n",
      "<td class=\"nowrap\">14.5 MB</td>\n",
      "<td class=\"nowrap\"><span class=\"pull-right\"><button class=\"btn preview-link btn-xs btn-default\" data-filename=\"annotations.csv\"><i class=\"fa fa-eye\"></i> Preview</button> <a class=\"btn btn-xs btn-default\" href=\"/record/3966543/files/annotations.csv?download=1\"><i class=\"fa fa-download\"></i> Download</a></span></td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>\n",
      "<a class=\"filename\" href=\"/record/3966543/files/audio-0.tar.gz?download=1\">audio-0.tar.gz</a>\n",
      "<br><small class=\"text-muted nowrap\">md5:bbb4dbae7d2e58e18d24878b9ee1eb51 <i class=\"fa fa-question-circle text-muted\" data-placement=\"top\" data-toggle=\"tooltip\" title=\"This is the file fingerprint (MD5 checksum), which can be used to verify the file integrity.\" tooltip=\"\"></i></small>\n",
      "</br></td>\n",
      "<td class=\"nowrap\">721.6 MB</td>\n",
      "<td class=\"nowrap\"><span class=\"pull-right\"><a class=\"btn btn-xs btn-default\" href=\"/record/3966543/files/audio-0.tar.gz?download=1\"><i class=\"fa fa-download\"></i> Download</a></span></td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>\n",
      "<a class=\"filename\" href=\"/record/3966543/files/audio-1.tar.gz?download=1\">audio-1.tar.gz</a>\n",
      "<br><small class=\"text-muted nowrap\">md5:7c369ff37ac6a1fdd8493fdffe62e0a1 <i class=\"fa fa-question-circle text-muted\" data-placement=\"top\" data-toggle=\"tooltip\" title=\"This is the file fingerprint (MD5 checksum), which can be used to verify the file integrity.\" tooltip=\"\"></i></small>\n",
      "</br></td>\n",
      "<td class=\"nowrap\">721.7 MB</td>\n",
      "<td class=\"nowrap\"><span class=\"pull-right\"><a class=\"btn btn-xs btn-default\" href=\"/record/3966543/files/audio-1.tar.gz?download=1\"><i class=\"fa fa-download\"></i> Download</a></span></td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>\n",
      "<a class=\"filename\" href=\"/record/3966543/files/audio-10.tar.gz?download=1\">audio-10.tar.gz</a>\n",
      "<br><small class=\"text-muted nowrap\">md5:84e94de273666c537a3e6b709ee88d9b <i class=\"fa fa-question-circle text-muted\" data-placement=\"top\" data-toggle=\"tooltip\" title=\"This is the file fingerprint (MD5 checksum), which can be used to verify the file integrity.\" tooltip=\"\"></i></small>\n",
      "</br></td>\n",
      "<td class=\"nowrap\">717.7 MB</td>\n",
      "<td class=\"nowrap\"><span class=\"pull-right\"><a class=\"btn btn-xs btn-default\" href=\"/record/3966543/files/audio-10.tar.gz?download=1\"><i class=\"fa fa-download\"></i> Download</a></span></td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>\n",
      "<a class=\"filename\" href=\"/record/3966543/files/audio-11.tar.gz?download=1\">audio-11.tar.gz</a>\n",
      "<br><small class=\"text-muted nowrap\">md5:44fe1f43121a7d1178aa0cdf7477793b <i class=\"fa fa-question-circle text-muted\" data-placement=\"top\" data-toggle=\"tooltip\" title=\"This is the file fingerprint (MD5 checksum), which can be used to verify the file integrity.\" tooltip=\"\"></i></small>\n",
      "</br></td>\n",
      "<td class=\"nowrap\">719.1 MB</td>\n",
      "<td class=\"nowrap\"><span class=\"pull-right\"><a class=\"btn btn-xs btn-default\" href=\"/record/3966543/files/audio-11.tar.gz?download=1\"><i class=\"fa fa-download\"></i> Download</a></span></td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>\n",
      "<a class=\"filename\" href=\"/record/3966543/files/audio-12.tar.gz?download=1\">audio-12.tar.gz</a>\n",
      "<br><small class=\"text-muted nowrap\">md5:a7eb21011f460b5ac289f8e285e875ab <i class=\"fa fa-question-circle text-muted\" data-placement=\"top\" data-toggle=\"tooltip\" title=\"This is the file fingerprint (MD5 checksum), which can be used to verify the file integrity.\" tooltip=\"\"></i></small>\n",
      "</br></td>\n",
      "<td class=\"nowrap\">716.1 MB</td>\n",
      "<td class=\"nowrap\"><span class=\"pull-right\"><a class=\"btn btn-xs btn-default\" href=\"/record/3966543/files/audio-12.tar.gz?download=1\"><i class=\"fa fa-download\"></i> Download</a></span></td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>\n",
      "<a class=\"filename\" href=\"/record/3966543/files/audio-13.tar.gz?download=1\">audio-13.tar.gz</a>\n",
      "<br><small class=\"text-muted nowrap\">md5:27945a8b1008eec787f97213164951e6 <i class=\"fa fa-question-circle text-muted\" data-placement=\"top\" data-toggle=\"tooltip\" title=\"This is the file fingerprint (MD5 checksum), which can be used to verify the file integrity.\" tooltip=\"\"></i></small>\n",
      "</br></td>\n",
      "<td class=\"nowrap\">719.9 MB</td>\n",
      "<td class=\"nowrap\"><span class=\"pull-right\"><a class=\"btn btn-xs btn-default\" href=\"/record/3966543/files/audio-13.tar.gz?download=1\"><i class=\"fa fa-download\"></i> Download</a></span></td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>\n",
      "<a class=\"filename\" href=\"/record/3966543/files/audio-14.tar.gz?download=1\">audio-14.tar.gz</a>\n",
      "<br><small class=\"text-muted nowrap\">md5:fb8c7d81c3bde3a0c86c1e55e6a85a24 <i class=\"fa fa-question-circle text-muted\" data-placement=\"top\" data-toggle=\"tooltip\" title=\"This is the file fingerprint (MD5 checksum), which can be used to verify the file integrity.\" tooltip=\"\"></i></small>\n",
      "</br></td>\n",
      "<td class=\"nowrap\">713.4 MB</td>\n",
      "<td class=\"nowrap\"><span class=\"pull-right\"><a class=\"btn btn-xs btn-default\" href=\"/record/3966543/files/audio-14.tar.gz?download=1\"><i class=\"fa fa-download\"></i> Download</a></span></td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>\n",
      "<a class=\"filename\" href=\"/record/3966543/files/audio-15.tar.gz?download=1\">audio-15.tar.gz</a>\n",
      "<br><small class=\"text-muted nowrap\">md5:f6af9b65d876ef96d199b9e5f0473cb9 <i class=\"fa fa-question-circle text-muted\" data-placement=\"top\" data-toggle=\"tooltip\" title=\"This is the file fingerprint (MD5 checksum), which can be used to verify the file integrity.\" tooltip=\"\"></i></small>\n",
      "</br></td>\n",
      "<td class=\"nowrap\">718.1 MB</td>\n",
      "<td class=\"nowrap\"><span class=\"pull-right\"><a class=\"btn btn-xs btn-default\" href=\"/record/3966543/files/audio-15.tar.gz?download=1\"><i class=\"fa fa-download\"></i> Download</a></span></td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>\n",
      "<a class=\"filename\" href=\"/record/3966543/files/audio-16.tar.gz?download=1\">audio-16.tar.gz</a>\n",
      "<br><small class=\"text-muted nowrap\">md5:e8337c61a90c30989d500f950fbe443a <i class=\"fa fa-question-circle text-muted\" data-placement=\"top\" data-toggle=\"tooltip\" title=\"This is the file fingerprint (MD5 checksum), which can be used to verify the file integrity.\" tooltip=\"\"></i></small>\n",
      "</br></td>\n",
      "<td class=\"nowrap\">714.2 MB</td>\n",
      "<td class=\"nowrap\"><span class=\"pull-right\"><a class=\"btn btn-xs btn-default\" href=\"/record/3966543/files/audio-16.tar.gz?download=1\"><i class=\"fa fa-download\"></i> Download</a></span></td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>\n",
      "<a class=\"filename\" href=\"/record/3966543/files/audio-17.tar.gz?download=1\">audio-17.tar.gz</a>\n",
      "<br><small class=\"text-muted nowrap\">md5:3e47e85eb4564a30fe7f442ee97ec7b9 <i class=\"fa fa-question-circle text-muted\" data-placement=\"top\" data-toggle=\"tooltip\" title=\"This is the file fingerprint (MD5 checksum), which can be used to verify the file integrity.\" tooltip=\"\"></i></small>\n",
      "</br></td>\n",
      "<td class=\"nowrap\">718.9 MB</td>\n",
      "<td class=\"nowrap\"><span class=\"pull-right\"><a class=\"btn btn-xs btn-default\" href=\"/record/3966543/files/audio-17.tar.gz?download=1\"><i class=\"fa fa-download\"></i> Download</a></span></td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>\n",
      "<a class=\"filename\" href=\"/record/3966543/files/audio-18.tar.gz?download=1\">audio-18.tar.gz</a>\n",
      "<br><small class=\"text-muted nowrap\">md5:c6b2ef4d0d5b7269d465c469cdbbdc4b <i class=\"fa fa-question-circle text-muted\" data-placement=\"top\" data-toggle=\"tooltip\" title=\"This is the file fingerprint (MD5 checksum), which can be used to verify the file integrity.\" tooltip=\"\"></i></small>\n",
      "</br></td>\n",
      "<td class=\"nowrap\">365.9 MB</td>\n",
      "<td class=\"nowrap\"><span class=\"pull-right\"><a class=\"btn btn-xs btn-default\" href=\"/record/3966543/files/audio-18.tar.gz?download=1\"><i class=\"fa fa-download\"></i> Download</a></span></td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>\n",
      "<a class=\"filename\" href=\"/record/3966543/files/audio-2.tar.gz?download=1\">audio-2.tar.gz</a>\n",
      "<br><small class=\"text-muted nowrap\">md5:412241c063d7f196953d3dd1c44aeb5e <i class=\"fa fa-question-circle text-muted\" data-placement=\"top\" data-toggle=\"tooltip\" title=\"This is the file fingerprint (MD5 checksum), which can be used to verify the file integrity.\" tooltip=\"\"></i></small>\n",
      "</br></td>\n",
      "<td class=\"nowrap\">715.8 MB</td>\n",
      "<td class=\"nowrap\"><span class=\"pull-right\"><a class=\"btn btn-xs btn-default\" href=\"/record/3966543/files/audio-2.tar.gz?download=1\"><i class=\"fa fa-download\"></i> Download</a></span></td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>\n",
      "<a class=\"filename\" href=\"/record/3966543/files/audio-3.tar.gz?download=1\">audio-3.tar.gz</a>\n",
      "<br><small class=\"text-muted nowrap\">md5:d2a872392d95993a2c238d305b940812 <i class=\"fa fa-question-circle text-muted\" data-placement=\"top\" data-toggle=\"tooltip\" title=\"This is the file fingerprint (MD5 checksum), which can be used to verify the file integrity.\" tooltip=\"\"></i></small>\n",
      "</br></td>\n",
      "<td class=\"nowrap\">718.6 MB</td>\n",
      "<td class=\"nowrap\"><span class=\"pull-right\"><a class=\"btn btn-xs btn-default\" href=\"/record/3966543/files/audio-3.tar.gz?download=1\"><i class=\"fa fa-download\"></i> Download</a></span></td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>\n",
      "<a class=\"filename\" href=\"/record/3966543/files/audio-4.tar.gz?download=1\">audio-4.tar.gz</a>\n",
      "<br><small class=\"text-muted nowrap\">md5:b15b0e0cb3f8584259ef0d24293a9be3 <i class=\"fa fa-question-circle text-muted\" data-placement=\"top\" data-toggle=\"tooltip\" title=\"This is the file fingerprint (MD5 checksum), which can be used to verify the file integrity.\" tooltip=\"\"></i></small>\n",
      "</br></td>\n",
      "<td class=\"nowrap\">714.7 MB</td>\n",
      "<td class=\"nowrap\"><span class=\"pull-right\"><a class=\"btn btn-xs btn-default\" href=\"/record/3966543/files/audio-4.tar.gz?download=1\"><i class=\"fa fa-download\"></i> Download</a></span></td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>\n",
      "<a class=\"filename\" href=\"/record/3966543/files/audio-5.tar.gz?download=1\">audio-5.tar.gz</a>\n",
      "<br><small class=\"text-muted nowrap\">md5:de745f887067433757a2f2c8f99f99bb <i class=\"fa fa-question-circle text-muted\" data-placement=\"top\" data-toggle=\"tooltip\" title=\"This is the file fingerprint (MD5 checksum), which can be used to verify the file integrity.\" tooltip=\"\"></i></small>\n",
      "</br></td>\n",
      "<td class=\"nowrap\">719.4 MB</td>\n",
      "<td class=\"nowrap\"><span class=\"pull-right\"><a class=\"btn btn-xs btn-default\" href=\"/record/3966543/files/audio-5.tar.gz?download=1\"><i class=\"fa fa-download\"></i> Download</a></span></td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>\n",
      "<a class=\"filename\" href=\"/record/3966543/files/audio-6.tar.gz?download=1\">audio-6.tar.gz</a>\n",
      "<br><small class=\"text-muted nowrap\">md5:b0286c67468369d66336f6f5ddede31f <i class=\"fa fa-question-circle text-muted\" data-placement=\"top\" data-toggle=\"tooltip\" title=\"This is the file fingerprint (MD5 checksum), which can be used to verify the file integrity.\" tooltip=\"\"></i></small>\n",
      "</br></td>\n",
      "<td class=\"nowrap\">714.7 MB</td>\n",
      "<td class=\"nowrap\"><span class=\"pull-right\"><a class=\"btn btn-xs btn-default\" href=\"/record/3966543/files/audio-6.tar.gz?download=1\"><i class=\"fa fa-download\"></i> Download</a></span></td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>\n",
      "<a class=\"filename\" href=\"/record/3966543/files/audio-7.tar.gz?download=1\">audio-7.tar.gz</a>\n",
      "<br><small class=\"text-muted nowrap\">md5:ff300183ab7a9d3f2f74c3b730ffeb52 <i class=\"fa fa-question-circle text-muted\" data-placement=\"top\" data-toggle=\"tooltip\" title=\"This is the file fingerprint (MD5 checksum), which can be used to verify the file integrity.\" tooltip=\"\"></i></small>\n",
      "</br></td>\n",
      "<td class=\"nowrap\">718.4 MB</td>\n",
      "<td class=\"nowrap\"><span class=\"pull-right\"><a class=\"btn btn-xs btn-default\" href=\"/record/3966543/files/audio-7.tar.gz?download=1\"><i class=\"fa fa-download\"></i> Download</a></span></td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>\n",
      "<a class=\"filename\" href=\"/record/3966543/files/audio-8.tar.gz?download=1\">audio-8.tar.gz</a>\n",
      "<br><small class=\"text-muted nowrap\">md5:7be76b821fa6dbe20f6b50ca440e1024 <i class=\"fa fa-question-circle text-muted\" data-placement=\"top\" data-toggle=\"tooltip\" title=\"This is the file fingerprint (MD5 checksum), which can be used to verify the file integrity.\" tooltip=\"\"></i></small>\n",
      "</br></td>\n",
      "<td class=\"nowrap\">715.5 MB</td>\n",
      "<td class=\"nowrap\"><span class=\"pull-right\"><a class=\"btn btn-xs btn-default\" href=\"/record/3966543/files/audio-8.tar.gz?download=1\"><i class=\"fa fa-download\"></i> Download</a></span></td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>\n",
      "<a class=\"filename\" href=\"/record/3966543/files/audio-9.tar.gz?download=1\">audio-9.tar.gz</a>\n",
      "<br><small class=\"text-muted nowrap\">md5:959f7edfbb26eadad9865069f38aa9dd <i class=\"fa fa-question-circle text-muted\" data-placement=\"top\" data-toggle=\"tooltip\" title=\"This is the file fingerprint (MD5 checksum), which can be used to verify the file integrity.\" tooltip=\"\"></i></small>\n",
      "</br></td>\n",
      "<td class=\"nowrap\">715.3 MB</td>\n",
      "<td class=\"nowrap\"><span class=\"pull-right\"><a class=\"btn btn-xs btn-default\" href=\"/record/3966543/files/audio-9.tar.gz?download=1\"><i class=\"fa fa-download\"></i> Download</a></span></td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>\n",
      "<a class=\"filename\" href=\"/record/3966543/files/dcase-ust-taxonomy.yaml?download=1\">dcase-ust-taxonomy.yaml</a>\n",
      "<br><small class=\"text-muted nowrap\">md5:6c1cca1c4c383a6ebb0cb71cb74fe3a9 <i class=\"fa fa-question-circle text-muted\" data-placement=\"top\" data-toggle=\"tooltip\" title=\"This is the file fingerprint (MD5 checksum), which can be used to verify the file integrity.\" tooltip=\"\"></i></small>\n",
      "</br></td>\n",
      "<td class=\"nowrap\">1.1 kB</td>\n",
      "<td class=\"nowrap\"><span class=\"pull-right\"><a class=\"btn btn-xs btn-default\" href=\"/record/3966543/files/dcase-ust-taxonomy.yaml?download=1\"><i class=\"fa fa-download\"></i> Download</a></span></td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>\n",
      "<a class=\"filename\" href=\"/record/3966543/files/README.md?download=1\">README.md</a>\n",
      "<br><small class=\"text-muted nowrap\">md5:bdfa3a4c0d90053f622b52afa0fc86f7 <i class=\"fa fa-question-circle text-muted\" data-placement=\"top\" data-toggle=\"tooltip\" title=\"This is the file fingerprint (MD5 checksum), which can be used to verify the file integrity.\" tooltip=\"\"></i></small>\n",
      "</br></td>\n",
      "<td class=\"nowrap\">11.3 kB</td>\n",
      "<td class=\"nowrap\"><span class=\"pull-right\"><button class=\"btn preview-link btn-xs btn-default\" data-filename=\"README.md\"><i class=\"fa fa-eye\"></i> Preview</button> <a class=\"btn btn-xs btn-default\" href=\"/record/3966543/files/README.md?download=1\"><i class=\"fa fa-download\"></i> Download</a></span></td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>\n",
      "<a class=\"filename\" href=\"/record/3966543/files/unpack_audio.sh?download=1\">unpack_audio.sh</a>\n",
      "<br><small class=\"text-muted nowrap\">md5:c78525e474d920b4b6af6385ee94bdc9 <i class=\"fa fa-question-circle text-muted\" data-placement=\"top\" data-toggle=\"tooltip\" title=\"This is the file fingerprint (MD5 checksum), which can be used to verify the file integrity.\" tooltip=\"\"></i></small>\n",
      "</br></td>\n",
      "<td class=\"nowrap\">144 Bytes</td>\n",
      "<td class=\"nowrap\"><span class=\"pull-right\"><a class=\"btn btn-xs btn-default\" href=\"/record/3966543/files/unpack_audio.sh?download=1\"><i class=\"fa fa-download\"></i> Download</a></span></td>\n",
      "</tr></tbody>\n",
      "</table>\n",
      "</div>\n",
      "</div>\n",
      "<div id=\"citations\">\n",
      "<invenio-search disable-url-handler=\"true\" search-endpoint=\"https://zenodo-broker.web.cern.ch/api/relationships\" search-extra-params='{\"group_by\": \"version\", \"id\": \"10.5281/zenodo.2590741\", \"page\": 1, \"size\": 10}' search-headers='{\"Accept\": \"application/json\"}' search-hidden-params='{\"relation\": \"isCitedBy\", \"scheme\": \"doi\"}'>\n",
      "<div class=\"panel panel-default\" id=\"citation\">\n",
      "<div class=\"panel-heading\">\n",
      "<!-- Beta ribbon -->\n",
      "<div class=\"row\" style=\"margin-bottom: -15px;\">\n",
      "<div class=\"col-sm-1\">\n",
      "<div class=\"ribbon-wrapper-green\">\n",
      "<a href=\"https://help.zenodo.org/#citations\" rel=\"noopener noreferrer\" target=\"_blank\">\n",
      "<div class=\"corner-ribbon top-left ribbon-green\">Beta</div>\n",
      "</a>\n",
      "</div>\n",
      "</div>\n",
      "<div class=\"col-sm-11\" style=\"margin-top: 5px;\">\n",
      "<a class=\"panel-toggle\" data-toggle=\"collapse\" href=\"#collapseCitations\" style=\"margin-left:-25px;\">\n",
      "              Citations\n",
      "            </a>\n",
      "<a href=\"https://help.zenodo.org/#citations\" rel=\"noopener noreferrer\" target=\"_blank\">\n",
      "<i class=\"fa fa-question-circle\"></i>\n",
      "</a>\n",
      "<small class=\"text-muted\">\n",
      "<invenio-search-count template=\"/static/templates/citations/count.html\">\n",
      "</invenio-search-count>\n",
      "</small>\n",
      "<a class=\"panel-toggle\" data-toggle=\"collapse\" href=\"#collapseCitations\" style=\"margin-left:-25px;\">\n",
      "<span class=\"pull-right show-on-collapsed\"><i class=\"fa fa-chevron-right\"></i></span>\n",
      "<span class=\"pull-right hide-on-collapsed\"><i class=\"fa fa-chevron-down\"></i></span>\n",
      "</a>\n",
      "</div>\n",
      "</div>\n",
      "<!-- Without beta ribbon -->\n",
      "<!--<a class=\"panel-toggle\" data-toggle=\"collapse\" href=\"#collapseCitations\">-->\n",
      "<!--Citations-->\n",
      "<!--<small class=\"text-muted\">-->\n",
      "<!--<invenio-search-count-->\n",
      "<!--template=\"/static/templates/citations/count.html\">-->\n",
      "<!--</invenio-search-count>-->\n",
      "<!--</small>-->\n",
      "<!--<span class=\"pull-right show-on-collapsed\"><i class=\"fa fa-chevron-right\"></i></span>-->\n",
      "<!--<span class=\"pull-right hide-on-collapsed\"><i class=\"fa fa-chevron-down\"></i></span>-->\n",
      "<!--</a>-->\n",
      "</div>\n",
      "<div class=\"collapse in\" id=\"collapseCitations\">\n",
      "<div class=\"search-page\">\n",
      "<div class=\"container-fluid facets\">\n",
      "<div class=\"row\">\n",
      "<div class=\"col-sm-9\">\n",
      "<invenio-search-facets ng-init=\"vm.record_id='10.5281/zenodo.3966543'; vm.version_id='10.5281/zenodo.2590741'\" template=\"/static/templates/citations/facets.html\">\n",
      "</invenio-search-facets>\n",
      "</div>\n",
      "<div class=\"col-sm-3\">\n",
      "<invenio-search-bar placeholder=\"Search\" template=\"/static/templates/invenio_search_ui/searchbar.html\">\n",
      "</invenio-search-bar>\n",
      "</div>\n",
      "</div>\n",
      "</div>\n",
      "<div class=\"container-fluid\">\n",
      "<invenio-search-error message=\"Error\" template=\"/static/templates/zenodo_search_ui/error.html\">\n",
      "</invenio-search-error>\n",
      "</div>\n",
      "<invenio-search-results template=\"/static/templates/citations/results.html\">\n",
      "</invenio-search-results>\n",
      "<div class=\"row\">\n",
      "<div class=\"col-md-2 col-sm-12\">\n",
      "</div>\n",
      "<div class=\"col-md-7 col-sm-12 text-center\">\n",
      "<invenio-search-pagination template=\"/static/templates/citations/pagination.html\">\n",
      "</invenio-search-pagination>\n",
      "</div>\n",
      "<div class=\"col-md-3 col-sm-12\" style=\"padding-top: 5px;\">\n",
      "<invenio-search-select-box available-options='{\n",
      "                  \"options\": [\n",
      "                     {\n",
      "                       \"title\": \"10\",\n",
      "                       \"value\": \"10\"\n",
      "                     },\n",
      "                     {\n",
      "                       \"title\": \"20\",\n",
      "                       \"value\": \"20\"\n",
      "                     },\n",
      "                     {\n",
      "                       \"title\": \"50\",\n",
      "                       \"value\": \"50\"\n",
      "                     }\n",
      "                  ]}' sort-key=\"size\" template=\"/static/templates/citations/selectbox.html\">\n",
      "</invenio-search-select-box>\n",
      "</div>\n",
      "</div>\n",
      "</div>\n",
      "</div>\n",
      "</div>\n",
      "</invenio-search>\n",
      "</div>\n",
      "</div>\n",
      "<div class=\"col-sm-4 col-md-4 col-right\">\n",
      "<div class=\"well\"><!-- Stats -->\n",
      "<div class=\"row stats-box\">\n",
      "<div id=\"accordion\">\n",
      "<!-- Banner --><div class=\"row\">\n",
      "<div class=\"col-sm-6\">\n",
      "<span class=\"stats-data\">9,876</span>\n",
      "</div>\n",
      "<div class=\"col-sm-6\">\n",
      "<span class=\"stats-data\">19,828</span>\n",
      "</div>\n",
      "</div>\n",
      "<div class=\"row\">\n",
      "<div class=\"col-sm-6\">\n",
      "<i class=\"fa fa-eye\" data-original-title=\"Total views.\" data-placement=\"top\" data-toggle=\"tooltip\" title=\"Total views.\" tooltip=\"\"></i> views\n",
      "      </div>\n",
      "<div class=\"col-sm-6\">\n",
      "<i class=\"fa fa-download\" data-original-title=\"Total downloads.\" data-placement=\"top\" data-toggle=\"tooltip\" title=\"Total downloads.\" tooltip=\"\"></i> downloads\n",
      "      </div>\n",
      "</div>\n",
      "<!-- Collapsed details -->\n",
      "<div class=\"row\" id=\"toggle-stats\">\n",
      "<div class=\"col-sm-12\">\n",
      "<a aria-controls=\"collapse-stats\" aria-expanded=\"true\" class=\"panel-toggle\" data-target=\"#collapse-stats\" data-toggle=\"collapse\" style=\"cursor: pointer;\">\n",
      "          See more details...\n",
      "        </a>\n",
      "</div>\n",
      "</div>\n",
      "<div aria-labelledby=\"toggle-stats\" class=\"collapse\" data-parent=\"#accordion\" id=\"collapse-stats\">\n",
      "<table class=\"table stats-table\">\n",
      "<!-- Skip table header if no versions --><tr>\n",
      "<th></th>\n",
      "<th>All versions</th>\n",
      "<th>This version</th>\n",
      "</tr><tr>\n",
      "<td>Views <i class=\"fa fa-question-circle text-muted\" data-original-title=\"Total views.\" data-placement=\"top\" data-toggle=\"tooltip\" title=\"Total views.\" tooltip=\"\"></i></td><td>9,876</td><td>1,939</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Downloads <i class=\"fa fa-question-circle text-muted\" data-original-title=\"Total downloads.\" data-placement=\"top\" data-toggle=\"tooltip\" title=\"Total downloads.\" tooltip=\"\"></i></td><td>19,828</td><td>3,279</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Data volume <i class=\"fa fa-question-circle text-muted\" data-original-title=\"Total download volume.\" data-placement=\"top\" data-toggle=\"tooltip\" title=\"Total download volume.\" tooltip=\"\"></i></td><td>63.6 TB</td><td>1.8 TB</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Unique views <i class=\"fa fa-question-circle text-muted\" data-original-title=\"Views in one hour user-sessions are counted only once.\" data-placement=\"top\" data-toggle=\"tooltip\" title=\"Views in one hour user-sessions are counted only once.\" tooltip=\"\"></i></td><td>7,541</td><td>1,647</td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>Unique downloads <i class=\"fa fa-question-circle text-muted\" data-original-title=\"Downloads in one hour user-sessions are counted only once.\" data-placement=\"top\" data-toggle=\"tooltip\" title=\"Downloads in one hour user-sessions are counted only once.\" tooltip=\"\"></i></td><td>5,698</td><td>704</td>\n",
      "</tr>\n",
      "</table>\n",
      "<div class=\"row\">\n",
      "<a href=\"https://help.zenodo.org/#statistics\">More info on how stats are collected.</a>\n",
      "</div>\n",
      "</div>\n",
      "</div>\n",
      "</div>\n",
      "</div>\n",
      "<div class=\"well metadata\">\n",
      "<small class=\"text-muted\">Indexed in</small>\n",
      "<a href=\"https://explore.openaire.eu/search/dataset?pid=10.5281/zenodo.3966543\">\n",
      "<img class=\"img-thumbnail\" src=\"/static/img/openaire-horizontal-old.png\" width=\"100%\"/>\n",
      "</a>\n",
      "</div>\n",
      "<div class=\"well metadata\"><dl>\n",
      "<dt>Publication date:</dt>\n",
      "<dd>September 14, 2020</dd>\n",
      "<dt>DOI:</dt>\n",
      "<dd>\n",
      "<span class=\"get-badge\" data-placement=\"bottom\" data-toggle=\"`tooltip\" title=\"Get the DOI badge!\">\n",
      "<img alt=\"10.5281/zenodo.3966543\" data-target=\"[data-modal='10.5281/zenodo.3966543']\" data-toggle=\"modal\" src=\"/badge/DOI/10.5281/zenodo.3966543.svg\"/>\n",
      "</span>\n",
      "<div class=\"modal fade badge-modal\" data-modal=\"10.5281/zenodo.3966543\">\n",
      "<div class=\"modal-dialog\">\n",
      "<div class=\"modal-content\">\n",
      "<div class=\"modal-body\">\n",
      "<h4>Zenodo DOI Badge</h4>\n",
      "<h4>\n",
      "<small>DOI</small>\n",
      "</h4>\n",
      "<h4>\n",
      "<pre>10.5281/zenodo.3966543</pre>\n",
      "</h4>\n",
      "<h4>\n",
      "<small>Markdown</small>\n",
      "</h4>\n",
      "<h4>\n",
      "<pre>[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3966543.svg)](https://doi.org/10.5281/zenodo.3966543)</pre>\n",
      "</h4>\n",
      "<h4>\n",
      "<small>reStructedText</small>\n",
      "</h4>\n",
      "<h4>\n",
      "<pre>.. image:: https://zenodo.org/badge/DOI/10.5281/zenodo.3966543.svg\n",
      "   :target: https://doi.org/10.5281/zenodo.3966543</pre>\n",
      "</h4>\n",
      "<h4>\n",
      "<small>HTML</small>\n",
      "</h4>\n",
      "<h4>\n",
      "<pre>&lt;a href=\"https://doi.org/10.5281/zenodo.3966543\"&gt;&lt;img src=\"https://zenodo.org/badge/DOI/10.5281/zenodo.3966543.svg\" alt=\"DOI\"&gt;&lt;/a&gt;</pre>\n",
      "</h4>\n",
      "<h4>\n",
      "<small>Image URL</small>\n",
      "</h4>\n",
      "<h4>\n",
      "<pre>https://zenodo.org/badge/DOI/10.5281/zenodo.3966543.svg</pre>\n",
      "</h4>\n",
      "<h4>\n",
      "<small>Target URL</small>\n",
      "</h4>\n",
      "<h4>\n",
      "<pre>https://doi.org/10.5281/zenodo.3966543</pre>\n",
      "</h4>\n",
      "</div>\n",
      "</div>\n",
      "</div>\n",
      "</div>\n",
      "</dd>\n",
      "<dt>Keyword(s):</dt>\n",
      "<dd>\n",
      "<a class=\"label-link\" href=\"/search?q=keywords%3A%22urban+sound%22\">\n",
      "<span class=\"label label-default\">urban sound</span>\n",
      "</a>\n",
      "<a class=\"label-link\" href=\"/search?q=keywords%3A%22noise+pollution%22\">\n",
      "<span class=\"label label-default\">noise pollution</span>\n",
      "</a>\n",
      "<a class=\"label-link\" href=\"/search?q=keywords%3A%22machine+listening%22\">\n",
      "<span class=\"label label-default\">machine listening</span>\n",
      "</a>\n",
      "<a class=\"label-link\" href=\"/search?q=keywords%3A%22computer+audition%22\">\n",
      "<span class=\"label label-default\">computer audition</span>\n",
      "</a>\n",
      "<a class=\"label-link\" href=\"/search?q=keywords%3A%22longterm+spatiotemporal+context%22\">\n",
      "<span class=\"label label-default\">longterm spatiotemporal context</span>\n",
      "</a>\n",
      "<a class=\"label-link\" href=\"/search?q=keywords%3A%22sound+tagging%22\">\n",
      "<span class=\"label label-default\">sound tagging</span>\n",
      "</a>\n",
      "</dd>\n",
      "<dt>Communities:</dt>\n",
      "<dd>\n",
      "<ul class=\"list-unstyled\">\n",
      "<li><a href=\"/communities/dcase/\">Detection and Classification of Acoustic Scenes and Events</a></li>\n",
      "</ul>\n",
      "</dd>\n",
      "<dt>License (for files):</dt>\n",
      "<dd><a href=\"https://creativecommons.org/licenses/by/4.0/legalcode\" rel=\"license\"><i class=\"fa fa-external-link\"></i> Creative Commons Attribution 4.0 International</a></dd>\n",
      "</dl>\n",
      "</div>\n",
      "<div class=\"well metadata\">\n",
      "<h4>Versions</h4>\n",
      "<table class=\"table\">\n",
      "<tr class=\"info\">\n",
      "<td>\n",
      "<a href=\"/record/3966543\">Version 2.3.0 </a>\n",
      "<small class=\"text-muted\">10.5281/zenodo.3966543</small>\n",
      "</td>\n",
      "<td align=\"right\"><small class=\"text-muted\">Sep 14, 2020</small></td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>\n",
      "<a href=\"/record/3873076\">Version 2.2.0 </a>\n",
      "<small class=\"text-muted\">10.5281/zenodo.3873076</small>\n",
      "</td>\n",
      "<td align=\"right\"><small class=\"text-muted\">Jun 2, 2020</small></td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>\n",
      "<a href=\"/record/3693077\">Version 2.1.0 </a>\n",
      "<small class=\"text-muted\">10.5281/zenodo.3693077</small>\n",
      "</td>\n",
      "<td align=\"right\"><small class=\"text-muted\">Mar 1, 2020</small></td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>\n",
      "<a href=\"/record/3692954\">Version 1.0.0 </a>\n",
      "<small class=\"text-muted\">10.5281/zenodo.3692954</small>\n",
      "</td>\n",
      "<td align=\"right\"><small class=\"text-muted\">Feb 29, 2020</small></td>\n",
      "</tr>\n",
      "<tr>\n",
      "<td>\n",
      "<a href=\"/record/3338310\">Version 0.4.0 </a>\n",
      "<small class=\"text-muted\">10.5281/zenodo.3338310</small>\n",
      "</td>\n",
      "<td align=\"right\"><small class=\"text-muted\">Jul 16, 2019</small></td>\n",
      "</tr>\n",
      "<tr><td align=\"center\" colspan=\"2\"><a href=\"/search?q=conceptrecid%3A%222590741%22&amp;sort=-version&amp;all_versions=True\"><small>View all 8 versions</small></a></td></tr>\n",
      "</table>\n",
      "<small>\n",
      "<strong>Cite all versions?</strong> You can cite all versions by using the DOI <a href=\"https://doi.org/10.5281/zenodo.2590741\">10.5281/zenodo.2590741</a>. This DOI represents all versions, and will always resolve to the latest one. <a href=\"http://help.zenodo.org/#versioning\">Read more</a>.\n",
      "    </small>\n",
      "</div>\n",
      "<div class=\"well\"><h4>Share</h4>\n",
      "<!-- AddThis Button BEGIN -->\n",
      "<div addthis:url=\"https://doi.org/10.5281/zenodo.3966543\" class=\"addthis_toolbox addthis_default_style addthis_32x32_style\">\n",
      "<a class=\"addthis_button_mendeley\"></a>\n",
      "<a class=\"addthis_button_citeulike\"></a>\n",
      "<a class=\"addthis_button_twitter\"></a>\n",
      "<a class=\"addthis_button_preferred_1\"></a>\n",
      "<a class=\"addthis_button_preferred_2\"></a>\n",
      "<a class=\"addthis_button_compact\"></a>\n",
      "</div>\n",
      "<!-- AddThis Button END -->\n",
      "<h4>Cite as</h4>\n",
      "<div id=\"invenio-csl\">\n",
      "<invenio-csl ng-init=\"vm.citationResult = 'Mark Cartwright, Jason Cramer, Ana Elisa Mendez Mendez, Yu Wang, Ho-Hsiang Wu, Vincent Lostanlen, Magdalena Fuentes, Graham Dove, Charlie Mydlarz, Justin Salamon, Oded Nov, &amp; Juan Pablo Bello. (2020). SONYC Urban Sound Tagging (SONYC-UST): a multilabel dataset from an urban acoustic sensor network (2.3.0) [Data set]. Zenodo. https://doi.org/10.5281/zenodo.3966543'\">\n",
      "<invenio-csl-citeproc endpoint=\"/api/records/3966543\" template=\"/static/templates/invenio_csl/citeproc.html\"></invenio-csl-citeproc>\n",
      "<invenio-csl-error template=\"/static/node_modules/invenio-csl-js/dist/templates/error.html\"></invenio-csl-error>\n",
      "<invenio-csl-loading template=\"/static/node_modules/invenio-csl-js/dist/templates/loading.html\"></invenio-csl-loading>\n",
      "<invenio-csl-typeahead item-template=\"/static/templates/invenio_csl/item.html\" lazy=\"true\" placeholder=\"Start typing a citation style...\" remote=\"/api/csl/styles\" template=\"/static/node_modules/invenio-csl-js/dist/templates/typeahead.html\">\n",
      "</invenio-csl-typeahead>\n",
      "</invenio-csl>\n",
      "</div>\n",
      "</div>\n",
      "<div class=\"well\"><h4>Export</h4>\n",
      "<ul class=\"list-inline\">\n",
      "<li><a href=\"/record/3966543/export/hx\">BibTeX</a></li>\n",
      "<li><a href=\"/record/3966543/export/csl\">CSL</a></li>\n",
      "<li><a href=\"/record/3966543/export/dcite4\">DataCite</a></li>\n",
      "<li><a href=\"/record/3966543/export/xd\">Dublin Core</a></li>\n",
      "<li><a href=\"/record/3966543/export/dcat\">DCAT</a></li>\n",
      "<li><a href=\"/record/3966543/export/json\">JSON</a></li>\n",
      "<li><a href=\"/record/3966543/export/schemaorg_jsonld\">JSON-LD</a></li>\n",
      "<li><a href=\"/record/3966543/export/geojson\">GeoJSON</a></li>\n",
      "<li><a href=\"/record/3966543/export/xm\">MARCXML</a></li>\n",
      "<li><a href=\"https://www.mendeley.com/import/?url=https://zenodo.org/record/3966543\"><i class=\"fa fa-external-link\"></i> Mendeley</a></li>\n",
      "</ul>\n",
      "</div>\n",
      "</div>\n",
      "</div>\n",
      "</div>\n",
      "<footer class=\"footer\">\n",
      "<div class=\"menu-wrapper\">\n",
      "<div class=\"container\">\n",
      "<div class=\"row footer-menu\">\n",
      "<div class=\"col-xs-12 col-md-8\">\n",
      "<div class=\"row\">\n",
      "<div class=\"col-xs-2 col-md-2\">\n",
      "<h5>About</h5>\n",
      "<ul class=\"list-unstyled\">\n",
      "<li><a href=\"http://about.zenodo.org\">About</a></li>\n",
      "<li><a href=\"http://about.zenodo.org/policies\">Policies</a></li>\n",
      "<li><a href=\"http://about.zenodo.org/infrastructure\">Infrastructure</a></li>\n",
      "<li><a href=\"http://about.zenodo.org/principles\">Principles</a></li>\n",
      "<li><a href=\"http://about.zenodo.org/contact\">Contact</a></li>\n",
      "</ul>\n",
      "</div>\n",
      "<div class=\"col-xs-2 col-md-2\">\n",
      "<h5>Blog</h5>\n",
      "<ul class=\"list-unstyled\">\n",
      "<li><a href=\"http://blog.zenodo.org\">Blog</a></li>\n",
      "</ul>\n",
      "</div>\n",
      "<div class=\"col-xs-2 col-md-2\">\n",
      "<h5>Help</h5>\n",
      "<ul class=\"list-unstyled\">\n",
      "<li><a href=\"http://help.zenodo.org\">FAQ</a></li>\n",
      "<li><a href=\"http://help.zenodo.org/features\">Features</a></li>\n",
      "<li><a href=\"/support\">Support</a></li>\n",
      "</ul>\n",
      "</div>\n",
      "<div class=\"col-xs-2 col-md-2\">\n",
      "<h5>Developers</h5>\n",
      "<ul class=\"list-unstyled\">\n",
      "<li><a href=\"http://developers.zenodo.org\">REST API</a></li>\n",
      "<li><a href=\"http://developers.zenodo.org#oai-pmh\">OAI-PMH</a></li>\n",
      "</ul>\n",
      "</div>\n",
      "<div class=\"col-xs-2 col-md-2\">\n",
      "<h5>Contribute</h5>\n",
      "<ul class=\"list-unstyled\">\n",
      "<li><a href=\"https://github.com/zenodo/zenodo\"><i class=\"fa fa-external-link\"></i> GitHub</a></li>\n",
      "<li><a href=\"/donate\"><i class=\"fa fa-external-link\"></i> Donate</a></li>\n",
      "</ul>\n",
      "</div>\n",
      "</div>\n",
      "</div>\n",
      "<div class=\"col-xs-12 col-md-4\">\n",
      "<div class=\"pull-right-md text-center-sm text-center-xs\">\n",
      "<h5>Funded by</h5>\n",
      "<ul class=\"list-inline\">\n",
      "<li><a href=\"https://home.cern\"><img height=\"60\" src=\"/static/img/cern.svg\" width=\"60\"/></a></li>\n",
      "<li><a href=\"https://www.openaire.eu\"><img src=\"/static/img/openaire.png\" width=\"80\"/></a></li>\n",
      "<li><a href=\"https://ec.europa.eu/programmes/horizon2020/\"><img height=\"60\" src=\"/static/img/eu.png\" width=\"88\"/></a></li>\n",
      "</ul>\n",
      "</div>\n",
      "</div>\n",
      "</div>\n",
      "</div>\n",
      "</div>\n",
      "<div class=\"container\">\n",
      "<div class=\"row\">\n",
      "<div class=\"col-xs-12 col-sm-6 col-sm-push-6\">\n",
      "<div class=\"pull-right-sm text-center-xs\">\n",
      "<ul class=\"list-inline\">\n",
      "<li><a href=\"https://stats.uptimerobot.com/vlYOVuWgM\">Status</a></li>\n",
      "<li><a href=\"http://about.zenodo.org/privacy-policy\">Privacy policy</a></li>\n",
      "<li><a href=\"http://about.zenodo.org/terms\">Terms of Use</a></li>\n",
      "<li><a href=\"/support\">Support</a></li>\n",
      "</ul>\n",
      "</div>\n",
      "</div>\n",
      "<div class=\"col-xs-12 col-sm-6 col-sm-pull-6 text-center-xs\">\n",
      "<p><a href=\"http://creativecommons.org/licenses/by/4.0/\" rel=\"license\" title=\"Except where otherwise noted, content on this site is licensed under a Creative Commons Attribution 4.0 International License.\"><img alt=\"Creative Commons Licence\" height=\"20\" src=\"https://i.creativecommons.org/l/by/4.0/88x31.png\"/></a>  Powered by <a href=\"https://home.cern/science/computing/data-centre\">CERN Data Centre</a> &amp; <a href=\"http://inveniosoftware.org\">Invenio</a>.</p>\n",
      "</div>\n",
      "</div>\n",
      "</div>\n",
      "</footer>\n",
      "<script type=\"application/ld+json\">{\"@context\": \"https://schema.org/\", \"@id\": \"https://doi.org/10.5281/zenodo.3966543\", \"@type\": \"Dataset\", \"creator\": [{\"@id\": \"https://orcid.org/0000-0002-5908-390X\", \"@type\": \"Person\", \"affiliation\": \"New York University\", \"name\": \"Mark Cartwright\"}, {\"@id\": \"https://orcid.org/0000-0001-5288-9399\", \"@type\": \"Person\", \"affiliation\": \"New York University\", \"name\": \"Jason Cramer\"}, {\"@id\": \"https://orcid.org/0000-0002-4861-5616\", \"@type\": \"Person\", \"affiliation\": \"New York University\", \"name\": \"Ana Elisa Mendez Mendez\"}, {\"@id\": \"https://orcid.org/0000-0002-1615-5141\", \"@type\": \"Person\", \"affiliation\": \"New York University\", \"name\": \"Yu Wang\"}, {\"@id\": \"https://orcid.org/0000-0002-1102-074X\", \"@type\": \"Person\", \"affiliation\": \"New York University\", \"name\": \"Ho-Hsiang Wu\"}, {\"@id\": \"https://orcid.org/0000-0003-0580-1651\", \"@type\": \"Person\", \"affiliation\": \"Cornel University\", \"name\": \"Vincent Lostanlen\"}, {\"@id\": \"https://orcid.org/0000-0003-4506-6639\", \"@type\": \"Person\", \"affiliation\": \"New York University\", \"name\": \"Magdalena Fuentes\"}, {\"@id\": \"https://orcid.org/0000-0002-3551-0209\", \"@type\": \"Person\", \"affiliation\": \"New York University\", \"name\": \"Graham Dove\"}, {\"@id\": \"https://orcid.org/0000-0001-7061-0638\", \"@type\": \"Person\", \"affiliation\": \"New York University\", \"name\": \"Charlie Mydlarz\"}, {\"@id\": \"https://orcid.org/0000-0001-6345-4593\", \"@type\": \"Person\", \"affiliation\": \"New York University\", \"name\": \"Justin Salamon\"}, {\"@id\": \"https://orcid.org/0000-0001-6410-2995\", \"@type\": \"Person\", \"affiliation\": \"New York University\", \"name\": \"Oded Nov\"}, {\"@id\": \"https://orcid.org/0000-0001-8561-5204\", \"@type\": \"Person\", \"affiliation\": \"New York University\", \"name\": \"Juan Pablo Bello\"}], \"datePublished\": \"2020-09-14\", \"description\": \"\\u003cp\\u003e\\u003cstrong\\u003eSONYC Urban Sound Tagging (SONYC-UST): a multilabel dataset from an urban acoustic sensor network\\u003c/strong\\u003e\\u003c/p\\u003e\\n\\n\\u003cp\\u003eVersion 2.3, September 2020\\u003c/p\\u003e\\n\\n\\u003cp\\u003e\\u0026nbsp;\\u003c/p\\u003e\\n\\n\\u003cp\\u003e\\u003cstrong\\u003eCreated by\\u003c/strong\\u003e\\u003c/p\\u003e\\n\\n\\u003cp\\u003eMark Cartwright (1,2,3), Jason Cramer (1), Ana Elisa Mendez Mendez (1), Yu Wang (1), Ho-Hsiang Wu (1), Vincent Lostanlen (1,2,4), Magdalena Fuentes (1), Graham Dove (2), Charlie Mydlarz (1,2), Justin Salamon (5), Oded Nov (6), Juan Pablo Bello (1,2,3)\\u003c/p\\u003e\\n\\n\\u003col\\u003e\\n\\t\\u003cli\\u003eMusic and Audio Research Lab, New York University\\u003c/li\\u003e\\n\\t\\u003cli\\u003eCenter for Urban Science and Progress, New York University\\u003c/li\\u003e\\n\\t\\u003cli\\u003eDepartment of Computer Science and Engineering, New York University\\u003c/li\\u003e\\n\\t\\u003cli\\u003eCornell Lab of Ornithology\\u003c/li\\u003e\\n\\t\\u003cli\\u003eAdobe Research\\u003c/li\\u003e\\n\\t\\u003cli\\u003eDepartment of Technology Management and Innovation, New York University\\u003c/li\\u003e\\n\\u003c/ol\\u003e\\n\\n\\u003cp\\u003e\\u0026nbsp;\\u003c/p\\u003e\\n\\n\\u003cp\\u003e\\u003cstrong\\u003ePublication\\u003c/strong\\u003e\\u003c/p\\u003e\\n\\n\\u003cp\\u003eIf using this data in an academic work, please reference the DOI and version, as well as cite the following paper, which presented the data collection procedure and the first version of the dataset:\\u003c/p\\u003e\\n\\n\\u003cp\\u003eCartwright, M., Cramer, J., Mendez, A.E.M., Wang, Y., Wu, H., Lostanlen, V., Fuentes, M., Dove, G., Mydlarz, C., Salamon, J., Nov, O., Bello, J.P. SONYC-UST-V2: An Urban Sound Tagging Dataset with Spatiotemporal Context. In \\u003cem\\u003eProceedings of the Workshop on Detection and Classification of Acoustic Scenes and Events (DCASE)\\u003c/em\\u003e, 2020.\\u003cbr\\u003e\\n\\u003ca href=\\\"https://arxiv.org/abs/2009.05188\\\"\\u003e[pdf]\\u003c/a\\u003e\\u003c/p\\u003e\\n\\n\\u003cp\\u003e\\u0026nbsp;\\u003c/p\\u003e\\n\\n\\u003cp\\u003e\\u003cstrong\\u003eDescription\\u003c/strong\\u003e\\u003c/p\\u003e\\n\\n\\u003cp\\u003eSONYC Urban Sound Tagging (SONYC-UST) is a dataset for the development and evaluation of machine listening systems for realistic urban noise monitoring. The audio was recorded from the \\u003ca href=\\\"https://wp.nyu.edu/sonyc\\\"\\u003eSONYC\\u003c/a\\u003e\\u0026nbsp;acoustic sensor network. Volunteers on the \\u0026nbsp;\\u003ca href=\\\"https://zooniverse.org\\\"\\u003eZooniverse\\u003c/a\\u003e\\u0026nbsp;citizen science platform tagged the presence of 23 classes that were chosen in consultation with the New York City Department of Environmental Protection. These 23 fine-grained classes can be grouped into 8 coarse-grained classes. The recordings are split into three sets: training, validation, and test. The training and validation sets are disjoint with respect to the sensor from which each recording came, and the test set is displaced in time. For increased reliability, three volunteers annotated each recording. In addition, members of the SONYC team subsequently created a subset of verified, ground-truth tags using a two-stage annotation procedure in which two annotators independently tagged and then collectively resolved any disagreements. This subset of recordings with verified annotations intersects with all three recording splits. All of the recordings in the test set have these verified annotations.\\u0026nbsp; In v2 version of this dataset, we have also included coarse spatiotemporal context information to aid in tag prediction when time and location is known. For more details on the motivation and creation of this dataset see the \\u003ca href=\\\"http://dcase.community/challenge2020/task-urban-sound-tagging-with-spatiotemporal-context\\\"\\u003eDCASE 2020 Urban Sound Tagging with Spatiotemporal Context Task website\\u003c/a\\u003e.\\u003c/p\\u003e\\n\\n\\u003cp\\u003e\\u0026nbsp;\\u003c/p\\u003e\\n\\n\\u003cp\\u003e\\u003cstrong\\u003eAudio data\\u003c/strong\\u003e\\u003c/p\\u003e\\n\\n\\u003cp\\u003eThe provided audio has been acquired using the SONYC acoustic sensor network for urban noise pollution monitoring. Over 60 different sensors have been deployed in New York City, and these sensors have collectively gathered the equivalent of over 50 years of audio data, of which we provide a small subset. The data was sampled by selecting the nearest neighbors on VGGish features of recordings known to have classes of interest. All recordings are 10 seconds and were recorded with identical microphones at identical gain settings. To maintain privacy, we quantized the spatial information to the level of a city block, and we quantized the temporal information to the level of an hour. We also limited the occurrence of recordings with positive human voice annotations to one per hour per sensor.\\u003c/p\\u003e\\n\\n\\u003cp\\u003e\\u0026nbsp;\\u003c/p\\u003e\\n\\n\\u003cp\\u003e\\u003cstrong\\u003eLabel taxonomy\\u003c/strong\\u003e\\u003c/p\\u003e\\n\\n\\u003cp\\u003eThe label taxonomy is as follows:\\u003c/p\\u003e\\n\\n\\u003col\\u003e\\n\\t\\u003cli\\u003eengine\\u003cbr\\u003e\\n\\t1: small-sounding-engine\\u003cbr\\u003e\\n\\t2: medium-sounding-engine\\u003cbr\\u003e\\n\\t3: large-sounding-engine\\u003cbr\\u003e\\n\\tX: engine-of-uncertain-size\\u003c/li\\u003e\\n\\t\\u003cli\\u003emachinery-impact\\u003cbr\\u003e\\n\\t1: rock-drill\\u003cbr\\u003e\\n\\t2: jackhammer\\u003cbr\\u003e\\n\\t3: hoe-ram\\u003cbr\\u003e\\n\\t4: pile-driver\\u003cbr\\u003e\\n\\tX: other-unknown-impact-machinery\\u003c/li\\u003e\\n\\t\\u003cli\\u003enon-machinery-impact\\u003cbr\\u003e\\n\\t1: non-machinery-impact\\u003c/li\\u003e\\n\\t\\u003cli\\u003epowered-saw\\u003cbr\\u003e\\n\\t1: chainsaw\\u003cbr\\u003e\\n\\t2: small-medium-rotating-saw\\u003cbr\\u003e\\n\\t3: large-rotating-saw\\u003cbr\\u003e\\n\\tX: other-unknown-powered-saw\\u003c/li\\u003e\\n\\t\\u003cli\\u003ealert-signal\\u003cbr\\u003e\\n\\t1: car-horn\\u003cbr\\u003e\\n\\t2: car-alarm\\u003cbr\\u003e\\n\\t3: siren\\u003cbr\\u003e\\n\\t4: reverse-beeper\\u003cbr\\u003e\\n\\tX: other-unknown-alert-signal\\u003c/li\\u003e\\n\\t\\u003cli\\u003emusic\\u003cbr\\u003e\\n\\t1: stationary-music\\u003cbr\\u003e\\n\\t2: mobile-music\\u003cbr\\u003e\\n\\t3: ice-cream-truck\\u003cbr\\u003e\\n\\tX: music-from-uncertain-source\\u003c/li\\u003e\\n\\t\\u003cli\\u003ehuman-voice\\u003cbr\\u003e\\n\\t1: person-or-small-group-talking\\u003cbr\\u003e\\n\\t2: person-or-small-group-shouting\\u003cbr\\u003e\\n\\t3: large-crowd\\u003cbr\\u003e\\n\\t4: amplified-speech\\u003cbr\\u003e\\n\\tX: other-unknown-human-voice\\u003c/li\\u003e\\n\\t\\u003cli\\u003edog\\u003cbr\\u003e\\n\\t1: dog-barking-whining\\u003c/li\\u003e\\n\\u003c/ol\\u003e\\n\\n\\u003cp\\u003eThe classes preceded by an \\u003ccode\\u003eX\\u003c/code\\u003e code indicate when an annotator was able to identify the coarse class, but couldn\\u0026rsquo;t identify the fine class because either they were uncertain which fine class it was or the fine class was not included in the taxonomy. \\u003ccode\\u003edcase-ust-taxonomy.yaml\\u003c/code\\u003e contains this taxonomy in an easily machine-readable form.\\u003c/p\\u003e\\n\\n\\u003cp\\u003e\\u0026nbsp;\\u003c/p\\u003e\\n\\n\\u003cp\\u003e\\u003cstrong\\u003eData splits\\u003c/strong\\u003e\\u003c/p\\u003e\\n\\n\\u003cp\\u003eThis release contains a training subset (13538 recordings from 35 sensors), and validation subset (4308 recordings from 9 sensors), and a test subset (669 recordings from 48 sensors). The training and validation subsets are disjoint with respect to the sensor from which each recording came. The sensors in the test set will not disjoint from the training and validation subsets, but the test recordings are displaced in time, occurring after any of the recordings in the training and validation subset. The subset of recordings with verified annotations (1380 recordings) intersects with all three recording splits.\\u0026nbsp; All of the recordings in the test set have these verified annotations.\\u003c/p\\u003e\\n\\n\\u003cp\\u003e\\u0026nbsp;\\u003c/p\\u003e\\n\\n\\u003cp\\u003e\\u003cstrong\\u003eAnnotation data\\u003c/strong\\u003e\\u003c/p\\u003e\\n\\n\\u003cp\\u003eThe annotation data are\\u0026nbsp;contained in \\u003ccode\\u003eannotations.csv\\u003c/code\\u003e, and\\u0026nbsp;encompass the training, validation, and test subsets. Each row in the file represents one multi-label annotation of a recording\\u0026mdash;it could be the annotation of a single citizen science volunteer, a single SONYC team member, or the agreed-upon ground truth by the SONYC team (see the \\u003cem\\u003eannotator_id\\u003c/em\\u003e column description for more information).\\u0026nbsp; Note that since the SONYC team members annotated each class group separately, there may be multiple annotation rows by a single SONYC team annotator for a particular audio recording.\\u003c/p\\u003e\\n\\n\\u003cp\\u003e\\u0026nbsp;\\u003c/p\\u003e\\n\\n\\u003cp\\u003e\\u0026nbsp;\\u003c/p\\u003e\\n\\n\\u003cp\\u003e\\u003cstrong\\u003eColumns\\u003c/strong\\u003e\\u003c/p\\u003e\\n\\n\\u003cp\\u003e\\u003cem\\u003esplit\\u003c/em\\u003e\\u003c/p\\u003e\\n\\n\\u003cp\\u003eThe data split. (\\u003cem\\u003etrain\\u003c/em\\u003e, \\u003cem\\u003evalidate, test\\u003c/em\\u003e)\\u003c/p\\u003e\\n\\n\\u003cp\\u003e\\u003cem\\u003esensor_id\\u003c/em\\u003e\\u003c/p\\u003e\\n\\n\\u003cp\\u003eThe ID of the sensor the recording is from.\\u003c/p\\u003e\\n\\n\\u003cp\\u003e\\u003cem\\u003eaudio_filename\\u003c/em\\u003e\\u003c/p\\u003e\\n\\n\\u003cp\\u003eThe filename of the audio recording\\u003c/p\\u003e\\n\\n\\u003cp\\u003e\\u003cem\\u003eannotator_id\\u003c/em\\u003e\\u003c/p\\u003e\\n\\n\\u003cp\\u003eThe anonymous ID of the annotator. If this value is positive, it is a citizen science volunteer from the Zooniverse platform. If it is negative, it is a SONYC team member. If it is \\u003ccode\\u003e0\\u003c/code\\u003e, then it is the ground truth agreed-upon by the SONYC team.\\u003c/p\\u003e\\n\\n\\u003cp\\u003e\\u003cem\\u003eyear\\u003c/em\\u003e\\u003c/p\\u003e\\n\\n\\u003cp\\u003eThe year the recording is from.\\u003c/p\\u003e\\n\\n\\u003cp\\u003e\\u003cem\\u003eweek\\u003c/em\\u003e\\u003c/p\\u003e\\n\\n\\u003cp\\u003eThe week of the year the recording is from.\\u003c/p\\u003e\\n\\n\\u003cp\\u003e\\u003cem\\u003eday\\u003c/em\\u003e\\u003c/p\\u003e\\n\\n\\u003cp\\u003eThe day of the week the recording is from, with Monday as the start (i.e. \\u003ccode\\u003e0\\u003c/code\\u003e=Monday).\\u003c/p\\u003e\\n\\n\\u003cp\\u003e\\u003cem\\u003ehour\\u003c/em\\u003e\\u003c/p\\u003e\\n\\n\\u003cp\\u003eThe hour of the day the recording is from\\u003c/p\\u003e\\n\\n\\u003cp\\u003e\\u003cem\\u003eborough\\u003c/em\\u003e\\u003cbr\\u003e\\nThe NYC borough in which the sensor is located (\\u003ccode\\u003e1\\u003c/code\\u003e=Manhattan, \\u003ccode\\u003e3\\u003c/code\\u003e=Brooklyn, \\u003ccode\\u003e4\\u003c/code\\u003e=Queens). This corresponds to the first digit in the 10-digit NYC parcel number system known as Borough, Block, Lot (BBL).\\u003c/p\\u003e\\n\\n\\u003cp\\u003e\\u003cem\\u003eblock\\u003c/em\\u003e\\u003c/p\\u003e\\n\\n\\u003cp\\u003eThe NYC block in which the sensor is located. This corresponds to digits 2\\u0026mdash;6 digit in the 10-digit NYC parcel number system known as Borough, Block, Lot (BBL).\\u003c/p\\u003e\\n\\n\\u003cp\\u003e\\u003cem\\u003elatitude\\u003c/em\\u003e\\u003c/p\\u003e\\n\\n\\u003cp\\u003eThe latitude coordinate of the \\u003cstrong\\u003eblock\\u003c/strong\\u003e\\u0026nbsp;in which the sensor is located.\\u003c/p\\u003e\\n\\n\\u003cp\\u003e\\u003cem\\u003elongitude\\u003c/em\\u003e\\u003c/p\\u003e\\n\\n\\u003cp\\u003eThe longitude coordinate of the \\u003cstrong\\u003eblock\\u003c/strong\\u003e\\u0026nbsp;in which the sensor is located.\\u003c/p\\u003e\\n\\n\\u003cp\\u003e\\u003cem\\u003e\\u0026lt;coarse_id\\u0026gt;-\\u0026lt;fine_id\\u0026gt;_\\u0026lt;fine_name\\u0026gt;_presence\\u003c/em\\u003e\\u003c/p\\u003e\\n\\n\\u003cp\\u003eColumns of this form indicate the presence of fine-level class. \\u003ccode\\u003e1\\u003c/code\\u003e if present, \\u003ccode\\u003e0\\u003c/code\\u003e if not present. If \\u003ccode\\u003e-1\\u003c/code\\u003e, then the class was not labeled in this annotation because the annotation was performed by a SONYC team member who only annotated one coarse group of classes at a time when annotating the verified subset.\\u003c/p\\u003e\\n\\n\\u003cp\\u003e\\u003cem\\u003e\\u0026lt;coarse_id\\u0026gt;_\\u0026lt;coarse_name\\u0026gt;_presence\\u003c/em\\u003e\\u003c/p\\u003e\\n\\n\\u003cp\\u003eColumns of this form indicate the presence of a coarse-level class. \\u003ccode\\u003e1\\u003c/code\\u003e if present, \\u003ccode\\u003e0\\u003c/code\\u003e if not present. If \\u003ccode\\u003e-1\\u003c/code\\u003e, then the class was not labeled in this annotation because the annotation was performed by a SONYC team member who only annotated one coarse group of classes at a time when annotating the verified subset. These columns are computed from the fine-level class presence columns and are presented here for convenience when training on only coarse-level classes.\\u003c/p\\u003e\\n\\n\\u003cp\\u003e\\u003cem\\u003e\\u0026lt;coarse_id\\u0026gt;-\\u0026lt;fine_id\\u0026gt;_\\u0026lt;fine_name\\u0026gt;_proximity\\u003c/em\\u003e\\u003c/p\\u003e\\n\\n\\u003cp\\u003eColumns of this form indicate the proximity of a fine-level class. After indicating the presence of a fine-level class, citizen science annotators were asked to indicate the proximity of the sound event to the sensor. Only the citizen science volunteers performed this task, and therefore this data is not included in the verified annotations. This column may take on one of the following four values: (\\u003ccode\\u003enear\\u003c/code\\u003e, \\u003ccode\\u003efar\\u003c/code\\u003e, \\u003ccode\\u003enotsure\\u003c/code\\u003e, \\u003ccode\\u003e-1\\u003c/code\\u003e). If \\u003ccode\\u003e-1\\u003c/code\\u003e, then the proximity was not annotated because either the annotation was not performed by a citizen science volunteer, or the citizen science volunteer did not indicate the presence of the class.\\u003c/p\\u003e\\n\\n\\u003cp\\u003e\\u0026nbsp;\\u003c/p\\u003e\\n\\n\\u003cp\\u003e\\u003cstrong\\u003eConditions of use\\u003c/strong\\u003e\\u003c/p\\u003e\\n\\n\\u003cp\\u003eDataset created by Mark Cartwright, Jason Cramer, Ana Elisa Mendez Mendez, Yu Wang, Ho-Hsiang Wu, Vincent Lostanlen, Magdalena Fuentes, Graham Dove, Charlie Mydlarz, Justin Salamon, Oded Nov, and Juan Pablo Bello\\u003c/p\\u003e\\n\\n\\u003cp\\u003eThe SONYC-UST dataset is offered free of charge under the terms of the Creative Commons Attribution 4.0 International (CC BY 4.0) license:\\u003cbr\\u003e\\n\\u003ca href=\\\"https://creativecommons.org/licenses/by/4.0/\\\"\\u003ehttps://creativecommons.org/licenses/by/4.0/\\u003c/a\\u003e\\u003c/p\\u003e\\n\\n\\u003cp\\u003eThe dataset and its contents are made available on an \\u0026ldquo;as is\\u0026rdquo; basis and without warranties of any kind, including without limitation satisfactory quality and conformity, merchantability, fitness for a particular purpose, accuracy or completeness, or absence of errors. Subject to any liability that may not be excluded or limited by law, New York University is not liable for, and expressly excludes all liability for, loss or damage however and whenever caused to anyone by any use of the SONYC-UST dataset or any part of it.\\u003c/p\\u003e\\n\\n\\u003cp\\u003e\\u0026nbsp;\\u003c/p\\u003e\\n\\n\\u003cp\\u003e\\u003cstrong\\u003eFeedback\\u003c/strong\\u003e\\u003c/p\\u003e\\n\\n\\u003cp\\u003ePlease help us improve SONYC-UST\\u0026nbsp;by sending your feedback to:\\u003c/p\\u003e\\n\\n\\u003cul\\u003e\\n\\t\\u003cli\\u003eMark Cartwright: \\u003ca href=\\\"mailto:mcartwright@gmail.com\\\"\\u003emcartwright@gmail.com\\u003c/a\\u003e\\u003c/li\\u003e\\n\\u003c/ul\\u003e\\n\\n\\u003cp\\u003eIn case of a problem, please include as many details as possible.\\u003c/p\\u003e\\n\\n\\u003cp\\u003e\\u0026nbsp;\\u003c/p\\u003e\\n\\n\\u003cp\\u003e\\u003cstrong\\u003eAcknowledgments\\u003c/strong\\u003e\\u003c/p\\u003e\\n\\n\\u003cp\\u003eWe would like to thank all the Zooniverse volunteers who continue to contribute to our project. This work is supported by \\u003ca href=\\\"https://www.nsf.gov/awardsearch/showAward?AWD_ID=1544753\\\"\\u003eNational Science Foundation award 1544753\\u003c/a\\u003e.\\u003c/p\\u003e\\n\\n\\u003cp\\u003e\\u0026nbsp;\\u003c/p\\u003e\\n\\n\\u003cp\\u003e\\u003cstrong\\u003eChange log\\u003c/strong\\u003e\\u003c/p\\u003e\\n\\n\\u003cul\\u003e\\n\\t\\u003cli\\u003e2.3 Added the ground truth annotations for the test set, and regrouped the audio files for upload to Zenodo.\\u003c/li\\u003e\\n\\t\\u003cli\\u003e2.2\\u0026nbsp;Added the audio for the test set (audio-eval.tar.gz).\\u003c/li\\u003e\\n\\t\\u003cli\\u003e2.1 The DCASE 2020 development dataset. 14778 new recordings added along with coarse spatiotemporal context information.\\u003c/li\\u003e\\n\\t\\u003cli\\u003e1.0 Data is the same as v0.4. Publication added to README.\\u003c/li\\u003e\\n\\t\\u003cli\\u003e0.4 Fixed error in annotations. Previously, the coarse class \\u0026quot;machinery-impact\\u0026quot; was accidentally indicated as present whenever \\u0026quot;non-machinery-impact\\u0026quot; was present regardless of the presence of \\u0026quot;machinery-impact\\u0026quot;. This error has been fixed.\\u003c/li\\u003e\\n\\t\\u003cli\\u003e0.3 Test set annotations added\\u003c/li\\u003e\\n\\t\\u003cli\\u003e0.2 Test set audio files added\\u003c/li\\u003e\\n\\u003c/ul\\u003e\", \"distribution\": [{\"@type\": \"DataDownload\", \"contentUrl\": \"https://zenodo.org/api/files/31fe1dae-031a-4bcf-8f1b-71b10dee72da/annotations.csv\", \"encodingFormat\": \"csv\"}, {\"@type\": \"DataDownload\", \"contentUrl\": \"https://zenodo.org/api/files/31fe1dae-031a-4bcf-8f1b-71b10dee72da/audio-0.tar.gz\", \"encodingFormat\": \"gz\"}, {\"@type\": \"DataDownload\", \"contentUrl\": \"https://zenodo.org/api/files/31fe1dae-031a-4bcf-8f1b-71b10dee72da/audio-10.tar.gz\", \"encodingFormat\": \"gz\"}, {\"@type\": \"DataDownload\", \"contentUrl\": \"https://zenodo.org/api/files/31fe1dae-031a-4bcf-8f1b-71b10dee72da/audio-11.tar.gz\", \"encodingFormat\": \"gz\"}, {\"@type\": \"DataDownload\", \"contentUrl\": \"https://zenodo.org/api/files/31fe1dae-031a-4bcf-8f1b-71b10dee72da/audio-12.tar.gz\", \"encodingFormat\": \"gz\"}, {\"@type\": \"DataDownload\", \"contentUrl\": \"https://zenodo.org/api/files/31fe1dae-031a-4bcf-8f1b-71b10dee72da/audio-13.tar.gz\", \"encodingFormat\": \"gz\"}, {\"@type\": \"DataDownload\", \"contentUrl\": \"https://zenodo.org/api/files/31fe1dae-031a-4bcf-8f1b-71b10dee72da/audio-14.tar.gz\", \"encodingFormat\": \"gz\"}, {\"@type\": \"DataDownload\", \"contentUrl\": \"https://zenodo.org/api/files/31fe1dae-031a-4bcf-8f1b-71b10dee72da/audio-15.tar.gz\", \"encodingFormat\": \"gz\"}, {\"@type\": \"DataDownload\", \"contentUrl\": \"https://zenodo.org/api/files/31fe1dae-031a-4bcf-8f1b-71b10dee72da/audio-16.tar.gz\", \"encodingFormat\": \"gz\"}, {\"@type\": \"DataDownload\", \"contentUrl\": \"https://zenodo.org/api/files/31fe1dae-031a-4bcf-8f1b-71b10dee72da/audio-17.tar.gz\", \"encodingFormat\": \"gz\"}, {\"@type\": \"DataDownload\", \"contentUrl\": \"https://zenodo.org/api/files/31fe1dae-031a-4bcf-8f1b-71b10dee72da/audio-18.tar.gz\", \"encodingFormat\": \"gz\"}, {\"@type\": \"DataDownload\", \"contentUrl\": \"https://zenodo.org/api/files/31fe1dae-031a-4bcf-8f1b-71b10dee72da/audio-1.tar.gz\", \"encodingFormat\": \"gz\"}, {\"@type\": \"DataDownload\", \"contentUrl\": \"https://zenodo.org/api/files/31fe1dae-031a-4bcf-8f1b-71b10dee72da/audio-2.tar.gz\", \"encodingFormat\": \"gz\"}, {\"@type\": \"DataDownload\", \"contentUrl\": \"https://zenodo.org/api/files/31fe1dae-031a-4bcf-8f1b-71b10dee72da/audio-3.tar.gz\", \"encodingFormat\": \"gz\"}, {\"@type\": \"DataDownload\", \"contentUrl\": \"https://zenodo.org/api/files/31fe1dae-031a-4bcf-8f1b-71b10dee72da/audio-4.tar.gz\", \"encodingFormat\": \"gz\"}, {\"@type\": \"DataDownload\", \"contentUrl\": \"https://zenodo.org/api/files/31fe1dae-031a-4bcf-8f1b-71b10dee72da/audio-5.tar.gz\", \"encodingFormat\": \"gz\"}, {\"@type\": \"DataDownload\", \"contentUrl\": \"https://zenodo.org/api/files/31fe1dae-031a-4bcf-8f1b-71b10dee72da/audio-6.tar.gz\", \"encodingFormat\": \"gz\"}, {\"@type\": \"DataDownload\", \"contentUrl\": \"https://zenodo.org/api/files/31fe1dae-031a-4bcf-8f1b-71b10dee72da/audio-7.tar.gz\", \"encodingFormat\": \"gz\"}, {\"@type\": \"DataDownload\", \"contentUrl\": \"https://zenodo.org/api/files/31fe1dae-031a-4bcf-8f1b-71b10dee72da/audio-8.tar.gz\", \"encodingFormat\": \"gz\"}, {\"@type\": \"DataDownload\", \"contentUrl\": \"https://zenodo.org/api/files/31fe1dae-031a-4bcf-8f1b-71b10dee72da/audio-9.tar.gz\", \"encodingFormat\": \"gz\"}, {\"@type\": \"DataDownload\", \"contentUrl\": \"https://zenodo.org/api/files/31fe1dae-031a-4bcf-8f1b-71b10dee72da/dcase-ust-taxonomy.yaml\", \"encodingFormat\": \"yaml\"}, {\"@type\": \"DataDownload\", \"contentUrl\": \"https://zenodo.org/api/files/31fe1dae-031a-4bcf-8f1b-71b10dee72da/README.md\", \"encodingFormat\": \"md\"}, {\"@type\": \"DataDownload\", \"contentUrl\": \"https://zenodo.org/api/files/31fe1dae-031a-4bcf-8f1b-71b10dee72da/unpack_audio.sh\", \"encodingFormat\": \"sh\"}], \"identifier\": \"https://doi.org/10.5281/zenodo.3966543\", \"keywords\": [\"urban sound\", \"noise pollution\", \"machine listening\", \"computer audition\", \"longterm spatiotemporal context\", \"sound tagging\"], \"license\": \"https://creativecommons.org/licenses/by/4.0/legalcode\", \"name\": \"SONYC Urban Sound Tagging (SONYC-UST): a multilabel dataset from an urban acoustic sensor network\", \"url\": \"https://zenodo.org/record/3966543\", \"version\": \"2.3.0\"}</script>\n",
      "<script src=\"/static/gen/zenodo.f843b7ec.js\"></script>\n",
      "<script src=\"//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML\" type=\"text/javascript\"></script>\n",
      "<script src=\"/static/gen/zenodo.search.ff7bb1ad.js\"></script>\n",
      "<script type=\"text/javascript\">var addthis_config = {\"data_track_addressbar\": true};</script>\n",
      "<script type=\"text/javascript\">\n",
      "  // Bootstrap the Invenio CSL Formatter and invenio-search-js\n",
      "  require([\n",
      "    \"jquery\",\n",
      "    'typeahead.js',\n",
      "    'bloodhound',\n",
      "    \"node_modules/angular/angular\",\n",
      "    \"node_modules/invenio-csl-js/dist/invenio-csl-js\",\n",
      "    \"node_modules/invenio-search-js/dist/invenio-search-js\",\n",
      "    \"js/zenodo/module\"\n",
      "    ], function(typeahead, Bloodhound) {\n",
      "      angular.element(document).ready(function() {\n",
      "\n",
      "        // FIXME: This is already defined in js/zenodo_deposit/filters.js.\n",
      "        // It should be moved to a common place...\n",
      "        angular.module('zenodo.filters').filter('limitToEllipsis', function () {\n",
      "          return function(text, n) {\n",
      "            return (text.length > n) ? text.substr(0, n-1) + '&hellip;' : text;\n",
      "          };\n",
      "        });\n",
      "\n",
      "        angular.bootstrap(document.getElementById(\"citations\"), [\n",
      "            'invenioSearch',\n",
      "            'zenodo.filters',\n",
      "            'mgcrea.ngStrap.tooltip',\n",
      "          ]\n",
      "        );\n",
      "\n",
      "        angular.bootstrap(document.getElementById(\"invenio-csl\"), [\n",
      "            'invenioCsl',\n",
      "          ]\n",
      "        );\n",
      "      });\n",
      "    }\n",
      "  );\n",
      "  require([\n",
      "    \"jquery\",\n",
      "    \"js/zenodo/functions\",\n",
      "    ], function($, recordCommunityCurate) {\n",
      "      $(function () {\n",
      "        $(\"#recordCommunityCuration .btn\").click(recordCommunityCurate);\n",
      "        $('.preview-link').on('click', function(event) {\n",
      "          $('#preview').show();\n",
      "          var filename = encodeURIComponent($(event.target).data('filename'));\n",
      "          $('#preview-iframe').attr(\"src\",\"/record/3966543/preview/\" + filename);\n",
      "        });\n",
      "      });\n",
      "    }\n",
      "  );\n",
      "  $(function () {\n",
      "    $('[data-toggle=\"tooltip\"]').tooltip();\n",
      "  });\n",
      "</script>\n",
      "<script src=\"//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5137aff66ad9c2a1\" type=\"text/javascript\"></script>\n",
      "<script type=\"text/javascript\">\n",
      "var _paq = _paq || [];\n",
      "_paq.push([\"setDomains\", [\"*.zenodo.org\",\"*.zenodo.cern.ch\",\"*.zenodo.eu\",\"*.zenodo.net\"]]);\n",
      "_paq.push([\"trackPageView\"]);\n",
      "_paq.push([\"enableLinkTracking\"]);\n",
      "\n",
      "(function() {\n",
      "  var u=((\"https:\" == document.location.protocol) ? \"https\" : \"http\") + \"://piwik.web.cern.ch/\";\n",
      "  _paq.push([\"setTrackerUrl\", u+\"piwik.php\"]);\n",
      "  _paq.push([\"setSiteId\", \"57\"]);\n",
      "  var d=document, g=d.createElement(\"script\"), s=d.getElementsByTagName(\"script\")[0]; g.type=\"text/javascript\";\n",
      "  g.defer=true; g.async=true; g.src=u+\"piwik.js\"; s.parentNode.insertBefore(g,s);\n",
      "})();\n",
      "</script>\n",
      "</body>\n",
      "</html>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method Tag.find of \n",
       "<!DOCTYPE html>\n",
       "\n",
       "<html dir=\"ltr\" lang=\"en\">\n",
       "<head>\n",
       "<meta charset=\"utf-8\"/>\n",
       "<meta content=\"IE=edge\" http-equiv=\"X-UA-Compatible\"/>\n",
       "<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n",
       "<meta content=\"5fPGCLllnWrvFxH9QWI0l1TadV7byeEvfPcyK2VkS_s\" name=\"google-site-verification\">\n",
       "<meta content=\"Rp5zp04IKW-s1IbpTOGB7Z6XY60oloZD5C3kTM-AiY4\" name=\"google-site-verification\">\n",
       "<meta content=\"umenay8zh4kswbi568zqp19bqb-jvngusibub1ygib0x3jne9rig0fnmtofm8abb7lkzgltqp5yhm68s5qz4iqqkm39xl2o-p5foixd-1xfq4yig07ugcd1sp5kmyvpe\" name=\"norton-safeweb-site-verification\">\n",
       "<title>SONYC Urban Sound Tagging (SONYC-UST): a multilabel dataset from an urban acoustic sensor network | Zenodo</title>\n",
       "<link href=\"/static/favicon.ico\" rel=\"shortcut icon\"/>\n",
       "<link href=\"/static/apple-touch-icon-144-precomposed.png\" rel=\"apple-touch-icon-precomposed\" sizes=\"144x144\"/>\n",
       "<link href=\"/static/apple-touch-icon-114-precomposed.png\" rel=\"apple-touch-icon-precomposed\" sizes=\"114x114\"/>\n",
       "<link href=\"/static/apple-touch-icon-72-precomposed.png\" rel=\"apple-touch-icon-precomposed\" sizes=\"72x72\"/>\n",
       "<link href=\"/static/apple-touch-icon-57-precomposed.png\" rel=\"apple-touch-icon-precomposed\" sizes=\"57x57\"/>\n",
       "<link href=\"https://fonts.googleapis.com/css?family=Roboto:300,400,500,700,100,italic\" rel=\"stylesheet\"/>\n",
       "<link href=\"/static/gen/zenodo.5fb6164a.css\" rel=\"stylesheet\"/>\n",
       "<!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->\n",
       "<!--[if lt IE 9]>\n",
       "  <script src=\"https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js\"></script>\n",
       "  <script src=\"https://oss.maxcdn.com/respond/1.4.2/respond.min.js\"></script>\n",
       "<![endif]-->\n",
       "<meta content='SONYC Urban Sound Tagging (SONYC-UST): a multilabel dataset from an urban acoustic sensor network Version 2.3, September 2020   Created by Mark Cartwright (1,2,3), Jason Cramer (1), Ana Elisa Mendez Mendez (1), Yu Wang (1), Ho-Hsiang Wu (1), Vincent Lostanlen (1,2,4), Magdalena Fuentes (1), Graham Dove (2), Charlie Mydlarz (1,2), Justin Salamon (5), Oded Nov (6), Juan Pablo Bello (1,2,3) Music and Audio Research Lab, New York University Center for Urban Science and Progress, New York University Department of Computer Science and Engineering, New York University Cornell Lab of Ornithology Adobe Research Department of Technology Management and Innovation, New York University   Publication If using this data in an academic work, please reference the DOI and version, as well as cite the following paper, which presented the data collection procedure and the first version of the dataset: Cartwright, M., Cramer, J., Mendez, A.E.M., Wang, Y., Wu, H., Lostanlen, V., Fuentes, M., Dove, G., Mydlarz, C., Salamon, J., Nov, O., Bello, J.P. SONYC-UST-V2: An Urban Sound Tagging Dataset with Spatiotemporal Context. In Proceedings of the Workshop on Detection and Classification of Acoustic Scenes and Events (DCASE), 2020. [pdf]   Description SONYC Urban Sound Tagging (SONYC-UST) is a dataset for the development and evaluation of machine listening systems for realistic urban noise monitoring. The audio was recorded from the SONYC acoustic sensor network. Volunteers on the  Zooniverse citizen science platform tagged the presence of 23 classes that were chosen in consultation with the New York City Department of Environmental Protection. These 23 fine-grained classes can be grouped into 8 coarse-grained classes. The recordings are split into three sets: training, validation, and test. The training and validation sets are disjoint with respect to the sensor from which each recording came, and the test set is displaced in time. For increased reliability, three volunteers annotated each recording. In addition, members of the SONYC team subsequently created a subset of verified, ground-truth tags using a two-stage annotation procedure in which two annotators independently tagged and then collectively resolved any disagreements. This subset of recordings with verified annotations intersects with all three recording splits. All of the recordings in the test set have these verified annotations.  In v2 version of this dataset, we have also included coarse spatiotemporal context information to aid in tag prediction when time and location is known. For more details on the motivation and creation of this dataset see the DCASE 2020 Urban Sound Tagging with Spatiotemporal Context Task website.   Audio data The provided audio has been acquired using the SONYC acoustic sensor network for urban noise pollution monitoring. Over 60 different sensors have been deployed in New York City, and these sensors have collectively gathered the equivalent of over 50 years of audio data, of which we provide a small subset. The data was sampled by selecting the nearest neighbors on VGGish features of recordings known to have classes of interest. All recordings are 10 seconds and were recorded with identical microphones at identical gain settings. To maintain privacy, we quantized the spatial information to the level of a city block, and we quantized the temporal information to the level of an hour. We also limited the occurrence of recordings with positive human voice annotations to one per hour per sensor.   Label taxonomy The label taxonomy is as follows: engine 1: small-sounding-engine 2: medium-sounding-engine 3: large-sounding-engine X: engine-of-uncertain-size machinery-impact 1: rock-drill 2: jackhammer 3: hoe-ram 4: pile-driver X: other-unknown-impact-machinery non-machinery-impact 1: non-machinery-impact powered-saw 1: chainsaw 2: small-medium-rotating-saw 3: large-rotating-saw X: other-unknown-powered-saw alert-signal 1: car-horn 2: car-alarm 3: siren 4: reverse-beeper X: other-unknown-alert-signal music 1: stationary-music 2: mobile-music 3: ice-cream-truck X: music-from-uncertain-source human-voice 1: person-or-small-group-talking 2: person-or-small-group-shouting 3: large-crowd 4: amplified-speech X: other-unknown-human-voice dog 1: dog-barking-whining The classes preceded by an X code indicate when an annotator was able to identify the coarse class, but couldn’t identify the fine class because either they were uncertain which fine class it was or the fine class was not included in the taxonomy. dcase-ust-taxonomy.yaml contains this taxonomy in an easily machine-readable form.   Data splits This release contains a training subset (13538 recordings from 35 sensors), and validation subset (4308 recordings from 9 sensors), and a test subset (669 recordings from 48 sensors). The training and validation subsets are disjoint with respect to the sensor from which each recording came. The sensors in the test set will not disjoint from the training and validation subsets, but the test recordings are displaced in time, occurring after any of the recordings in the training and validation subset. The subset of recordings with verified annotations (1380 recordings) intersects with all three recording splits.  All of the recordings in the test set have these verified annotations.   Annotation data The annotation data are contained in annotations.csv, and encompass the training, validation, and test subsets. Each row in the file represents one multi-label annotation of a recording—it could be the annotation of a single citizen science volunteer, a single SONYC team member, or the agreed-upon ground truth by the SONYC team (see the annotator_id column description for more information).  Note that since the SONYC team members annotated each class group separately, there may be multiple annotation rows by a single SONYC team annotator for a particular audio recording.     Columns split The data split. (train, validate, test) sensor_id The ID of the sensor the recording is from. audio_filename The filename of the audio recording annotator_id The anonymous ID of the annotator. If this value is positive, it is a citizen science volunteer from the Zooniverse platform. If it is negative, it is a SONYC team member. If it is 0, then it is the ground truth agreed-upon by the SONYC team. year The year the recording is from. week The week of the year the recording is from. day The day of the week the recording is from, with Monday as the start (i.e. 0=Monday). hour The hour of the day the recording is from borough The NYC borough in which the sensor is located (1=Manhattan, 3=Brooklyn, 4=Queens). This corresponds to the first digit in the 10-digit NYC parcel number system known as Borough, Block, Lot (BBL). block The NYC block in which the sensor is located. This corresponds to digits 2—6 digit in the 10-digit NYC parcel number system known as Borough, Block, Lot (BBL). latitude The latitude coordinate of the block in which the sensor is located. longitude The longitude coordinate of the block in which the sensor is located. &lt;coarse_id&gt;-&lt;fine_id&gt;_&lt;fine_name&gt;_presence Columns of this form indicate the presence of fine-level class. 1 if present, 0 if not present. If -1, then the class was not labeled in this annotation because the annotation was performed by a SONYC team member who only annotated one coarse group of classes at a time when annotating the verified subset. &lt;coarse_id&gt;_&lt;coarse_name&gt;_presence Columns of this form indicate the presence of a coarse-level class. 1 if present, 0 if not present. If -1, then the class was not labeled in this annotation because the annotation was performed by a SONYC team member who only annotated one coarse group of classes at a time when annotating the verified subset. These columns are computed from the fine-level class presence columns and are presented here for convenience when training on only coarse-level classes. &lt;coarse_id&gt;-&lt;fine_id&gt;_&lt;fine_name&gt;_proximity Columns of this form indicate the proximity of a fine-level class. After indicating the presence of a fine-level class, citizen science annotators were asked to indicate the proximity of the sound event to the sensor. Only the citizen science volunteers performed this task, and therefore this data is not included in the verified annotations. This column may take on one of the following four values: (near, far, notsure, -1). If -1, then the proximity was not annotated because either the annotation was not performed by a citizen science volunteer, or the citizen science volunteer did not indicate the presence of the class.   Conditions of use Dataset created by Mark Cartwright, Jason Cramer, Ana Elisa Mendez Mendez, Yu Wang, Ho-Hsiang Wu, Vincent Lostanlen, Magdalena Fuentes, Graham Dove, Charlie Mydlarz, Justin Salamon, Oded Nov, and Juan Pablo Bello The SONYC-UST dataset is offered free of charge under the terms of the Creative Commons Attribution 4.0 International (CC BY 4.0) license: https://creativecommons.org/licenses/by/4.0/ The dataset and its contents are made available on an “as is” basis and without warranties of any kind, including without limitation satisfactory quality and conformity, merchantability, fitness for a particular purpose, accuracy or completeness, or absence of errors. Subject to any liability that may not be excluded or limited by law, New York University is not liable for, and expressly excludes all liability for, loss or damage however and whenever caused to anyone by any use of the SONYC-UST dataset or any part of it.   Feedback Please help us improve SONYC-UST by sending your feedback to: Mark Cartwright: mcartwright@gmail.com In case of a problem, please include as many details as possible.   Acknowledgments We would like to thank all the Zooniverse volunteers who continue to contribute to our project. This work is supported by National Science Foundation award 1544753.   Change log 2.3 Added the ground truth annotations for the test set, and regrouped the audio files for upload to Zenodo. 2.2 Added the audio for the test set (audio-eval.tar.gz). 2.1 The DCASE 2020 development dataset. 14778 new recordings added along with coarse spatiotemporal context information. 1.0 Data is the same as v0.4. Publication added to README. 0.4 Fixed error in annotations. Previously, the coarse class \"machinery-impact\" was accidentally indicated as present whenever \"non-machinery-impact\" was present regardless of the presence of \"machinery-impact\". This error has been fixed. 0.3 Test set annotations added 0.2 Test set audio files added' name=\"description\"/>\n",
       "<meta content=\"SONYC Urban Sound Tagging (SONYC-UST): a multilabel dataset from an urban acoustic sensor network\" name=\"citation_title\"/>\n",
       "<meta content=\"Mark Cartwright\" name=\"citation_author\"/>\n",
       "<meta content=\"Jason Cramer\" name=\"citation_author\"/>\n",
       "<meta content=\"Ana Elisa Mendez Mendez\" name=\"citation_author\"/>\n",
       "<meta content=\"Yu Wang\" name=\"citation_author\"/>\n",
       "<meta content=\"Ho-Hsiang Wu\" name=\"citation_author\"/>\n",
       "<meta content=\"Vincent Lostanlen\" name=\"citation_author\"/>\n",
       "<meta content=\"Magdalena Fuentes\" name=\"citation_author\"/>\n",
       "<meta content=\"Graham Dove\" name=\"citation_author\"/>\n",
       "<meta content=\"Charlie Mydlarz\" name=\"citation_author\"/>\n",
       "<meta content=\"Justin Salamon\" name=\"citation_author\"/>\n",
       "<meta content=\"Oded Nov\" name=\"citation_author\"/>\n",
       "<meta content=\"Juan Pablo Bello\" name=\"citation_author\"/>\n",
       "<meta content=\"2020/09/14\" name=\"citation_publication_date\"/>\n",
       "<meta content=\"10.5281/zenodo.3966543\" name=\"citation_doi\"/>\n",
       "<meta content=\"urban sound\" name=\"citation_keywords\"/>\n",
       "<meta content=\"noise pollution\" name=\"citation_keywords\"/>\n",
       "<meta content=\"machine listening\" name=\"citation_keywords\"/>\n",
       "<meta content=\"computer audition\" name=\"citation_keywords\"/>\n",
       "<meta content=\"longterm spatiotemporal context\" name=\"citation_keywords\"/>\n",
       "<meta content=\"sound tagging\" name=\"citation_keywords\"/>\n",
       "<meta content=\"https://zenodo.org/record/3966543\" name=\"citation_abstract_html_url\"/>\n",
       "<meta content=\"SONYC Urban Sound Tagging (SONYC-UST): a multilabel dataset from an urban acoustic sensor network\" property=\"og:title\"/>\n",
       "<meta content='SONYC Urban Sound Tagging (SONYC-UST): a multilabel dataset from an urban acoustic sensor network Version 2.3, September 2020   Created by Mark Cartwright (1,2,3), Jason Cramer (1), Ana Elisa Mendez Mendez (1), Yu Wang (1), Ho-Hsiang Wu (1), Vincent Lostanlen (1,2,4), Magdalena Fuentes (1), Graham Dove (2), Charlie Mydlarz (1,2), Justin Salamon (5), Oded Nov (6), Juan Pablo Bello (1,2,3) Music and Audio Research Lab, New York University Center for Urban Science and Progress, New York University Department of Computer Science and Engineering, New York University Cornell Lab of Ornithology Adobe Research Department of Technology Management and Innovation, New York University   Publication If using this data in an academic work, please reference the DOI and version, as well as cite the following paper, which presented the data collection procedure and the first version of the dataset: Cartwright, M., Cramer, J., Mendez, A.E.M., Wang, Y., Wu, H., Lostanlen, V., Fuentes, M., Dove, G., Mydlarz, C., Salamon, J., Nov, O., Bello, J.P. SONYC-UST-V2: An Urban Sound Tagging Dataset with Spatiotemporal Context. In Proceedings of the Workshop on Detection and Classification of Acoustic Scenes and Events (DCASE), 2020. [pdf]   Description SONYC Urban Sound Tagging (SONYC-UST) is a dataset for the development and evaluation of machine listening systems for realistic urban noise monitoring. The audio was recorded from the SONYC acoustic sensor network. Volunteers on the  Zooniverse citizen science platform tagged the presence of 23 classes that were chosen in consultation with the New York City Department of Environmental Protection. These 23 fine-grained classes can be grouped into 8 coarse-grained classes. The recordings are split into three sets: training, validation, and test. The training and validation sets are disjoint with respect to the sensor from which each recording came, and the test set is displaced in time. For increased reliability, three volunteers annotated each recording. In addition, members of the SONYC team subsequently created a subset of verified, ground-truth tags using a two-stage annotation procedure in which two annotators independently tagged and then collectively resolved any disagreements. This subset of recordings with verified annotations intersects with all three recording splits. All of the recordings in the test set have these verified annotations.  In v2 version of this dataset, we have also included coarse spatiotemporal context information to aid in tag prediction when time and location is known. For more details on the motivation and creation of this dataset see the DCASE 2020 Urban Sound Tagging with Spatiotemporal Context Task website.   Audio data The provided audio has been acquired using the SONYC acoustic sensor network for urban noise pollution monitoring. Over 60 different sensors have been deployed in New York City, and these sensors have collectively gathered the equivalent of over 50 years of audio data, of which we provide a small subset. The data was sampled by selecting the nearest neighbors on VGGish features of recordings known to have classes of interest. All recordings are 10 seconds and were recorded with identical microphones at identical gain settings. To maintain privacy, we quantized the spatial information to the level of a city block, and we quantized the temporal information to the level of an hour. We also limited the occurrence of recordings with positive human voice annotations to one per hour per sensor.   Label taxonomy The label taxonomy is as follows: engine 1: small-sounding-engine 2: medium-sounding-engine 3: large-sounding-engine X: engine-of-uncertain-size machinery-impact 1: rock-drill 2: jackhammer 3: hoe-ram 4: pile-driver X: other-unknown-impact-machinery non-machinery-impact 1: non-machinery-impact powered-saw 1: chainsaw 2: small-medium-rotating-saw 3: large-rotating-saw X: other-unknown-powered-saw alert-signal 1: car-horn 2: car-alarm 3: siren 4: reverse-beeper X: other-unknown-alert-signal music 1: stationary-music 2: mobile-music 3: ice-cream-truck X: music-from-uncertain-source human-voice 1: person-or-small-group-talking 2: person-or-small-group-shouting 3: large-crowd 4: amplified-speech X: other-unknown-human-voice dog 1: dog-barking-whining The classes preceded by an X code indicate when an annotator was able to identify the coarse class, but couldn’t identify the fine class because either they were uncertain which fine class it was or the fine class was not included in the taxonomy. dcase-ust-taxonomy.yaml contains this taxonomy in an easily machine-readable form.   Data splits This release contains a training subset (13538 recordings from 35 sensors), and validation subset (4308 recordings from 9 sensors), and a test subset (669 recordings from 48 sensors). The training and validation subsets are disjoint with respect to the sensor from which each recording came. The sensors in the test set will not disjoint from the training and validation subsets, but the test recordings are displaced in time, occurring after any of the recordings in the training and validation subset. The subset of recordings with verified annotations (1380 recordings) intersects with all three recording splits.  All of the recordings in the test set have these verified annotations.   Annotation data The annotation data are contained in annotations.csv, and encompass the training, validation, and test subsets. Each row in the file represents one multi-label annotation of a recording—it could be the annotation of a single citizen science volunteer, a single SONYC team member, or the agreed-upon ground truth by the SONYC team (see the annotator_id column description for more information).  Note that since the SONYC team members annotated each class group separately, there may be multiple annotation rows by a single SONYC team annotator for a particular audio recording.     Columns split The data split. (train, validate, test) sensor_id The ID of the sensor the recording is from. audio_filename The filename of the audio recording annotator_id The anonymous ID of the annotator. If this value is positive, it is a citizen science volunteer from the Zooniverse platform. If it is negative, it is a SONYC team member. If it is 0, then it is the ground truth agreed-upon by the SONYC team. year The year the recording is from. week The week of the year the recording is from. day The day of the week the recording is from, with Monday as the start (i.e. 0=Monday). hour The hour of the day the recording is from borough The NYC borough in which the sensor is located (1=Manhattan, 3=Brooklyn, 4=Queens). This corresponds to the first digit in the 10-digit NYC parcel number system known as Borough, Block, Lot (BBL). block The NYC block in which the sensor is located. This corresponds to digits 2—6 digit in the 10-digit NYC parcel number system known as Borough, Block, Lot (BBL). latitude The latitude coordinate of the block in which the sensor is located. longitude The longitude coordinate of the block in which the sensor is located. &lt;coarse_id&gt;-&lt;fine_id&gt;_&lt;fine_name&gt;_presence Columns of this form indicate the presence of fine-level class. 1 if present, 0 if not present. If -1, then the class was not labeled in this annotation because the annotation was performed by a SONYC team member who only annotated one coarse group of classes at a time when annotating the verified subset. &lt;coarse_id&gt;_&lt;coarse_name&gt;_presence Columns of this form indicate the presence of a coarse-level class. 1 if present, 0 if not present. If -1, then the class was not labeled in this annotation because the annotation was performed by a SONYC team member who only annotated one coarse group of classes at a time when annotating the verified subset. These columns are computed from the fine-level class presence columns and are presented here for convenience when training on only coarse-level classes. &lt;coarse_id&gt;-&lt;fine_id&gt;_&lt;fine_name&gt;_proximity Columns of this form indicate the proximity of a fine-level class. After indicating the presence of a fine-level class, citizen science annotators were asked to indicate the proximity of the sound event to the sensor. Only the citizen science volunteers performed this task, and therefore this data is not included in the verified annotations. This column may take on one of the following four values: (near, far, notsure, -1). If -1, then the proximity was not annotated because either the annotation was not performed by a citizen science volunteer, or the citizen science volunteer did not indicate the presence of the class.   Conditions of use Dataset created by Mark Cartwright, Jason Cramer, Ana Elisa Mendez Mendez, Yu Wang, Ho-Hsiang Wu, Vincent Lostanlen, Magdalena Fuentes, Graham Dove, Charlie Mydlarz, Justin Salamon, Oded Nov, and Juan Pablo Bello The SONYC-UST dataset is offered free of charge under the terms of the Creative Commons Attribution 4.0 International (CC BY 4.0) license: https://creativecommons.org/licenses/by/4.0/ The dataset and its contents are made available on an “as is” basis and without warranties of any kind, including without limitation satisfactory quality and conformity, merchantability, fitness for a particular purpose, accuracy or completeness, or absence of errors. Subject to any liability that may not be excluded or limited by law, New York University is not liable for, and expressly excludes all liability for, loss or damage however and whenever caused to anyone by any use of the SONYC-UST dataset or any part of it.   Feedback Please help us improve SONYC-UST by sending your feedback to: Mark Cartwright: mcartwright@gmail.com In case of a problem, please include as many details as possible.   Acknowledgments We would like to thank all the Zooniverse volunteers who continue to contribute to our project. This work is supported by National Science Foundation award 1544753.   Change log 2.3 Added the ground truth annotations for the test set, and regrouped the audio files for upload to Zenodo. 2.2 Added the audio for the test set (audio-eval.tar.gz). 2.1 The DCASE 2020 development dataset. 14778 new recordings added along with coarse spatiotemporal context information. 1.0 Data is the same as v0.4. Publication added to README. 0.4 Fixed error in annotations. Previously, the coarse class \"machinery-impact\" was accidentally indicated as present whenever \"non-machinery-impact\" was present regardless of the presence of \"machinery-impact\". This error has been fixed. 0.3 Test set annotations added 0.2 Test set audio files added' property=\"og:description\"/>\n",
       "<meta content=\"https://zenodo.org/record/3966543\" property=\"og:url\"/>\n",
       "<meta content=\"Zenodo\" property=\"og:site_name\"/>\n",
       "<meta content=\"summary\" name=\"twitter:card\"/>\n",
       "<meta content=\"@zenodo_org\" name=\"twitter:site\"/>\n",
       "<meta content=\"SONYC Urban Sound Tagging (SONYC-UST): a multilabel dataset from an urban acoustic sensor network\" name=\"twitter:title\"/>\n",
       "<meta content='SONYC Urban Sound Tagging (SONYC-UST): a multilabel dataset from an urban acoustic sensor network Version 2.3, September 2020   Created by Mark Cartwright (1,2,3), Jason Cramer (1), Ana Elisa Mendez Mendez (1), Yu Wang (1), Ho-Hsiang Wu (1), Vincent Lostanlen (1,2,4), Magdalena Fuentes (1), Graham Dove (2), Charlie Mydlarz (1,2), Justin Salamon (5), Oded Nov (6), Juan Pablo Bello (1,2,3) Music and Audio Research Lab, New York University Center for Urban Science and Progress, New York University Department of Computer Science and Engineering, New York University Cornell Lab of Ornithology Adobe Research Department of Technology Management and Innovation, New York University   Publication If using this data in an academic work, please reference the DOI and version, as well as cite the following paper, which presented the data collection procedure and the first version of the dataset: Cartwright, M., Cramer, J., Mendez, A.E.M., Wang, Y., Wu, H., Lostanlen, V., Fuentes, M., Dove, G., Mydlarz, C., Salamon, J., Nov, O., Bello, J.P. SONYC-UST-V2: An Urban Sound Tagging Dataset with Spatiotemporal Context. In Proceedings of the Workshop on Detection and Classification of Acoustic Scenes and Events (DCASE), 2020. [pdf]   Description SONYC Urban Sound Tagging (SONYC-UST) is a dataset for the development and evaluation of machine listening systems for realistic urban noise monitoring. The audio was recorded from the SONYC acoustic sensor network. Volunteers on the  Zooniverse citizen science platform tagged the presence of 23 classes that were chosen in consultation with the New York City Department of Environmental Protection. These 23 fine-grained classes can be grouped into 8 coarse-grained classes. The recordings are split into three sets: training, validation, and test. The training and validation sets are disjoint with respect to the sensor from which each recording came, and the test set is displaced in time. For increased reliability, three volunteers annotated each recording. In addition, members of the SONYC team subsequently created a subset of verified, ground-truth tags using a two-stage annotation procedure in which two annotators independently tagged and then collectively resolved any disagreements. This subset of recordings with verified annotations intersects with all three recording splits. All of the recordings in the test set have these verified annotations.  In v2 version of this dataset, we have also included coarse spatiotemporal context information to aid in tag prediction when time and location is known. For more details on the motivation and creation of this dataset see the DCASE 2020 Urban Sound Tagging with Spatiotemporal Context Task website.   Audio data The provided audio has been acquired using the SONYC acoustic sensor network for urban noise pollution monitoring. Over 60 different sensors have been deployed in New York City, and these sensors have collectively gathered the equivalent of over 50 years of audio data, of which we provide a small subset. The data was sampled by selecting the nearest neighbors on VGGish features of recordings known to have classes of interest. All recordings are 10 seconds and were recorded with identical microphones at identical gain settings. To maintain privacy, we quantized the spatial information to the level of a city block, and we quantized the temporal information to the level of an hour. We also limited the occurrence of recordings with positive human voice annotations to one per hour per sensor.   Label taxonomy The label taxonomy is as follows: engine 1: small-sounding-engine 2: medium-sounding-engine 3: large-sounding-engine X: engine-of-uncertain-size machinery-impact 1: rock-drill 2: jackhammer 3: hoe-ram 4: pile-driver X: other-unknown-impact-machinery non-machinery-impact 1: non-machinery-impact powered-saw 1: chainsaw 2: small-medium-rotating-saw 3: large-rotating-saw X: other-unknown-powered-saw alert-signal 1: car-horn 2: car-alarm 3: siren 4: reverse-beeper X: other-unknown-alert-signal music 1: stationary-music 2: mobile-music 3: ice-cream-truck X: music-from-uncertain-source human-voice 1: person-or-small-group-talking 2: person-or-small-group-shouting 3: large-crowd 4: amplified-speech X: other-unknown-human-voice dog 1: dog-barking-whining The classes preceded by an X code indicate when an annotator was able to identify the coarse class, but couldn’t identify the fine class because either they were uncertain which fine class it was or the fine class was not included in the taxonomy. dcase-ust-taxonomy.yaml contains this taxonomy in an easily machine-readable form.   Data splits This release contains a training subset (13538 recordings from 35 sensors), and validation subset (4308 recordings from 9 sensors), and a test subset (669 recordings from 48 sensors). The training and validation subsets are disjoint with respect to the sensor from which each recording came. The sensors in the test set will not disjoint from the training and validation subsets, but the test recordings are displaced in time, occurring after any of the recordings in the training and validation subset. The subset of recordings with verified annotations (1380 recordings) intersects with all three recording splits.  All of the recordings in the test set have these verified annotations.   Annotation data The annotation data are contained in annotations.csv, and encompass the training, validation, and test subsets. Each row in the file represents one multi-label annotation of a recording—it could be the annotation of a single citizen science volunteer, a single SONYC team member, or the agreed-upon ground truth by the SONYC team (see the annotator_id column description for more information).  Note that since the SONYC team members annotated each class group separately, there may be multiple annotation rows by a single SONYC team annotator for a particular audio recording.     Columns split The data split. (train, validate, test) sensor_id The ID of the sensor the recording is from. audio_filename The filename of the audio recording annotator_id The anonymous ID of the annotator. If this value is positive, it is a citizen science volunteer from the Zooniverse platform. If it is negative, it is a SONYC team member. If it is 0, then it is the ground truth agreed-upon by the SONYC team. year The year the recording is from. week The week of the year the recording is from. day The day of the week the recording is from, with Monday as the start (i.e. 0=Monday). hour The hour of the day the recording is from borough The NYC borough in which the sensor is located (1=Manhattan, 3=Brooklyn, 4=Queens). This corresponds to the first digit in the 10-digit NYC parcel number system known as Borough, Block, Lot (BBL). block The NYC block in which the sensor is located. This corresponds to digits 2—6 digit in the 10-digit NYC parcel number system known as Borough, Block, Lot (BBL). latitude The latitude coordinate of the block in which the sensor is located. longitude The longitude coordinate of the block in which the sensor is located. &lt;coarse_id&gt;-&lt;fine_id&gt;_&lt;fine_name&gt;_presence Columns of this form indicate the presence of fine-level class. 1 if present, 0 if not present. If -1, then the class was not labeled in this annotation because the annotation was performed by a SONYC team member who only annotated one coarse group of classes at a time when annotating the verified subset. &lt;coarse_id&gt;_&lt;coarse_name&gt;_presence Columns of this form indicate the presence of a coarse-level class. 1 if present, 0 if not present. If -1, then the class was not labeled in this annotation because the annotation was performed by a SONYC team member who only annotated one coarse group of classes at a time when annotating the verified subset. These columns are computed from the fine-level class presence columns and are presented here for convenience when training on only coarse-level classes. &lt;coarse_id&gt;-&lt;fine_id&gt;_&lt;fine_name&gt;_proximity Columns of this form indicate the proximity of a fine-level class. After indicating the presence of a fine-level class, citizen science annotators were asked to indicate the proximity of the sound event to the sensor. Only the citizen science volunteers performed this task, and therefore this data is not included in the verified annotations. This column may take on one of the following four values: (near, far, notsure, -1). If -1, then the proximity was not annotated because either the annotation was not performed by a citizen science volunteer, or the citizen science volunteer did not indicate the presence of the class.   Conditions of use Dataset created by Mark Cartwright, Jason Cramer, Ana Elisa Mendez Mendez, Yu Wang, Ho-Hsiang Wu, Vincent Lostanlen, Magdalena Fuentes, Graham Dove, Charlie Mydlarz, Justin Salamon, Oded Nov, and Juan Pablo Bello The SONYC-UST dataset is offered free of charge under the terms of the Creative Commons Attribution 4.0 International (CC BY 4.0) license: https://creativecommons.org/licenses/by/4.0/ The dataset and its contents are made available on an “as is” basis and without warranties of any kind, including without limitation satisfactory quality and conformity, merchantability, fitness for a particular purpose, accuracy or completeness, or absence of errors. Subject to any liability that may not be excluded or limited by law, New York University is not liable for, and expressly excludes all liability for, loss or damage however and whenever caused to anyone by any use of the SONYC-UST dataset or any part of it.   Feedback Please help us improve SONYC-UST by sending your feedback to: Mark Cartwright: mcartwright@gmail.com In case of a problem, please include as many details as possible.   Acknowledgments We would like to thank all the Zooniverse volunteers who continue to contribute to our project. This work is supported by National Science Foundation award 1544753.   Change log 2.3 Added the ground truth annotations for the test set, and regrouped the audio files for upload to Zenodo. 2.2 Added the audio for the test set (audio-eval.tar.gz). 2.1 The DCASE 2020 development dataset. 14778 new recordings added along with coarse spatiotemporal context information. 1.0 Data is the same as v0.4. Publication added to README. 0.4 Fixed error in annotations. Previously, the coarse class \"machinery-impact\" was accidentally indicated as present whenever \"non-machinery-impact\" was present regardless of the presence of \"machinery-impact\". This error has been fixed. 0.3 Test set annotations added 0.2 Test set audio files added' name=\"twitter:description\"/>\n",
       "<link href=\"https://zenodo.org/record/3966543\" rel=\"canonical\"/>\n",
       "<link href=\"https://zenodo.org/record/3966543/files/annotations.csv\" rel=\"alternate\" type=\"text/csv\"/>\n",
       "<link href=\"https://zenodo.org/record/3966543/files/audio-0.tar.gz\" rel=\"alternate\" type=\"application/gzip\"/>\n",
       "<link href=\"https://zenodo.org/record/3966543/files/audio-10.tar.gz\" rel=\"alternate\" type=\"application/gzip\"/>\n",
       "<link href=\"https://zenodo.org/record/3966543/files/audio-11.tar.gz\" rel=\"alternate\" type=\"application/gzip\"/>\n",
       "<link href=\"https://zenodo.org/record/3966543/files/audio-12.tar.gz\" rel=\"alternate\" type=\"application/gzip\"/>\n",
       "<link href=\"https://zenodo.org/record/3966543/files/audio-13.tar.gz\" rel=\"alternate\" type=\"application/gzip\"/>\n",
       "<link href=\"https://zenodo.org/record/3966543/files/audio-14.tar.gz\" rel=\"alternate\" type=\"application/gzip\"/>\n",
       "<link href=\"https://zenodo.org/record/3966543/files/audio-15.tar.gz\" rel=\"alternate\" type=\"application/gzip\"/>\n",
       "<link href=\"https://zenodo.org/record/3966543/files/audio-16.tar.gz\" rel=\"alternate\" type=\"application/gzip\"/>\n",
       "<link href=\"https://zenodo.org/record/3966543/files/audio-17.tar.gz\" rel=\"alternate\" type=\"application/gzip\"/>\n",
       "<link href=\"https://zenodo.org/record/3966543/files/audio-18.tar.gz\" rel=\"alternate\" type=\"application/gzip\"/>\n",
       "<link href=\"https://zenodo.org/record/3966543/files/audio-1.tar.gz\" rel=\"alternate\" type=\"application/gzip\"/>\n",
       "<link href=\"https://zenodo.org/record/3966543/files/audio-2.tar.gz\" rel=\"alternate\" type=\"application/gzip\"/>\n",
       "<link href=\"https://zenodo.org/record/3966543/files/audio-3.tar.gz\" rel=\"alternate\" type=\"application/gzip\"/>\n",
       "<link href=\"https://zenodo.org/record/3966543/files/audio-4.tar.gz\" rel=\"alternate\" type=\"application/gzip\"/>\n",
       "<link href=\"https://zenodo.org/record/3966543/files/audio-5.tar.gz\" rel=\"alternate\" type=\"application/gzip\"/>\n",
       "<link href=\"https://zenodo.org/record/3966543/files/audio-6.tar.gz\" rel=\"alternate\" type=\"application/gzip\"/>\n",
       "<link href=\"https://zenodo.org/record/3966543/files/audio-7.tar.gz\" rel=\"alternate\" type=\"application/gzip\"/>\n",
       "<link href=\"https://zenodo.org/record/3966543/files/audio-8.tar.gz\" rel=\"alternate\" type=\"application/gzip\"/>\n",
       "<link href=\"https://zenodo.org/record/3966543/files/audio-9.tar.gz\" rel=\"alternate\" type=\"application/gzip\"/>\n",
       "<link href=\"https://zenodo.org/record/3966543/files/dcase-ust-taxonomy.yaml\" rel=\"alternate\" type=\"application/octet-stream\"/>\n",
       "<link href=\"https://zenodo.org/record/3966543/files/README.md\" rel=\"alternate\" type=\"application/octet-stream\"/>\n",
       "<link href=\"https://zenodo.org/record/3966543/files/unpack_audio.sh\" rel=\"alternate\" type=\"application/x-sh\"/>\n",
       "</meta></meta></meta></head>\n",
       "<body data-spy=\"scroll\" data-target=\".scrollspy-target\" itemscope=\"\" itemtype=\"http://schema.org/WebPage\" ng-csp=\"\">\n",
       "<!--[if lt IE 8]>\n",
       "      <p class=\"browserupgrade\">You are using an <strong>outdated</strong> browser. Please <a href=\"http://browsehappy.com/\">upgrade your browser</a> to improve your experience.</p>\n",
       "    <![endif]-->\n",
       "<header>\n",
       "<nav class=\"navbar navbar-default navbar-static-top\">\n",
       "<div class=\"container\">\n",
       "<div class=\"navbar-header\">\n",
       "<button aria-controls=\"navbar\" aria-expanded=\"false\" class=\"navbar-toggle collapsed\" data-target=\"#navbar\" data-toggle=\"collapse\" type=\"button\">\n",
       "<span class=\"sr-only\">Toggle navigation</span>\n",
       "<span class=\"icon-bar\"></span>\n",
       "<span class=\"icon-bar\"></span>\n",
       "<span class=\"icon-bar\"></span>\n",
       "</button>\n",
       "<a href=\"/\"><img alt=\"Zenodo\" class=\"navbar-brand\" src=\"/static/img/zenodo.svg\"/></a>\n",
       "</div>\n",
       "<div class=\"navbar-collapse collapse\" id=\"navbar\">\n",
       "<form action=\"/search\" class=\"navbar-form navbar-left navbar-search\" role=\"search\">\n",
       "<div class=\"form-group\">\n",
       "<div class=\"input-group\">\n",
       "<input class=\"form-control search\" name=\"q\" placeholder=\"Search\" type=\"text\"/>\n",
       "<div class=\"input-group-btn\">\n",
       "<button class=\"btn btn-default\" type=\"submit\"><i class=\"fa fa-search\"></i></button>\n",
       "</div>\n",
       "</div>\n",
       "</div>\n",
       "</form>\n",
       "<ul class=\"nav navbar-nav\">\n",
       "<li><a href=\"/deposit\">Upload</a></li>\n",
       "<li><a href=\"/communities/\">Communities</a></li>\n",
       "</ul>\n",
       "<form class=\"navbar-form navbar-right\">\n",
       "<a class=\"btn btn-default\" href=\"/login/?next=%2Frecord%2F3966543%2F\"><i class=\"fa fa-sign-in\"></i> Log in</a>\n",
       "<a class=\"btn btn-default btn-warning\" href=\"/signup/\" type=\"submit\"><i class=\"fa fa-edit\"></i> Sign up</a>\n",
       "</form>\n",
       "</div>\n",
       "</div>\n",
       "</nav>\n",
       "</header>\n",
       "<div class=\"container record-detail\">\n",
       "<div class=\"row\">\n",
       "<div class=\"col-sm-8 col-md-8 col-left\">\n",
       "<p>\n",
       "<time data-toggle=\"tooltip\" datetime=\"September 14, 2020\" title=\"Publication date\">September 14, 2020</time>\n",
       "<span class=\"pull-right\">\n",
       "<span class=\"label label-default\">Dataset</span>\n",
       "<span class=\"label label-success\">\n",
       "Open Access\n",
       "</span>\n",
       "</span>\n",
       "</p>\n",
       "<h1>SONYC Urban Sound Tagging (SONYC-UST): a multilabel dataset from an urban acoustic sensor network</h1>\n",
       "<p>\n",
       "<a href=\"https://orcid.org/0000-0002-5908-390X\"><img class=\"inline-orcid\" src=\"/static/img/orcid.png\"/></a>\n",
       "<span class=\"text-muted\" data-toggle=\"tooltip\" title=\"New York University\">Mark Cartwright</span>; \n",
       "<a href=\"https://orcid.org/0000-0001-5288-9399\"><img class=\"inline-orcid\" src=\"/static/img/orcid.png\"/></a>\n",
       "<span class=\"text-muted\" data-toggle=\"tooltip\" title=\"New York University\">Jason Cramer</span>; \n",
       "<a href=\"https://orcid.org/0000-0002-4861-5616\"><img class=\"inline-orcid\" src=\"/static/img/orcid.png\"/></a>\n",
       "<span class=\"text-muted\" data-toggle=\"tooltip\" title=\"New York University\">Ana Elisa Mendez Mendez</span>; \n",
       "<a href=\"https://orcid.org/0000-0002-1615-5141\"><img class=\"inline-orcid\" src=\"/static/img/orcid.png\"/></a>\n",
       "<span class=\"text-muted\" data-toggle=\"tooltip\" title=\"New York University\">Yu Wang</span>; \n",
       "<a href=\"https://orcid.org/0000-0002-1102-074X\"><img class=\"inline-orcid\" src=\"/static/img/orcid.png\"/></a>\n",
       "<span class=\"text-muted\" data-toggle=\"tooltip\" title=\"New York University\">Ho-Hsiang Wu</span>; \n",
       "<a href=\"https://orcid.org/0000-0003-0580-1651\"><img class=\"inline-orcid\" src=\"/static/img/orcid.png\"/></a>\n",
       "<span class=\"text-muted\" data-toggle=\"tooltip\" title=\"Cornel University\">Vincent Lostanlen</span>; \n",
       "<a href=\"https://orcid.org/0000-0003-4506-6639\"><img class=\"inline-orcid\" src=\"/static/img/orcid.png\"/></a>\n",
       "<span class=\"text-muted\" data-toggle=\"tooltip\" title=\"New York University\">Magdalena Fuentes</span>; \n",
       "<a href=\"https://orcid.org/0000-0002-3551-0209\"><img class=\"inline-orcid\" src=\"/static/img/orcid.png\"/></a>\n",
       "<span class=\"text-muted\" data-toggle=\"tooltip\" title=\"New York University\">Graham Dove</span>; \n",
       "<a href=\"https://orcid.org/0000-0001-7061-0638\"><img class=\"inline-orcid\" src=\"/static/img/orcid.png\"/></a>\n",
       "<span class=\"text-muted\" data-toggle=\"tooltip\" title=\"New York University\">Charlie Mydlarz</span>; \n",
       "<a href=\"https://orcid.org/0000-0001-6345-4593\"><img class=\"inline-orcid\" src=\"/static/img/orcid.png\"/></a>\n",
       "<span class=\"text-muted\" data-toggle=\"tooltip\" title=\"New York University\">Justin Salamon</span>; \n",
       "<a href=\"https://orcid.org/0000-0001-6410-2995\"><img class=\"inline-orcid\" src=\"/static/img/orcid.png\"/></a>\n",
       "<span class=\"text-muted\" data-toggle=\"tooltip\" title=\"New York University\">Oded Nov</span>; \n",
       "<a href=\"https://orcid.org/0000-0001-8561-5204\"><img class=\"inline-orcid\" src=\"/static/img/orcid.png\"/></a>\n",
       "<span class=\"text-muted\" data-toggle=\"tooltip\" title=\"New York University\">Juan Pablo Bello</span></p>\n",
       "<div class=\"record-description\"><p><strong>SONYC Urban Sound Tagging (SONYC-UST): a multilabel dataset from an urban acoustic sensor network</strong></p>\n",
       "<p>Version 2.3, September 2020</p>\n",
       "<p> </p>\n",
       "<p><strong>Created by</strong></p>\n",
       "<p>Mark Cartwright (1,2,3), Jason Cramer (1), Ana Elisa Mendez Mendez (1), Yu Wang (1), Ho-Hsiang Wu (1), Vincent Lostanlen (1,2,4), Magdalena Fuentes (1), Graham Dove (2), Charlie Mydlarz (1,2), Justin Salamon (5), Oded Nov (6), Juan Pablo Bello (1,2,3)</p>\n",
       "<ol>\n",
       "<li>Music and Audio Research Lab, New York University</li>\n",
       "<li>Center for Urban Science and Progress, New York University</li>\n",
       "<li>Department of Computer Science and Engineering, New York University</li>\n",
       "<li>Cornell Lab of Ornithology</li>\n",
       "<li>Adobe Research</li>\n",
       "<li>Department of Technology Management and Innovation, New York University</li>\n",
       "</ol>\n",
       "<p> </p>\n",
       "<p><strong>Publication</strong></p>\n",
       "<p>If using this data in an academic work, please reference the DOI and version, as well as cite the following paper, which presented the data collection procedure and the first version of the dataset:</p>\n",
       "<p>Cartwright, M., Cramer, J., Mendez, A.E.M., Wang, Y., Wu, H., Lostanlen, V., Fuentes, M., Dove, G., Mydlarz, C., Salamon, J., Nov, O., Bello, J.P. SONYC-UST-V2: An Urban Sound Tagging Dataset with Spatiotemporal Context. In <em>Proceedings of the Workshop on Detection and Classification of Acoustic Scenes and Events (DCASE)</em>, 2020.<br/>\n",
       "<a href=\"https://arxiv.org/abs/2009.05188\">[pdf]</a></p>\n",
       "<p> </p>\n",
       "<p><strong>Description</strong></p>\n",
       "<p>SONYC Urban Sound Tagging (SONYC-UST) is a dataset for the development and evaluation of machine listening systems for realistic urban noise monitoring. The audio was recorded from the <a href=\"https://wp.nyu.edu/sonyc\">SONYC</a> acoustic sensor network. Volunteers on the  <a href=\"https://zooniverse.org\">Zooniverse</a> citizen science platform tagged the presence of 23 classes that were chosen in consultation with the New York City Department of Environmental Protection. These 23 fine-grained classes can be grouped into 8 coarse-grained classes. The recordings are split into three sets: training, validation, and test. The training and validation sets are disjoint with respect to the sensor from which each recording came, and the test set is displaced in time. For increased reliability, three volunteers annotated each recording. In addition, members of the SONYC team subsequently created a subset of verified, ground-truth tags using a two-stage annotation procedure in which two annotators independently tagged and then collectively resolved any disagreements. This subset of recordings with verified annotations intersects with all three recording splits. All of the recordings in the test set have these verified annotations.  In v2 version of this dataset, we have also included coarse spatiotemporal context information to aid in tag prediction when time and location is known. For more details on the motivation and creation of this dataset see the <a href=\"http://dcase.community/challenge2020/task-urban-sound-tagging-with-spatiotemporal-context\">DCASE 2020 Urban Sound Tagging with Spatiotemporal Context Task website</a>.</p>\n",
       "<p> </p>\n",
       "<p><strong>Audio data</strong></p>\n",
       "<p>The provided audio has been acquired using the SONYC acoustic sensor network for urban noise pollution monitoring. Over 60 different sensors have been deployed in New York City, and these sensors have collectively gathered the equivalent of over 50 years of audio data, of which we provide a small subset. The data was sampled by selecting the nearest neighbors on VGGish features of recordings known to have classes of interest. All recordings are 10 seconds and were recorded with identical microphones at identical gain settings. To maintain privacy, we quantized the spatial information to the level of a city block, and we quantized the temporal information to the level of an hour. We also limited the occurrence of recordings with positive human voice annotations to one per hour per sensor.</p>\n",
       "<p> </p>\n",
       "<p><strong>Label taxonomy</strong></p>\n",
       "<p>The label taxonomy is as follows:</p>\n",
       "<ol>\n",
       "<li>engine<br/>\n",
       "\t1: small-sounding-engine<br/>\n",
       "\t2: medium-sounding-engine<br/>\n",
       "\t3: large-sounding-engine<br/>\n",
       "\tX: engine-of-uncertain-size</li>\n",
       "<li>machinery-impact<br/>\n",
       "\t1: rock-drill<br/>\n",
       "\t2: jackhammer<br/>\n",
       "\t3: hoe-ram<br/>\n",
       "\t4: pile-driver<br/>\n",
       "\tX: other-unknown-impact-machinery</li>\n",
       "<li>non-machinery-impact<br/>\n",
       "\t1: non-machinery-impact</li>\n",
       "<li>powered-saw<br/>\n",
       "\t1: chainsaw<br/>\n",
       "\t2: small-medium-rotating-saw<br/>\n",
       "\t3: large-rotating-saw<br/>\n",
       "\tX: other-unknown-powered-saw</li>\n",
       "<li>alert-signal<br/>\n",
       "\t1: car-horn<br/>\n",
       "\t2: car-alarm<br/>\n",
       "\t3: siren<br/>\n",
       "\t4: reverse-beeper<br/>\n",
       "\tX: other-unknown-alert-signal</li>\n",
       "<li>music<br/>\n",
       "\t1: stationary-music<br/>\n",
       "\t2: mobile-music<br/>\n",
       "\t3: ice-cream-truck<br/>\n",
       "\tX: music-from-uncertain-source</li>\n",
       "<li>human-voice<br/>\n",
       "\t1: person-or-small-group-talking<br/>\n",
       "\t2: person-or-small-group-shouting<br/>\n",
       "\t3: large-crowd<br/>\n",
       "\t4: amplified-speech<br/>\n",
       "\tX: other-unknown-human-voice</li>\n",
       "<li>dog<br/>\n",
       "\t1: dog-barking-whining</li>\n",
       "</ol>\n",
       "<p>The classes preceded by an <code>X</code> code indicate when an annotator was able to identify the coarse class, but couldn’t identify the fine class because either they were uncertain which fine class it was or the fine class was not included in the taxonomy. <code>dcase-ust-taxonomy.yaml</code> contains this taxonomy in an easily machine-readable form.</p>\n",
       "<p> </p>\n",
       "<p><strong>Data splits</strong></p>\n",
       "<p>This release contains a training subset (13538 recordings from 35 sensors), and validation subset (4308 recordings from 9 sensors), and a test subset (669 recordings from 48 sensors). The training and validation subsets are disjoint with respect to the sensor from which each recording came. The sensors in the test set will not disjoint from the training and validation subsets, but the test recordings are displaced in time, occurring after any of the recordings in the training and validation subset. The subset of recordings with verified annotations (1380 recordings) intersects with all three recording splits.  All of the recordings in the test set have these verified annotations.</p>\n",
       "<p> </p>\n",
       "<p><strong>Annotation data</strong></p>\n",
       "<p>The annotation data are contained in <code>annotations.csv</code>, and encompass the training, validation, and test subsets. Each row in the file represents one multi-label annotation of a recording—it could be the annotation of a single citizen science volunteer, a single SONYC team member, or the agreed-upon ground truth by the SONYC team (see the <em>annotator_id</em> column description for more information).  Note that since the SONYC team members annotated each class group separately, there may be multiple annotation rows by a single SONYC team annotator for a particular audio recording.</p>\n",
       "<p> </p>\n",
       "<p> </p>\n",
       "<p><strong>Columns</strong></p>\n",
       "<p><em>split</em></p>\n",
       "<p>The data split. (<em>train</em>, <em>validate, test</em>)</p>\n",
       "<p><em>sensor_id</em></p>\n",
       "<p>The ID of the sensor the recording is from.</p>\n",
       "<p><em>audio_filename</em></p>\n",
       "<p>The filename of the audio recording</p>\n",
       "<p><em>annotator_id</em></p>\n",
       "<p>The anonymous ID of the annotator. If this value is positive, it is a citizen science volunteer from the Zooniverse platform. If it is negative, it is a SONYC team member. If it is <code>0</code>, then it is the ground truth agreed-upon by the SONYC team.</p>\n",
       "<p><em>year</em></p>\n",
       "<p>The year the recording is from.</p>\n",
       "<p><em>week</em></p>\n",
       "<p>The week of the year the recording is from.</p>\n",
       "<p><em>day</em></p>\n",
       "<p>The day of the week the recording is from, with Monday as the start (i.e. <code>0</code>=Monday).</p>\n",
       "<p><em>hour</em></p>\n",
       "<p>The hour of the day the recording is from</p>\n",
       "<p><em>borough</em><br/>\n",
       "The NYC borough in which the sensor is located (<code>1</code>=Manhattan, <code>3</code>=Brooklyn, <code>4</code>=Queens). This corresponds to the first digit in the 10-digit NYC parcel number system known as Borough, Block, Lot (BBL).</p>\n",
       "<p><em>block</em></p>\n",
       "<p>The NYC block in which the sensor is located. This corresponds to digits 2—6 digit in the 10-digit NYC parcel number system known as Borough, Block, Lot (BBL).</p>\n",
       "<p><em>latitude</em></p>\n",
       "<p>The latitude coordinate of the <strong>block</strong> in which the sensor is located.</p>\n",
       "<p><em>longitude</em></p>\n",
       "<p>The longitude coordinate of the <strong>block</strong> in which the sensor is located.</p>\n",
       "<p><em>&lt;coarse_id&gt;-&lt;fine_id&gt;_&lt;fine_name&gt;_presence</em></p>\n",
       "<p>Columns of this form indicate the presence of fine-level class. <code>1</code> if present, <code>0</code> if not present. If <code>-1</code>, then the class was not labeled in this annotation because the annotation was performed by a SONYC team member who only annotated one coarse group of classes at a time when annotating the verified subset.</p>\n",
       "<p><em>&lt;coarse_id&gt;_&lt;coarse_name&gt;_presence</em></p>\n",
       "<p>Columns of this form indicate the presence of a coarse-level class. <code>1</code> if present, <code>0</code> if not present. If <code>-1</code>, then the class was not labeled in this annotation because the annotation was performed by a SONYC team member who only annotated one coarse group of classes at a time when annotating the verified subset. These columns are computed from the fine-level class presence columns and are presented here for convenience when training on only coarse-level classes.</p>\n",
       "<p><em>&lt;coarse_id&gt;-&lt;fine_id&gt;_&lt;fine_name&gt;_proximity</em></p>\n",
       "<p>Columns of this form indicate the proximity of a fine-level class. After indicating the presence of a fine-level class, citizen science annotators were asked to indicate the proximity of the sound event to the sensor. Only the citizen science volunteers performed this task, and therefore this data is not included in the verified annotations. This column may take on one of the following four values: (<code>near</code>, <code>far</code>, <code>notsure</code>, <code>-1</code>). If <code>-1</code>, then the proximity was not annotated because either the annotation was not performed by a citizen science volunteer, or the citizen science volunteer did not indicate the presence of the class.</p>\n",
       "<p> </p>\n",
       "<p><strong>Conditions of use</strong></p>\n",
       "<p>Dataset created by Mark Cartwright, Jason Cramer, Ana Elisa Mendez Mendez, Yu Wang, Ho-Hsiang Wu, Vincent Lostanlen, Magdalena Fuentes, Graham Dove, Charlie Mydlarz, Justin Salamon, Oded Nov, and Juan Pablo Bello</p>\n",
       "<p>The SONYC-UST dataset is offered free of charge under the terms of the Creative Commons Attribution 4.0 International (CC BY 4.0) license:<br/>\n",
       "<a href=\"https://creativecommons.org/licenses/by/4.0/\">https://creativecommons.org/licenses/by/4.0/</a></p>\n",
       "<p>The dataset and its contents are made available on an “as is” basis and without warranties of any kind, including without limitation satisfactory quality and conformity, merchantability, fitness for a particular purpose, accuracy or completeness, or absence of errors. Subject to any liability that may not be excluded or limited by law, New York University is not liable for, and expressly excludes all liability for, loss or damage however and whenever caused to anyone by any use of the SONYC-UST dataset or any part of it.</p>\n",
       "<p> </p>\n",
       "<p><strong>Feedback</strong></p>\n",
       "<p>Please help us improve SONYC-UST by sending your feedback to:</p>\n",
       "<ul>\n",
       "<li>Mark Cartwright: <a href=\"mailto:mcartwright@gmail.com\">mcartwright@gmail.com</a></li>\n",
       "</ul>\n",
       "<p>In case of a problem, please include as many details as possible.</p>\n",
       "<p> </p>\n",
       "<p><strong>Acknowledgments</strong></p>\n",
       "<p>We would like to thank all the Zooniverse volunteers who continue to contribute to our project. This work is supported by <a href=\"https://www.nsf.gov/awardsearch/showAward?AWD_ID=1544753\">National Science Foundation award 1544753</a>.</p>\n",
       "<p> </p>\n",
       "<p><strong>Change log</strong></p>\n",
       "<ul>\n",
       "<li>2.3 Added the ground truth annotations for the test set, and regrouped the audio files for upload to Zenodo.</li>\n",
       "<li>2.2 Added the audio for the test set (audio-eval.tar.gz).</li>\n",
       "<li>2.1 The DCASE 2020 development dataset. 14778 new recordings added along with coarse spatiotemporal context information.</li>\n",
       "<li>1.0 Data is the same as v0.4. Publication added to README.</li>\n",
       "<li>0.4 Fixed error in annotations. Previously, the coarse class \"machinery-impact\" was accidentally indicated as present whenever \"non-machinery-impact\" was present regardless of the presence of \"machinery-impact\". This error has been fixed.</li>\n",
       "<li>0.3 Test set annotations added</li>\n",
       "<li>0.2 Test set audio files added</li>\n",
       "</ul></div>\n",
       "<div class=\"alert alert-warning record-notes\">\n",
       "        This work is supported by National Science Foundation award 1544753.\n",
       "      </div>\n",
       "<div class=\"panel panel-default\" id=\"preview\">\n",
       "<div class=\"panel-heading\">\n",
       "<a class=\"panel-toggle\" data-toggle=\"collapse\" href=\"#collapseOne\">\n",
       "        Preview\n",
       "        <span class=\"pull-right show-on-collapsed\"><i class=\"fa fa-chevron-right\"></i></span>\n",
       "<span class=\"pull-right hide-on-collapsed\"><i class=\"fa fa-chevron-down\"></i></span>\n",
       "</a>\n",
       "</div>\n",
       "<div class=\"collapse in\" id=\"collapseOne\">\n",
       "<iframe class=\"preview-iframe\" height=\"400\" id=\"preview-iframe\" src=\"/record/3966543/preview/README.md\" width=\"100%\"></iframe>\n",
       "</div>\n",
       "</div><div class=\"panel panel-default files-box\" id=\"files\">\n",
       "<div class=\"panel-heading\">\n",
       "<a class=\"panel-toggle\" data-toggle=\"collapse\" href=\"#collapseTwo\">\n",
       "      Files\n",
       "      <span class=\"pull-right show-on-collapsed\"><i class=\"fa fa-chevron-right\"></i></span>\n",
       "<span class=\"pull-right hide-on-collapsed\"><i class=\"fa fa-chevron-down\"></i></span>\n",
       "</a>\n",
       "<small class=\"text-muted\"> (13.3 GB)</small>\n",
       "</div>\n",
       "<div class=\"collapse in\" id=\"collapseTwo\">\n",
       "<table class=\"table table-striped\">\n",
       "<thead>\n",
       "<tr class=\"\">\n",
       "<th>Name</th>\n",
       "<th>Size</th>\n",
       "</tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr>\n",
       "<td>\n",
       "<a class=\"filename\" href=\"/record/3966543/files/annotations.csv?download=1\">annotations.csv</a>\n",
       "<br><small class=\"text-muted nowrap\">md5:70b507b15bb4cfcce4870925302f276b <i class=\"fa fa-question-circle text-muted\" data-placement=\"top\" data-toggle=\"tooltip\" title=\"This is the file fingerprint (MD5 checksum), which can be used to verify the file integrity.\" tooltip=\"\"></i></small>\n",
       "</br></td>\n",
       "<td class=\"nowrap\">14.5 MB</td>\n",
       "<td class=\"nowrap\"><span class=\"pull-right\"><button class=\"btn preview-link btn-xs btn-default\" data-filename=\"annotations.csv\"><i class=\"fa fa-eye\"></i> Preview</button> <a class=\"btn btn-xs btn-default\" href=\"/record/3966543/files/annotations.csv?download=1\"><i class=\"fa fa-download\"></i> Download</a></span></td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "<a class=\"filename\" href=\"/record/3966543/files/audio-0.tar.gz?download=1\">audio-0.tar.gz</a>\n",
       "<br><small class=\"text-muted nowrap\">md5:bbb4dbae7d2e58e18d24878b9ee1eb51 <i class=\"fa fa-question-circle text-muted\" data-placement=\"top\" data-toggle=\"tooltip\" title=\"This is the file fingerprint (MD5 checksum), which can be used to verify the file integrity.\" tooltip=\"\"></i></small>\n",
       "</br></td>\n",
       "<td class=\"nowrap\">721.6 MB</td>\n",
       "<td class=\"nowrap\"><span class=\"pull-right\"><a class=\"btn btn-xs btn-default\" href=\"/record/3966543/files/audio-0.tar.gz?download=1\"><i class=\"fa fa-download\"></i> Download</a></span></td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "<a class=\"filename\" href=\"/record/3966543/files/audio-1.tar.gz?download=1\">audio-1.tar.gz</a>\n",
       "<br><small class=\"text-muted nowrap\">md5:7c369ff37ac6a1fdd8493fdffe62e0a1 <i class=\"fa fa-question-circle text-muted\" data-placement=\"top\" data-toggle=\"tooltip\" title=\"This is the file fingerprint (MD5 checksum), which can be used to verify the file integrity.\" tooltip=\"\"></i></small>\n",
       "</br></td>\n",
       "<td class=\"nowrap\">721.7 MB</td>\n",
       "<td class=\"nowrap\"><span class=\"pull-right\"><a class=\"btn btn-xs btn-default\" href=\"/record/3966543/files/audio-1.tar.gz?download=1\"><i class=\"fa fa-download\"></i> Download</a></span></td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "<a class=\"filename\" href=\"/record/3966543/files/audio-10.tar.gz?download=1\">audio-10.tar.gz</a>\n",
       "<br><small class=\"text-muted nowrap\">md5:84e94de273666c537a3e6b709ee88d9b <i class=\"fa fa-question-circle text-muted\" data-placement=\"top\" data-toggle=\"tooltip\" title=\"This is the file fingerprint (MD5 checksum), which can be used to verify the file integrity.\" tooltip=\"\"></i></small>\n",
       "</br></td>\n",
       "<td class=\"nowrap\">717.7 MB</td>\n",
       "<td class=\"nowrap\"><span class=\"pull-right\"><a class=\"btn btn-xs btn-default\" href=\"/record/3966543/files/audio-10.tar.gz?download=1\"><i class=\"fa fa-download\"></i> Download</a></span></td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "<a class=\"filename\" href=\"/record/3966543/files/audio-11.tar.gz?download=1\">audio-11.tar.gz</a>\n",
       "<br><small class=\"text-muted nowrap\">md5:44fe1f43121a7d1178aa0cdf7477793b <i class=\"fa fa-question-circle text-muted\" data-placement=\"top\" data-toggle=\"tooltip\" title=\"This is the file fingerprint (MD5 checksum), which can be used to verify the file integrity.\" tooltip=\"\"></i></small>\n",
       "</br></td>\n",
       "<td class=\"nowrap\">719.1 MB</td>\n",
       "<td class=\"nowrap\"><span class=\"pull-right\"><a class=\"btn btn-xs btn-default\" href=\"/record/3966543/files/audio-11.tar.gz?download=1\"><i class=\"fa fa-download\"></i> Download</a></span></td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "<a class=\"filename\" href=\"/record/3966543/files/audio-12.tar.gz?download=1\">audio-12.tar.gz</a>\n",
       "<br><small class=\"text-muted nowrap\">md5:a7eb21011f460b5ac289f8e285e875ab <i class=\"fa fa-question-circle text-muted\" data-placement=\"top\" data-toggle=\"tooltip\" title=\"This is the file fingerprint (MD5 checksum), which can be used to verify the file integrity.\" tooltip=\"\"></i></small>\n",
       "</br></td>\n",
       "<td class=\"nowrap\">716.1 MB</td>\n",
       "<td class=\"nowrap\"><span class=\"pull-right\"><a class=\"btn btn-xs btn-default\" href=\"/record/3966543/files/audio-12.tar.gz?download=1\"><i class=\"fa fa-download\"></i> Download</a></span></td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "<a class=\"filename\" href=\"/record/3966543/files/audio-13.tar.gz?download=1\">audio-13.tar.gz</a>\n",
       "<br><small class=\"text-muted nowrap\">md5:27945a8b1008eec787f97213164951e6 <i class=\"fa fa-question-circle text-muted\" data-placement=\"top\" data-toggle=\"tooltip\" title=\"This is the file fingerprint (MD5 checksum), which can be used to verify the file integrity.\" tooltip=\"\"></i></small>\n",
       "</br></td>\n",
       "<td class=\"nowrap\">719.9 MB</td>\n",
       "<td class=\"nowrap\"><span class=\"pull-right\"><a class=\"btn btn-xs btn-default\" href=\"/record/3966543/files/audio-13.tar.gz?download=1\"><i class=\"fa fa-download\"></i> Download</a></span></td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "<a class=\"filename\" href=\"/record/3966543/files/audio-14.tar.gz?download=1\">audio-14.tar.gz</a>\n",
       "<br><small class=\"text-muted nowrap\">md5:fb8c7d81c3bde3a0c86c1e55e6a85a24 <i class=\"fa fa-question-circle text-muted\" data-placement=\"top\" data-toggle=\"tooltip\" title=\"This is the file fingerprint (MD5 checksum), which can be used to verify the file integrity.\" tooltip=\"\"></i></small>\n",
       "</br></td>\n",
       "<td class=\"nowrap\">713.4 MB</td>\n",
       "<td class=\"nowrap\"><span class=\"pull-right\"><a class=\"btn btn-xs btn-default\" href=\"/record/3966543/files/audio-14.tar.gz?download=1\"><i class=\"fa fa-download\"></i> Download</a></span></td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "<a class=\"filename\" href=\"/record/3966543/files/audio-15.tar.gz?download=1\">audio-15.tar.gz</a>\n",
       "<br><small class=\"text-muted nowrap\">md5:f6af9b65d876ef96d199b9e5f0473cb9 <i class=\"fa fa-question-circle text-muted\" data-placement=\"top\" data-toggle=\"tooltip\" title=\"This is the file fingerprint (MD5 checksum), which can be used to verify the file integrity.\" tooltip=\"\"></i></small>\n",
       "</br></td>\n",
       "<td class=\"nowrap\">718.1 MB</td>\n",
       "<td class=\"nowrap\"><span class=\"pull-right\"><a class=\"btn btn-xs btn-default\" href=\"/record/3966543/files/audio-15.tar.gz?download=1\"><i class=\"fa fa-download\"></i> Download</a></span></td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "<a class=\"filename\" href=\"/record/3966543/files/audio-16.tar.gz?download=1\">audio-16.tar.gz</a>\n",
       "<br><small class=\"text-muted nowrap\">md5:e8337c61a90c30989d500f950fbe443a <i class=\"fa fa-question-circle text-muted\" data-placement=\"top\" data-toggle=\"tooltip\" title=\"This is the file fingerprint (MD5 checksum), which can be used to verify the file integrity.\" tooltip=\"\"></i></small>\n",
       "</br></td>\n",
       "<td class=\"nowrap\">714.2 MB</td>\n",
       "<td class=\"nowrap\"><span class=\"pull-right\"><a class=\"btn btn-xs btn-default\" href=\"/record/3966543/files/audio-16.tar.gz?download=1\"><i class=\"fa fa-download\"></i> Download</a></span></td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "<a class=\"filename\" href=\"/record/3966543/files/audio-17.tar.gz?download=1\">audio-17.tar.gz</a>\n",
       "<br><small class=\"text-muted nowrap\">md5:3e47e85eb4564a30fe7f442ee97ec7b9 <i class=\"fa fa-question-circle text-muted\" data-placement=\"top\" data-toggle=\"tooltip\" title=\"This is the file fingerprint (MD5 checksum), which can be used to verify the file integrity.\" tooltip=\"\"></i></small>\n",
       "</br></td>\n",
       "<td class=\"nowrap\">718.9 MB</td>\n",
       "<td class=\"nowrap\"><span class=\"pull-right\"><a class=\"btn btn-xs btn-default\" href=\"/record/3966543/files/audio-17.tar.gz?download=1\"><i class=\"fa fa-download\"></i> Download</a></span></td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "<a class=\"filename\" href=\"/record/3966543/files/audio-18.tar.gz?download=1\">audio-18.tar.gz</a>\n",
       "<br><small class=\"text-muted nowrap\">md5:c6b2ef4d0d5b7269d465c469cdbbdc4b <i class=\"fa fa-question-circle text-muted\" data-placement=\"top\" data-toggle=\"tooltip\" title=\"This is the file fingerprint (MD5 checksum), which can be used to verify the file integrity.\" tooltip=\"\"></i></small>\n",
       "</br></td>\n",
       "<td class=\"nowrap\">365.9 MB</td>\n",
       "<td class=\"nowrap\"><span class=\"pull-right\"><a class=\"btn btn-xs btn-default\" href=\"/record/3966543/files/audio-18.tar.gz?download=1\"><i class=\"fa fa-download\"></i> Download</a></span></td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "<a class=\"filename\" href=\"/record/3966543/files/audio-2.tar.gz?download=1\">audio-2.tar.gz</a>\n",
       "<br><small class=\"text-muted nowrap\">md5:412241c063d7f196953d3dd1c44aeb5e <i class=\"fa fa-question-circle text-muted\" data-placement=\"top\" data-toggle=\"tooltip\" title=\"This is the file fingerprint (MD5 checksum), which can be used to verify the file integrity.\" tooltip=\"\"></i></small>\n",
       "</br></td>\n",
       "<td class=\"nowrap\">715.8 MB</td>\n",
       "<td class=\"nowrap\"><span class=\"pull-right\"><a class=\"btn btn-xs btn-default\" href=\"/record/3966543/files/audio-2.tar.gz?download=1\"><i class=\"fa fa-download\"></i> Download</a></span></td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "<a class=\"filename\" href=\"/record/3966543/files/audio-3.tar.gz?download=1\">audio-3.tar.gz</a>\n",
       "<br><small class=\"text-muted nowrap\">md5:d2a872392d95993a2c238d305b940812 <i class=\"fa fa-question-circle text-muted\" data-placement=\"top\" data-toggle=\"tooltip\" title=\"This is the file fingerprint (MD5 checksum), which can be used to verify the file integrity.\" tooltip=\"\"></i></small>\n",
       "</br></td>\n",
       "<td class=\"nowrap\">718.6 MB</td>\n",
       "<td class=\"nowrap\"><span class=\"pull-right\"><a class=\"btn btn-xs btn-default\" href=\"/record/3966543/files/audio-3.tar.gz?download=1\"><i class=\"fa fa-download\"></i> Download</a></span></td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "<a class=\"filename\" href=\"/record/3966543/files/audio-4.tar.gz?download=1\">audio-4.tar.gz</a>\n",
       "<br><small class=\"text-muted nowrap\">md5:b15b0e0cb3f8584259ef0d24293a9be3 <i class=\"fa fa-question-circle text-muted\" data-placement=\"top\" data-toggle=\"tooltip\" title=\"This is the file fingerprint (MD5 checksum), which can be used to verify the file integrity.\" tooltip=\"\"></i></small>\n",
       "</br></td>\n",
       "<td class=\"nowrap\">714.7 MB</td>\n",
       "<td class=\"nowrap\"><span class=\"pull-right\"><a class=\"btn btn-xs btn-default\" href=\"/record/3966543/files/audio-4.tar.gz?download=1\"><i class=\"fa fa-download\"></i> Download</a></span></td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "<a class=\"filename\" href=\"/record/3966543/files/audio-5.tar.gz?download=1\">audio-5.tar.gz</a>\n",
       "<br><small class=\"text-muted nowrap\">md5:de745f887067433757a2f2c8f99f99bb <i class=\"fa fa-question-circle text-muted\" data-placement=\"top\" data-toggle=\"tooltip\" title=\"This is the file fingerprint (MD5 checksum), which can be used to verify the file integrity.\" tooltip=\"\"></i></small>\n",
       "</br></td>\n",
       "<td class=\"nowrap\">719.4 MB</td>\n",
       "<td class=\"nowrap\"><span class=\"pull-right\"><a class=\"btn btn-xs btn-default\" href=\"/record/3966543/files/audio-5.tar.gz?download=1\"><i class=\"fa fa-download\"></i> Download</a></span></td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "<a class=\"filename\" href=\"/record/3966543/files/audio-6.tar.gz?download=1\">audio-6.tar.gz</a>\n",
       "<br><small class=\"text-muted nowrap\">md5:b0286c67468369d66336f6f5ddede31f <i class=\"fa fa-question-circle text-muted\" data-placement=\"top\" data-toggle=\"tooltip\" title=\"This is the file fingerprint (MD5 checksum), which can be used to verify the file integrity.\" tooltip=\"\"></i></small>\n",
       "</br></td>\n",
       "<td class=\"nowrap\">714.7 MB</td>\n",
       "<td class=\"nowrap\"><span class=\"pull-right\"><a class=\"btn btn-xs btn-default\" href=\"/record/3966543/files/audio-6.tar.gz?download=1\"><i class=\"fa fa-download\"></i> Download</a></span></td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "<a class=\"filename\" href=\"/record/3966543/files/audio-7.tar.gz?download=1\">audio-7.tar.gz</a>\n",
       "<br><small class=\"text-muted nowrap\">md5:ff300183ab7a9d3f2f74c3b730ffeb52 <i class=\"fa fa-question-circle text-muted\" data-placement=\"top\" data-toggle=\"tooltip\" title=\"This is the file fingerprint (MD5 checksum), which can be used to verify the file integrity.\" tooltip=\"\"></i></small>\n",
       "</br></td>\n",
       "<td class=\"nowrap\">718.4 MB</td>\n",
       "<td class=\"nowrap\"><span class=\"pull-right\"><a class=\"btn btn-xs btn-default\" href=\"/record/3966543/files/audio-7.tar.gz?download=1\"><i class=\"fa fa-download\"></i> Download</a></span></td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "<a class=\"filename\" href=\"/record/3966543/files/audio-8.tar.gz?download=1\">audio-8.tar.gz</a>\n",
       "<br><small class=\"text-muted nowrap\">md5:7be76b821fa6dbe20f6b50ca440e1024 <i class=\"fa fa-question-circle text-muted\" data-placement=\"top\" data-toggle=\"tooltip\" title=\"This is the file fingerprint (MD5 checksum), which can be used to verify the file integrity.\" tooltip=\"\"></i></small>\n",
       "</br></td>\n",
       "<td class=\"nowrap\">715.5 MB</td>\n",
       "<td class=\"nowrap\"><span class=\"pull-right\"><a class=\"btn btn-xs btn-default\" href=\"/record/3966543/files/audio-8.tar.gz?download=1\"><i class=\"fa fa-download\"></i> Download</a></span></td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "<a class=\"filename\" href=\"/record/3966543/files/audio-9.tar.gz?download=1\">audio-9.tar.gz</a>\n",
       "<br><small class=\"text-muted nowrap\">md5:959f7edfbb26eadad9865069f38aa9dd <i class=\"fa fa-question-circle text-muted\" data-placement=\"top\" data-toggle=\"tooltip\" title=\"This is the file fingerprint (MD5 checksum), which can be used to verify the file integrity.\" tooltip=\"\"></i></small>\n",
       "</br></td>\n",
       "<td class=\"nowrap\">715.3 MB</td>\n",
       "<td class=\"nowrap\"><span class=\"pull-right\"><a class=\"btn btn-xs btn-default\" href=\"/record/3966543/files/audio-9.tar.gz?download=1\"><i class=\"fa fa-download\"></i> Download</a></span></td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "<a class=\"filename\" href=\"/record/3966543/files/dcase-ust-taxonomy.yaml?download=1\">dcase-ust-taxonomy.yaml</a>\n",
       "<br><small class=\"text-muted nowrap\">md5:6c1cca1c4c383a6ebb0cb71cb74fe3a9 <i class=\"fa fa-question-circle text-muted\" data-placement=\"top\" data-toggle=\"tooltip\" title=\"This is the file fingerprint (MD5 checksum), which can be used to verify the file integrity.\" tooltip=\"\"></i></small>\n",
       "</br></td>\n",
       "<td class=\"nowrap\">1.1 kB</td>\n",
       "<td class=\"nowrap\"><span class=\"pull-right\"><a class=\"btn btn-xs btn-default\" href=\"/record/3966543/files/dcase-ust-taxonomy.yaml?download=1\"><i class=\"fa fa-download\"></i> Download</a></span></td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "<a class=\"filename\" href=\"/record/3966543/files/README.md?download=1\">README.md</a>\n",
       "<br><small class=\"text-muted nowrap\">md5:bdfa3a4c0d90053f622b52afa0fc86f7 <i class=\"fa fa-question-circle text-muted\" data-placement=\"top\" data-toggle=\"tooltip\" title=\"This is the file fingerprint (MD5 checksum), which can be used to verify the file integrity.\" tooltip=\"\"></i></small>\n",
       "</br></td>\n",
       "<td class=\"nowrap\">11.3 kB</td>\n",
       "<td class=\"nowrap\"><span class=\"pull-right\"><button class=\"btn preview-link btn-xs btn-default\" data-filename=\"README.md\"><i class=\"fa fa-eye\"></i> Preview</button> <a class=\"btn btn-xs btn-default\" href=\"/record/3966543/files/README.md?download=1\"><i class=\"fa fa-download\"></i> Download</a></span></td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "<a class=\"filename\" href=\"/record/3966543/files/unpack_audio.sh?download=1\">unpack_audio.sh</a>\n",
       "<br><small class=\"text-muted nowrap\">md5:c78525e474d920b4b6af6385ee94bdc9 <i class=\"fa fa-question-circle text-muted\" data-placement=\"top\" data-toggle=\"tooltip\" title=\"This is the file fingerprint (MD5 checksum), which can be used to verify the file integrity.\" tooltip=\"\"></i></small>\n",
       "</br></td>\n",
       "<td class=\"nowrap\">144 Bytes</td>\n",
       "<td class=\"nowrap\"><span class=\"pull-right\"><a class=\"btn btn-xs btn-default\" href=\"/record/3966543/files/unpack_audio.sh?download=1\"><i class=\"fa fa-download\"></i> Download</a></span></td>\n",
       "</tr></tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div id=\"citations\">\n",
       "<invenio-search disable-url-handler=\"true\" search-endpoint=\"https://zenodo-broker.web.cern.ch/api/relationships\" search-extra-params='{\"group_by\": \"version\", \"id\": \"10.5281/zenodo.2590741\", \"page\": 1, \"size\": 10}' search-headers='{\"Accept\": \"application/json\"}' search-hidden-params='{\"relation\": \"isCitedBy\", \"scheme\": \"doi\"}'>\n",
       "<div class=\"panel panel-default\" id=\"citation\">\n",
       "<div class=\"panel-heading\">\n",
       "<!-- Beta ribbon -->\n",
       "<div class=\"row\" style=\"margin-bottom: -15px;\">\n",
       "<div class=\"col-sm-1\">\n",
       "<div class=\"ribbon-wrapper-green\">\n",
       "<a href=\"https://help.zenodo.org/#citations\" rel=\"noopener noreferrer\" target=\"_blank\">\n",
       "<div class=\"corner-ribbon top-left ribbon-green\">Beta</div>\n",
       "</a>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"col-sm-11\" style=\"margin-top: 5px;\">\n",
       "<a class=\"panel-toggle\" data-toggle=\"collapse\" href=\"#collapseCitations\" style=\"margin-left:-25px;\">\n",
       "              Citations\n",
       "            </a>\n",
       "<a href=\"https://help.zenodo.org/#citations\" rel=\"noopener noreferrer\" target=\"_blank\">\n",
       "<i class=\"fa fa-question-circle\"></i>\n",
       "</a>\n",
       "<small class=\"text-muted\">\n",
       "<invenio-search-count template=\"/static/templates/citations/count.html\">\n",
       "</invenio-search-count>\n",
       "</small>\n",
       "<a class=\"panel-toggle\" data-toggle=\"collapse\" href=\"#collapseCitations\" style=\"margin-left:-25px;\">\n",
       "<span class=\"pull-right show-on-collapsed\"><i class=\"fa fa-chevron-right\"></i></span>\n",
       "<span class=\"pull-right hide-on-collapsed\"><i class=\"fa fa-chevron-down\"></i></span>\n",
       "</a>\n",
       "</div>\n",
       "</div>\n",
       "<!-- Without beta ribbon -->\n",
       "<!--<a class=\"panel-toggle\" data-toggle=\"collapse\" href=\"#collapseCitations\">-->\n",
       "<!--Citations-->\n",
       "<!--<small class=\"text-muted\">-->\n",
       "<!--<invenio-search-count-->\n",
       "<!--template=\"/static/templates/citations/count.html\">-->\n",
       "<!--</invenio-search-count>-->\n",
       "<!--</small>-->\n",
       "<!--<span class=\"pull-right show-on-collapsed\"><i class=\"fa fa-chevron-right\"></i></span>-->\n",
       "<!--<span class=\"pull-right hide-on-collapsed\"><i class=\"fa fa-chevron-down\"></i></span>-->\n",
       "<!--</a>-->\n",
       "</div>\n",
       "<div class=\"collapse in\" id=\"collapseCitations\">\n",
       "<div class=\"search-page\">\n",
       "<div class=\"container-fluid facets\">\n",
       "<div class=\"row\">\n",
       "<div class=\"col-sm-9\">\n",
       "<invenio-search-facets ng-init=\"vm.record_id='10.5281/zenodo.3966543'; vm.version_id='10.5281/zenodo.2590741'\" template=\"/static/templates/citations/facets.html\">\n",
       "</invenio-search-facets>\n",
       "</div>\n",
       "<div class=\"col-sm-3\">\n",
       "<invenio-search-bar placeholder=\"Search\" template=\"/static/templates/invenio_search_ui/searchbar.html\">\n",
       "</invenio-search-bar>\n",
       "</div>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"container-fluid\">\n",
       "<invenio-search-error message=\"Error\" template=\"/static/templates/zenodo_search_ui/error.html\">\n",
       "</invenio-search-error>\n",
       "</div>\n",
       "<invenio-search-results template=\"/static/templates/citations/results.html\">\n",
       "</invenio-search-results>\n",
       "<div class=\"row\">\n",
       "<div class=\"col-md-2 col-sm-12\">\n",
       "</div>\n",
       "<div class=\"col-md-7 col-sm-12 text-center\">\n",
       "<invenio-search-pagination template=\"/static/templates/citations/pagination.html\">\n",
       "</invenio-search-pagination>\n",
       "</div>\n",
       "<div class=\"col-md-3 col-sm-12\" style=\"padding-top: 5px;\">\n",
       "<invenio-search-select-box available-options='{\n",
       "                  \"options\": [\n",
       "                     {\n",
       "                       \"title\": \"10\",\n",
       "                       \"value\": \"10\"\n",
       "                     },\n",
       "                     {\n",
       "                       \"title\": \"20\",\n",
       "                       \"value\": \"20\"\n",
       "                     },\n",
       "                     {\n",
       "                       \"title\": \"50\",\n",
       "                       \"value\": \"50\"\n",
       "                     }\n",
       "                  ]}' sort-key=\"size\" template=\"/static/templates/citations/selectbox.html\">\n",
       "</invenio-search-select-box>\n",
       "</div>\n",
       "</div>\n",
       "</div>\n",
       "</div>\n",
       "</div>\n",
       "</invenio-search>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"col-sm-4 col-md-4 col-right\">\n",
       "<div class=\"well\"><!-- Stats -->\n",
       "<div class=\"row stats-box\">\n",
       "<div id=\"accordion\">\n",
       "<!-- Banner --><div class=\"row\">\n",
       "<div class=\"col-sm-6\">\n",
       "<span class=\"stats-data\">9,876</span>\n",
       "</div>\n",
       "<div class=\"col-sm-6\">\n",
       "<span class=\"stats-data\">19,828</span>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"row\">\n",
       "<div class=\"col-sm-6\">\n",
       "<i class=\"fa fa-eye\" data-original-title=\"Total views.\" data-placement=\"top\" data-toggle=\"tooltip\" title=\"Total views.\" tooltip=\"\"></i> views\n",
       "      </div>\n",
       "<div class=\"col-sm-6\">\n",
       "<i class=\"fa fa-download\" data-original-title=\"Total downloads.\" data-placement=\"top\" data-toggle=\"tooltip\" title=\"Total downloads.\" tooltip=\"\"></i> downloads\n",
       "      </div>\n",
       "</div>\n",
       "<!-- Collapsed details -->\n",
       "<div class=\"row\" id=\"toggle-stats\">\n",
       "<div class=\"col-sm-12\">\n",
       "<a aria-controls=\"collapse-stats\" aria-expanded=\"true\" class=\"panel-toggle\" data-target=\"#collapse-stats\" data-toggle=\"collapse\" style=\"cursor: pointer;\">\n",
       "          See more details...\n",
       "        </a>\n",
       "</div>\n",
       "</div>\n",
       "<div aria-labelledby=\"toggle-stats\" class=\"collapse\" data-parent=\"#accordion\" id=\"collapse-stats\">\n",
       "<table class=\"table stats-table\">\n",
       "<!-- Skip table header if no versions --><tr>\n",
       "<th></th>\n",
       "<th>All versions</th>\n",
       "<th>This version</th>\n",
       "</tr><tr>\n",
       "<td>Views <i class=\"fa fa-question-circle text-muted\" data-original-title=\"Total views.\" data-placement=\"top\" data-toggle=\"tooltip\" title=\"Total views.\" tooltip=\"\"></i></td><td>9,876</td><td>1,939</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>Downloads <i class=\"fa fa-question-circle text-muted\" data-original-title=\"Total downloads.\" data-placement=\"top\" data-toggle=\"tooltip\" title=\"Total downloads.\" tooltip=\"\"></i></td><td>19,828</td><td>3,279</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>Data volume <i class=\"fa fa-question-circle text-muted\" data-original-title=\"Total download volume.\" data-placement=\"top\" data-toggle=\"tooltip\" title=\"Total download volume.\" tooltip=\"\"></i></td><td>63.6 TB</td><td>1.8 TB</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>Unique views <i class=\"fa fa-question-circle text-muted\" data-original-title=\"Views in one hour user-sessions are counted only once.\" data-placement=\"top\" data-toggle=\"tooltip\" title=\"Views in one hour user-sessions are counted only once.\" tooltip=\"\"></i></td><td>7,541</td><td>1,647</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>Unique downloads <i class=\"fa fa-question-circle text-muted\" data-original-title=\"Downloads in one hour user-sessions are counted only once.\" data-placement=\"top\" data-toggle=\"tooltip\" title=\"Downloads in one hour user-sessions are counted only once.\" tooltip=\"\"></i></td><td>5,698</td><td>704</td>\n",
       "</tr>\n",
       "</table>\n",
       "<div class=\"row\">\n",
       "<a href=\"https://help.zenodo.org/#statistics\">More info on how stats are collected.</a>\n",
       "</div>\n",
       "</div>\n",
       "</div>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"well metadata\">\n",
       "<small class=\"text-muted\">Indexed in</small>\n",
       "<a href=\"https://explore.openaire.eu/search/dataset?pid=10.5281/zenodo.3966543\">\n",
       "<img class=\"img-thumbnail\" src=\"/static/img/openaire-horizontal-old.png\" width=\"100%\"/>\n",
       "</a>\n",
       "</div>\n",
       "<div class=\"well metadata\"><dl>\n",
       "<dt>Publication date:</dt>\n",
       "<dd>September 14, 2020</dd>\n",
       "<dt>DOI:</dt>\n",
       "<dd>\n",
       "<span class=\"get-badge\" data-placement=\"bottom\" data-toggle=\"`tooltip\" title=\"Get the DOI badge!\">\n",
       "<img alt=\"10.5281/zenodo.3966543\" data-target=\"[data-modal='10.5281/zenodo.3966543']\" data-toggle=\"modal\" src=\"/badge/DOI/10.5281/zenodo.3966543.svg\"/>\n",
       "</span>\n",
       "<div class=\"modal fade badge-modal\" data-modal=\"10.5281/zenodo.3966543\">\n",
       "<div class=\"modal-dialog\">\n",
       "<div class=\"modal-content\">\n",
       "<div class=\"modal-body\">\n",
       "<h4>Zenodo DOI Badge</h4>\n",
       "<h4>\n",
       "<small>DOI</small>\n",
       "</h4>\n",
       "<h4>\n",
       "<pre>10.5281/zenodo.3966543</pre>\n",
       "</h4>\n",
       "<h4>\n",
       "<small>Markdown</small>\n",
       "</h4>\n",
       "<h4>\n",
       "<pre>[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3966543.svg)](https://doi.org/10.5281/zenodo.3966543)</pre>\n",
       "</h4>\n",
       "<h4>\n",
       "<small>reStructedText</small>\n",
       "</h4>\n",
       "<h4>\n",
       "<pre>.. image:: https://zenodo.org/badge/DOI/10.5281/zenodo.3966543.svg\n",
       "   :target: https://doi.org/10.5281/zenodo.3966543</pre>\n",
       "</h4>\n",
       "<h4>\n",
       "<small>HTML</small>\n",
       "</h4>\n",
       "<h4>\n",
       "<pre>&lt;a href=\"https://doi.org/10.5281/zenodo.3966543\"&gt;&lt;img src=\"https://zenodo.org/badge/DOI/10.5281/zenodo.3966543.svg\" alt=\"DOI\"&gt;&lt;/a&gt;</pre>\n",
       "</h4>\n",
       "<h4>\n",
       "<small>Image URL</small>\n",
       "</h4>\n",
       "<h4>\n",
       "<pre>https://zenodo.org/badge/DOI/10.5281/zenodo.3966543.svg</pre>\n",
       "</h4>\n",
       "<h4>\n",
       "<small>Target URL</small>\n",
       "</h4>\n",
       "<h4>\n",
       "<pre>https://doi.org/10.5281/zenodo.3966543</pre>\n",
       "</h4>\n",
       "</div>\n",
       "</div>\n",
       "</div>\n",
       "</div>\n",
       "</dd>\n",
       "<dt>Keyword(s):</dt>\n",
       "<dd>\n",
       "<a class=\"label-link\" href=\"/search?q=keywords%3A%22urban+sound%22\">\n",
       "<span class=\"label label-default\">urban sound</span>\n",
       "</a>\n",
       "<a class=\"label-link\" href=\"/search?q=keywords%3A%22noise+pollution%22\">\n",
       "<span class=\"label label-default\">noise pollution</span>\n",
       "</a>\n",
       "<a class=\"label-link\" href=\"/search?q=keywords%3A%22machine+listening%22\">\n",
       "<span class=\"label label-default\">machine listening</span>\n",
       "</a>\n",
       "<a class=\"label-link\" href=\"/search?q=keywords%3A%22computer+audition%22\">\n",
       "<span class=\"label label-default\">computer audition</span>\n",
       "</a>\n",
       "<a class=\"label-link\" href=\"/search?q=keywords%3A%22longterm+spatiotemporal+context%22\">\n",
       "<span class=\"label label-default\">longterm spatiotemporal context</span>\n",
       "</a>\n",
       "<a class=\"label-link\" href=\"/search?q=keywords%3A%22sound+tagging%22\">\n",
       "<span class=\"label label-default\">sound tagging</span>\n",
       "</a>\n",
       "</dd>\n",
       "<dt>Communities:</dt>\n",
       "<dd>\n",
       "<ul class=\"list-unstyled\">\n",
       "<li><a href=\"/communities/dcase/\">Detection and Classification of Acoustic Scenes and Events</a></li>\n",
       "</ul>\n",
       "</dd>\n",
       "<dt>License (for files):</dt>\n",
       "<dd><a href=\"https://creativecommons.org/licenses/by/4.0/legalcode\" rel=\"license\"><i class=\"fa fa-external-link\"></i> Creative Commons Attribution 4.0 International</a></dd>\n",
       "</dl>\n",
       "</div>\n",
       "<div class=\"well metadata\">\n",
       "<h4>Versions</h4>\n",
       "<table class=\"table\">\n",
       "<tr class=\"info\">\n",
       "<td>\n",
       "<a href=\"/record/3966543\">Version 2.3.0 </a>\n",
       "<small class=\"text-muted\">10.5281/zenodo.3966543</small>\n",
       "</td>\n",
       "<td align=\"right\"><small class=\"text-muted\">Sep 14, 2020</small></td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "<a href=\"/record/3873076\">Version 2.2.0 </a>\n",
       "<small class=\"text-muted\">10.5281/zenodo.3873076</small>\n",
       "</td>\n",
       "<td align=\"right\"><small class=\"text-muted\">Jun 2, 2020</small></td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "<a href=\"/record/3693077\">Version 2.1.0 </a>\n",
       "<small class=\"text-muted\">10.5281/zenodo.3693077</small>\n",
       "</td>\n",
       "<td align=\"right\"><small class=\"text-muted\">Mar 1, 2020</small></td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "<a href=\"/record/3692954\">Version 1.0.0 </a>\n",
       "<small class=\"text-muted\">10.5281/zenodo.3692954</small>\n",
       "</td>\n",
       "<td align=\"right\"><small class=\"text-muted\">Feb 29, 2020</small></td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>\n",
       "<a href=\"/record/3338310\">Version 0.4.0 </a>\n",
       "<small class=\"text-muted\">10.5281/zenodo.3338310</small>\n",
       "</td>\n",
       "<td align=\"right\"><small class=\"text-muted\">Jul 16, 2019</small></td>\n",
       "</tr>\n",
       "<tr><td align=\"center\" colspan=\"2\"><a href=\"/search?q=conceptrecid%3A%222590741%22&amp;sort=-version&amp;all_versions=True\"><small>View all 8 versions</small></a></td></tr>\n",
       "</table>\n",
       "<small>\n",
       "<strong>Cite all versions?</strong> You can cite all versions by using the DOI <a href=\"https://doi.org/10.5281/zenodo.2590741\">10.5281/zenodo.2590741</a>. This DOI represents all versions, and will always resolve to the latest one. <a href=\"http://help.zenodo.org/#versioning\">Read more</a>.\n",
       "    </small>\n",
       "</div>\n",
       "<div class=\"well\"><h4>Share</h4>\n",
       "<!-- AddThis Button BEGIN -->\n",
       "<div addthis:url=\"https://doi.org/10.5281/zenodo.3966543\" class=\"addthis_toolbox addthis_default_style addthis_32x32_style\">\n",
       "<a class=\"addthis_button_mendeley\"></a>\n",
       "<a class=\"addthis_button_citeulike\"></a>\n",
       "<a class=\"addthis_button_twitter\"></a>\n",
       "<a class=\"addthis_button_preferred_1\"></a>\n",
       "<a class=\"addthis_button_preferred_2\"></a>\n",
       "<a class=\"addthis_button_compact\"></a>\n",
       "</div>\n",
       "<!-- AddThis Button END -->\n",
       "<h4>Cite as</h4>\n",
       "<div id=\"invenio-csl\">\n",
       "<invenio-csl ng-init=\"vm.citationResult = 'Mark Cartwright, Jason Cramer, Ana Elisa Mendez Mendez, Yu Wang, Ho-Hsiang Wu, Vincent Lostanlen, Magdalena Fuentes, Graham Dove, Charlie Mydlarz, Justin Salamon, Oded Nov, &amp; Juan Pablo Bello. (2020). SONYC Urban Sound Tagging (SONYC-UST): a multilabel dataset from an urban acoustic sensor network (2.3.0) [Data set]. Zenodo. https://doi.org/10.5281/zenodo.3966543'\">\n",
       "<invenio-csl-citeproc endpoint=\"/api/records/3966543\" template=\"/static/templates/invenio_csl/citeproc.html\"></invenio-csl-citeproc>\n",
       "<invenio-csl-error template=\"/static/node_modules/invenio-csl-js/dist/templates/error.html\"></invenio-csl-error>\n",
       "<invenio-csl-loading template=\"/static/node_modules/invenio-csl-js/dist/templates/loading.html\"></invenio-csl-loading>\n",
       "<invenio-csl-typeahead item-template=\"/static/templates/invenio_csl/item.html\" lazy=\"true\" placeholder=\"Start typing a citation style...\" remote=\"/api/csl/styles\" template=\"/static/node_modules/invenio-csl-js/dist/templates/typeahead.html\">\n",
       "</invenio-csl-typeahead>\n",
       "</invenio-csl>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"well\"><h4>Export</h4>\n",
       "<ul class=\"list-inline\">\n",
       "<li><a href=\"/record/3966543/export/hx\">BibTeX</a></li>\n",
       "<li><a href=\"/record/3966543/export/csl\">CSL</a></li>\n",
       "<li><a href=\"/record/3966543/export/dcite4\">DataCite</a></li>\n",
       "<li><a href=\"/record/3966543/export/xd\">Dublin Core</a></li>\n",
       "<li><a href=\"/record/3966543/export/dcat\">DCAT</a></li>\n",
       "<li><a href=\"/record/3966543/export/json\">JSON</a></li>\n",
       "<li><a href=\"/record/3966543/export/schemaorg_jsonld\">JSON-LD</a></li>\n",
       "<li><a href=\"/record/3966543/export/geojson\">GeoJSON</a></li>\n",
       "<li><a href=\"/record/3966543/export/xm\">MARCXML</a></li>\n",
       "<li><a href=\"https://www.mendeley.com/import/?url=https://zenodo.org/record/3966543\"><i class=\"fa fa-external-link\"></i> Mendeley</a></li>\n",
       "</ul>\n",
       "</div>\n",
       "</div>\n",
       "</div>\n",
       "</div>\n",
       "<footer class=\"footer\">\n",
       "<div class=\"menu-wrapper\">\n",
       "<div class=\"container\">\n",
       "<div class=\"row footer-menu\">\n",
       "<div class=\"col-xs-12 col-md-8\">\n",
       "<div class=\"row\">\n",
       "<div class=\"col-xs-2 col-md-2\">\n",
       "<h5>About</h5>\n",
       "<ul class=\"list-unstyled\">\n",
       "<li><a href=\"http://about.zenodo.org\">About</a></li>\n",
       "<li><a href=\"http://about.zenodo.org/policies\">Policies</a></li>\n",
       "<li><a href=\"http://about.zenodo.org/infrastructure\">Infrastructure</a></li>\n",
       "<li><a href=\"http://about.zenodo.org/principles\">Principles</a></li>\n",
       "<li><a href=\"http://about.zenodo.org/contact\">Contact</a></li>\n",
       "</ul>\n",
       "</div>\n",
       "<div class=\"col-xs-2 col-md-2\">\n",
       "<h5>Blog</h5>\n",
       "<ul class=\"list-unstyled\">\n",
       "<li><a href=\"http://blog.zenodo.org\">Blog</a></li>\n",
       "</ul>\n",
       "</div>\n",
       "<div class=\"col-xs-2 col-md-2\">\n",
       "<h5>Help</h5>\n",
       "<ul class=\"list-unstyled\">\n",
       "<li><a href=\"http://help.zenodo.org\">FAQ</a></li>\n",
       "<li><a href=\"http://help.zenodo.org/features\">Features</a></li>\n",
       "<li><a href=\"/support\">Support</a></li>\n",
       "</ul>\n",
       "</div>\n",
       "<div class=\"col-xs-2 col-md-2\">\n",
       "<h5>Developers</h5>\n",
       "<ul class=\"list-unstyled\">\n",
       "<li><a href=\"http://developers.zenodo.org\">REST API</a></li>\n",
       "<li><a href=\"http://developers.zenodo.org#oai-pmh\">OAI-PMH</a></li>\n",
       "</ul>\n",
       "</div>\n",
       "<div class=\"col-xs-2 col-md-2\">\n",
       "<h5>Contribute</h5>\n",
       "<ul class=\"list-unstyled\">\n",
       "<li><a href=\"https://github.com/zenodo/zenodo\"><i class=\"fa fa-external-link\"></i> GitHub</a></li>\n",
       "<li><a href=\"/donate\"><i class=\"fa fa-external-link\"></i> Donate</a></li>\n",
       "</ul>\n",
       "</div>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"col-xs-12 col-md-4\">\n",
       "<div class=\"pull-right-md text-center-sm text-center-xs\">\n",
       "<h5>Funded by</h5>\n",
       "<ul class=\"list-inline\">\n",
       "<li><a href=\"https://home.cern\"><img height=\"60\" src=\"/static/img/cern.svg\" width=\"60\"/></a></li>\n",
       "<li><a href=\"https://www.openaire.eu\"><img src=\"/static/img/openaire.png\" width=\"80\"/></a></li>\n",
       "<li><a href=\"https://ec.europa.eu/programmes/horizon2020/\"><img height=\"60\" src=\"/static/img/eu.png\" width=\"88\"/></a></li>\n",
       "</ul>\n",
       "</div>\n",
       "</div>\n",
       "</div>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"container\">\n",
       "<div class=\"row\">\n",
       "<div class=\"col-xs-12 col-sm-6 col-sm-push-6\">\n",
       "<div class=\"pull-right-sm text-center-xs\">\n",
       "<ul class=\"list-inline\">\n",
       "<li><a href=\"https://stats.uptimerobot.com/vlYOVuWgM\">Status</a></li>\n",
       "<li><a href=\"http://about.zenodo.org/privacy-policy\">Privacy policy</a></li>\n",
       "<li><a href=\"http://about.zenodo.org/terms\">Terms of Use</a></li>\n",
       "<li><a href=\"/support\">Support</a></li>\n",
       "</ul>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"col-xs-12 col-sm-6 col-sm-pull-6 text-center-xs\">\n",
       "<p><a href=\"http://creativecommons.org/licenses/by/4.0/\" rel=\"license\" title=\"Except where otherwise noted, content on this site is licensed under a Creative Commons Attribution 4.0 International License.\"><img alt=\"Creative Commons Licence\" height=\"20\" src=\"https://i.creativecommons.org/l/by/4.0/88x31.png\"/></a>  Powered by <a href=\"https://home.cern/science/computing/data-centre\">CERN Data Centre</a> &amp; <a href=\"http://inveniosoftware.org\">Invenio</a>.</p>\n",
       "</div>\n",
       "</div>\n",
       "</div>\n",
       "</footer>\n",
       "<script type=\"application/ld+json\">{\"@context\": \"https://schema.org/\", \"@id\": \"https://doi.org/10.5281/zenodo.3966543\", \"@type\": \"Dataset\", \"creator\": [{\"@id\": \"https://orcid.org/0000-0002-5908-390X\", \"@type\": \"Person\", \"affiliation\": \"New York University\", \"name\": \"Mark Cartwright\"}, {\"@id\": \"https://orcid.org/0000-0001-5288-9399\", \"@type\": \"Person\", \"affiliation\": \"New York University\", \"name\": \"Jason Cramer\"}, {\"@id\": \"https://orcid.org/0000-0002-4861-5616\", \"@type\": \"Person\", \"affiliation\": \"New York University\", \"name\": \"Ana Elisa Mendez Mendez\"}, {\"@id\": \"https://orcid.org/0000-0002-1615-5141\", \"@type\": \"Person\", \"affiliation\": \"New York University\", \"name\": \"Yu Wang\"}, {\"@id\": \"https://orcid.org/0000-0002-1102-074X\", \"@type\": \"Person\", \"affiliation\": \"New York University\", \"name\": \"Ho-Hsiang Wu\"}, {\"@id\": \"https://orcid.org/0000-0003-0580-1651\", \"@type\": \"Person\", \"affiliation\": \"Cornel University\", \"name\": \"Vincent Lostanlen\"}, {\"@id\": \"https://orcid.org/0000-0003-4506-6639\", \"@type\": \"Person\", \"affiliation\": \"New York University\", \"name\": \"Magdalena Fuentes\"}, {\"@id\": \"https://orcid.org/0000-0002-3551-0209\", \"@type\": \"Person\", \"affiliation\": \"New York University\", \"name\": \"Graham Dove\"}, {\"@id\": \"https://orcid.org/0000-0001-7061-0638\", \"@type\": \"Person\", \"affiliation\": \"New York University\", \"name\": \"Charlie Mydlarz\"}, {\"@id\": \"https://orcid.org/0000-0001-6345-4593\", \"@type\": \"Person\", \"affiliation\": \"New York University\", \"name\": \"Justin Salamon\"}, {\"@id\": \"https://orcid.org/0000-0001-6410-2995\", \"@type\": \"Person\", \"affiliation\": \"New York University\", \"name\": \"Oded Nov\"}, {\"@id\": \"https://orcid.org/0000-0001-8561-5204\", \"@type\": \"Person\", \"affiliation\": \"New York University\", \"name\": \"Juan Pablo Bello\"}], \"datePublished\": \"2020-09-14\", \"description\": \"\\u003cp\\u003e\\u003cstrong\\u003eSONYC Urban Sound Tagging (SONYC-UST): a multilabel dataset from an urban acoustic sensor network\\u003c/strong\\u003e\\u003c/p\\u003e\\n\\n\\u003cp\\u003eVersion 2.3, September 2020\\u003c/p\\u003e\\n\\n\\u003cp\\u003e\\u0026nbsp;\\u003c/p\\u003e\\n\\n\\u003cp\\u003e\\u003cstrong\\u003eCreated by\\u003c/strong\\u003e\\u003c/p\\u003e\\n\\n\\u003cp\\u003eMark Cartwright (1,2,3), Jason Cramer (1), Ana Elisa Mendez Mendez (1), Yu Wang (1), Ho-Hsiang Wu (1), Vincent Lostanlen (1,2,4), Magdalena Fuentes (1), Graham Dove (2), Charlie Mydlarz (1,2), Justin Salamon (5), Oded Nov (6), Juan Pablo Bello (1,2,3)\\u003c/p\\u003e\\n\\n\\u003col\\u003e\\n\\t\\u003cli\\u003eMusic and Audio Research Lab, New York University\\u003c/li\\u003e\\n\\t\\u003cli\\u003eCenter for Urban Science and Progress, New York University\\u003c/li\\u003e\\n\\t\\u003cli\\u003eDepartment of Computer Science and Engineering, New York University\\u003c/li\\u003e\\n\\t\\u003cli\\u003eCornell Lab of Ornithology\\u003c/li\\u003e\\n\\t\\u003cli\\u003eAdobe Research\\u003c/li\\u003e\\n\\t\\u003cli\\u003eDepartment of Technology Management and Innovation, New York University\\u003c/li\\u003e\\n\\u003c/ol\\u003e\\n\\n\\u003cp\\u003e\\u0026nbsp;\\u003c/p\\u003e\\n\\n\\u003cp\\u003e\\u003cstrong\\u003ePublication\\u003c/strong\\u003e\\u003c/p\\u003e\\n\\n\\u003cp\\u003eIf using this data in an academic work, please reference the DOI and version, as well as cite the following paper, which presented the data collection procedure and the first version of the dataset:\\u003c/p\\u003e\\n\\n\\u003cp\\u003eCartwright, M., Cramer, J., Mendez, A.E.M., Wang, Y., Wu, H., Lostanlen, V., Fuentes, M., Dove, G., Mydlarz, C., Salamon, J., Nov, O., Bello, J.P. SONYC-UST-V2: An Urban Sound Tagging Dataset with Spatiotemporal Context. In \\u003cem\\u003eProceedings of the Workshop on Detection and Classification of Acoustic Scenes and Events (DCASE)\\u003c/em\\u003e, 2020.\\u003cbr\\u003e\\n\\u003ca href=\\\"https://arxiv.org/abs/2009.05188\\\"\\u003e[pdf]\\u003c/a\\u003e\\u003c/p\\u003e\\n\\n\\u003cp\\u003e\\u0026nbsp;\\u003c/p\\u003e\\n\\n\\u003cp\\u003e\\u003cstrong\\u003eDescription\\u003c/strong\\u003e\\u003c/p\\u003e\\n\\n\\u003cp\\u003eSONYC Urban Sound Tagging (SONYC-UST) is a dataset for the development and evaluation of machine listening systems for realistic urban noise monitoring. The audio was recorded from the \\u003ca href=\\\"https://wp.nyu.edu/sonyc\\\"\\u003eSONYC\\u003c/a\\u003e\\u0026nbsp;acoustic sensor network. Volunteers on the \\u0026nbsp;\\u003ca href=\\\"https://zooniverse.org\\\"\\u003eZooniverse\\u003c/a\\u003e\\u0026nbsp;citizen science platform tagged the presence of 23 classes that were chosen in consultation with the New York City Department of Environmental Protection. These 23 fine-grained classes can be grouped into 8 coarse-grained classes. The recordings are split into three sets: training, validation, and test. The training and validation sets are disjoint with respect to the sensor from which each recording came, and the test set is displaced in time. For increased reliability, three volunteers annotated each recording. In addition, members of the SONYC team subsequently created a subset of verified, ground-truth tags using a two-stage annotation procedure in which two annotators independently tagged and then collectively resolved any disagreements. This subset of recordings with verified annotations intersects with all three recording splits. All of the recordings in the test set have these verified annotations.\\u0026nbsp; In v2 version of this dataset, we have also included coarse spatiotemporal context information to aid in tag prediction when time and location is known. For more details on the motivation and creation of this dataset see the \\u003ca href=\\\"http://dcase.community/challenge2020/task-urban-sound-tagging-with-spatiotemporal-context\\\"\\u003eDCASE 2020 Urban Sound Tagging with Spatiotemporal Context Task website\\u003c/a\\u003e.\\u003c/p\\u003e\\n\\n\\u003cp\\u003e\\u0026nbsp;\\u003c/p\\u003e\\n\\n\\u003cp\\u003e\\u003cstrong\\u003eAudio data\\u003c/strong\\u003e\\u003c/p\\u003e\\n\\n\\u003cp\\u003eThe provided audio has been acquired using the SONYC acoustic sensor network for urban noise pollution monitoring. Over 60 different sensors have been deployed in New York City, and these sensors have collectively gathered the equivalent of over 50 years of audio data, of which we provide a small subset. The data was sampled by selecting the nearest neighbors on VGGish features of recordings known to have classes of interest. All recordings are 10 seconds and were recorded with identical microphones at identical gain settings. To maintain privacy, we quantized the spatial information to the level of a city block, and we quantized the temporal information to the level of an hour. We also limited the occurrence of recordings with positive human voice annotations to one per hour per sensor.\\u003c/p\\u003e\\n\\n\\u003cp\\u003e\\u0026nbsp;\\u003c/p\\u003e\\n\\n\\u003cp\\u003e\\u003cstrong\\u003eLabel taxonomy\\u003c/strong\\u003e\\u003c/p\\u003e\\n\\n\\u003cp\\u003eThe label taxonomy is as follows:\\u003c/p\\u003e\\n\\n\\u003col\\u003e\\n\\t\\u003cli\\u003eengine\\u003cbr\\u003e\\n\\t1: small-sounding-engine\\u003cbr\\u003e\\n\\t2: medium-sounding-engine\\u003cbr\\u003e\\n\\t3: large-sounding-engine\\u003cbr\\u003e\\n\\tX: engine-of-uncertain-size\\u003c/li\\u003e\\n\\t\\u003cli\\u003emachinery-impact\\u003cbr\\u003e\\n\\t1: rock-drill\\u003cbr\\u003e\\n\\t2: jackhammer\\u003cbr\\u003e\\n\\t3: hoe-ram\\u003cbr\\u003e\\n\\t4: pile-driver\\u003cbr\\u003e\\n\\tX: other-unknown-impact-machinery\\u003c/li\\u003e\\n\\t\\u003cli\\u003enon-machinery-impact\\u003cbr\\u003e\\n\\t1: non-machinery-impact\\u003c/li\\u003e\\n\\t\\u003cli\\u003epowered-saw\\u003cbr\\u003e\\n\\t1: chainsaw\\u003cbr\\u003e\\n\\t2: small-medium-rotating-saw\\u003cbr\\u003e\\n\\t3: large-rotating-saw\\u003cbr\\u003e\\n\\tX: other-unknown-powered-saw\\u003c/li\\u003e\\n\\t\\u003cli\\u003ealert-signal\\u003cbr\\u003e\\n\\t1: car-horn\\u003cbr\\u003e\\n\\t2: car-alarm\\u003cbr\\u003e\\n\\t3: siren\\u003cbr\\u003e\\n\\t4: reverse-beeper\\u003cbr\\u003e\\n\\tX: other-unknown-alert-signal\\u003c/li\\u003e\\n\\t\\u003cli\\u003emusic\\u003cbr\\u003e\\n\\t1: stationary-music\\u003cbr\\u003e\\n\\t2: mobile-music\\u003cbr\\u003e\\n\\t3: ice-cream-truck\\u003cbr\\u003e\\n\\tX: music-from-uncertain-source\\u003c/li\\u003e\\n\\t\\u003cli\\u003ehuman-voice\\u003cbr\\u003e\\n\\t1: person-or-small-group-talking\\u003cbr\\u003e\\n\\t2: person-or-small-group-shouting\\u003cbr\\u003e\\n\\t3: large-crowd\\u003cbr\\u003e\\n\\t4: amplified-speech\\u003cbr\\u003e\\n\\tX: other-unknown-human-voice\\u003c/li\\u003e\\n\\t\\u003cli\\u003edog\\u003cbr\\u003e\\n\\t1: dog-barking-whining\\u003c/li\\u003e\\n\\u003c/ol\\u003e\\n\\n\\u003cp\\u003eThe classes preceded by an \\u003ccode\\u003eX\\u003c/code\\u003e code indicate when an annotator was able to identify the coarse class, but couldn\\u0026rsquo;t identify the fine class because either they were uncertain which fine class it was or the fine class was not included in the taxonomy. \\u003ccode\\u003edcase-ust-taxonomy.yaml\\u003c/code\\u003e contains this taxonomy in an easily machine-readable form.\\u003c/p\\u003e\\n\\n\\u003cp\\u003e\\u0026nbsp;\\u003c/p\\u003e\\n\\n\\u003cp\\u003e\\u003cstrong\\u003eData splits\\u003c/strong\\u003e\\u003c/p\\u003e\\n\\n\\u003cp\\u003eThis release contains a training subset (13538 recordings from 35 sensors), and validation subset (4308 recordings from 9 sensors), and a test subset (669 recordings from 48 sensors). The training and validation subsets are disjoint with respect to the sensor from which each recording came. The sensors in the test set will not disjoint from the training and validation subsets, but the test recordings are displaced in time, occurring after any of the recordings in the training and validation subset. The subset of recordings with verified annotations (1380 recordings) intersects with all three recording splits.\\u0026nbsp; All of the recordings in the test set have these verified annotations.\\u003c/p\\u003e\\n\\n\\u003cp\\u003e\\u0026nbsp;\\u003c/p\\u003e\\n\\n\\u003cp\\u003e\\u003cstrong\\u003eAnnotation data\\u003c/strong\\u003e\\u003c/p\\u003e\\n\\n\\u003cp\\u003eThe annotation data are\\u0026nbsp;contained in \\u003ccode\\u003eannotations.csv\\u003c/code\\u003e, and\\u0026nbsp;encompass the training, validation, and test subsets. Each row in the file represents one multi-label annotation of a recording\\u0026mdash;it could be the annotation of a single citizen science volunteer, a single SONYC team member, or the agreed-upon ground truth by the SONYC team (see the \\u003cem\\u003eannotator_id\\u003c/em\\u003e column description for more information).\\u0026nbsp; Note that since the SONYC team members annotated each class group separately, there may be multiple annotation rows by a single SONYC team annotator for a particular audio recording.\\u003c/p\\u003e\\n\\n\\u003cp\\u003e\\u0026nbsp;\\u003c/p\\u003e\\n\\n\\u003cp\\u003e\\u0026nbsp;\\u003c/p\\u003e\\n\\n\\u003cp\\u003e\\u003cstrong\\u003eColumns\\u003c/strong\\u003e\\u003c/p\\u003e\\n\\n\\u003cp\\u003e\\u003cem\\u003esplit\\u003c/em\\u003e\\u003c/p\\u003e\\n\\n\\u003cp\\u003eThe data split. (\\u003cem\\u003etrain\\u003c/em\\u003e, \\u003cem\\u003evalidate, test\\u003c/em\\u003e)\\u003c/p\\u003e\\n\\n\\u003cp\\u003e\\u003cem\\u003esensor_id\\u003c/em\\u003e\\u003c/p\\u003e\\n\\n\\u003cp\\u003eThe ID of the sensor the recording is from.\\u003c/p\\u003e\\n\\n\\u003cp\\u003e\\u003cem\\u003eaudio_filename\\u003c/em\\u003e\\u003c/p\\u003e\\n\\n\\u003cp\\u003eThe filename of the audio recording\\u003c/p\\u003e\\n\\n\\u003cp\\u003e\\u003cem\\u003eannotator_id\\u003c/em\\u003e\\u003c/p\\u003e\\n\\n\\u003cp\\u003eThe anonymous ID of the annotator. If this value is positive, it is a citizen science volunteer from the Zooniverse platform. If it is negative, it is a SONYC team member. If it is \\u003ccode\\u003e0\\u003c/code\\u003e, then it is the ground truth agreed-upon by the SONYC team.\\u003c/p\\u003e\\n\\n\\u003cp\\u003e\\u003cem\\u003eyear\\u003c/em\\u003e\\u003c/p\\u003e\\n\\n\\u003cp\\u003eThe year the recording is from.\\u003c/p\\u003e\\n\\n\\u003cp\\u003e\\u003cem\\u003eweek\\u003c/em\\u003e\\u003c/p\\u003e\\n\\n\\u003cp\\u003eThe week of the year the recording is from.\\u003c/p\\u003e\\n\\n\\u003cp\\u003e\\u003cem\\u003eday\\u003c/em\\u003e\\u003c/p\\u003e\\n\\n\\u003cp\\u003eThe day of the week the recording is from, with Monday as the start (i.e. \\u003ccode\\u003e0\\u003c/code\\u003e=Monday).\\u003c/p\\u003e\\n\\n\\u003cp\\u003e\\u003cem\\u003ehour\\u003c/em\\u003e\\u003c/p\\u003e\\n\\n\\u003cp\\u003eThe hour of the day the recording is from\\u003c/p\\u003e\\n\\n\\u003cp\\u003e\\u003cem\\u003eborough\\u003c/em\\u003e\\u003cbr\\u003e\\nThe NYC borough in which the sensor is located (\\u003ccode\\u003e1\\u003c/code\\u003e=Manhattan, \\u003ccode\\u003e3\\u003c/code\\u003e=Brooklyn, \\u003ccode\\u003e4\\u003c/code\\u003e=Queens). This corresponds to the first digit in the 10-digit NYC parcel number system known as Borough, Block, Lot (BBL).\\u003c/p\\u003e\\n\\n\\u003cp\\u003e\\u003cem\\u003eblock\\u003c/em\\u003e\\u003c/p\\u003e\\n\\n\\u003cp\\u003eThe NYC block in which the sensor is located. This corresponds to digits 2\\u0026mdash;6 digit in the 10-digit NYC parcel number system known as Borough, Block, Lot (BBL).\\u003c/p\\u003e\\n\\n\\u003cp\\u003e\\u003cem\\u003elatitude\\u003c/em\\u003e\\u003c/p\\u003e\\n\\n\\u003cp\\u003eThe latitude coordinate of the \\u003cstrong\\u003eblock\\u003c/strong\\u003e\\u0026nbsp;in which the sensor is located.\\u003c/p\\u003e\\n\\n\\u003cp\\u003e\\u003cem\\u003elongitude\\u003c/em\\u003e\\u003c/p\\u003e\\n\\n\\u003cp\\u003eThe longitude coordinate of the \\u003cstrong\\u003eblock\\u003c/strong\\u003e\\u0026nbsp;in which the sensor is located.\\u003c/p\\u003e\\n\\n\\u003cp\\u003e\\u003cem\\u003e\\u0026lt;coarse_id\\u0026gt;-\\u0026lt;fine_id\\u0026gt;_\\u0026lt;fine_name\\u0026gt;_presence\\u003c/em\\u003e\\u003c/p\\u003e\\n\\n\\u003cp\\u003eColumns of this form indicate the presence of fine-level class. \\u003ccode\\u003e1\\u003c/code\\u003e if present, \\u003ccode\\u003e0\\u003c/code\\u003e if not present. If \\u003ccode\\u003e-1\\u003c/code\\u003e, then the class was not labeled in this annotation because the annotation was performed by a SONYC team member who only annotated one coarse group of classes at a time when annotating the verified subset.\\u003c/p\\u003e\\n\\n\\u003cp\\u003e\\u003cem\\u003e\\u0026lt;coarse_id\\u0026gt;_\\u0026lt;coarse_name\\u0026gt;_presence\\u003c/em\\u003e\\u003c/p\\u003e\\n\\n\\u003cp\\u003eColumns of this form indicate the presence of a coarse-level class. \\u003ccode\\u003e1\\u003c/code\\u003e if present, \\u003ccode\\u003e0\\u003c/code\\u003e if not present. If \\u003ccode\\u003e-1\\u003c/code\\u003e, then the class was not labeled in this annotation because the annotation was performed by a SONYC team member who only annotated one coarse group of classes at a time when annotating the verified subset. These columns are computed from the fine-level class presence columns and are presented here for convenience when training on only coarse-level classes.\\u003c/p\\u003e\\n\\n\\u003cp\\u003e\\u003cem\\u003e\\u0026lt;coarse_id\\u0026gt;-\\u0026lt;fine_id\\u0026gt;_\\u0026lt;fine_name\\u0026gt;_proximity\\u003c/em\\u003e\\u003c/p\\u003e\\n\\n\\u003cp\\u003eColumns of this form indicate the proximity of a fine-level class. After indicating the presence of a fine-level class, citizen science annotators were asked to indicate the proximity of the sound event to the sensor. Only the citizen science volunteers performed this task, and therefore this data is not included in the verified annotations. This column may take on one of the following four values: (\\u003ccode\\u003enear\\u003c/code\\u003e, \\u003ccode\\u003efar\\u003c/code\\u003e, \\u003ccode\\u003enotsure\\u003c/code\\u003e, \\u003ccode\\u003e-1\\u003c/code\\u003e). If \\u003ccode\\u003e-1\\u003c/code\\u003e, then the proximity was not annotated because either the annotation was not performed by a citizen science volunteer, or the citizen science volunteer did not indicate the presence of the class.\\u003c/p\\u003e\\n\\n\\u003cp\\u003e\\u0026nbsp;\\u003c/p\\u003e\\n\\n\\u003cp\\u003e\\u003cstrong\\u003eConditions of use\\u003c/strong\\u003e\\u003c/p\\u003e\\n\\n\\u003cp\\u003eDataset created by Mark Cartwright, Jason Cramer, Ana Elisa Mendez Mendez, Yu Wang, Ho-Hsiang Wu, Vincent Lostanlen, Magdalena Fuentes, Graham Dove, Charlie Mydlarz, Justin Salamon, Oded Nov, and Juan Pablo Bello\\u003c/p\\u003e\\n\\n\\u003cp\\u003eThe SONYC-UST dataset is offered free of charge under the terms of the Creative Commons Attribution 4.0 International (CC BY 4.0) license:\\u003cbr\\u003e\\n\\u003ca href=\\\"https://creativecommons.org/licenses/by/4.0/\\\"\\u003ehttps://creativecommons.org/licenses/by/4.0/\\u003c/a\\u003e\\u003c/p\\u003e\\n\\n\\u003cp\\u003eThe dataset and its contents are made available on an \\u0026ldquo;as is\\u0026rdquo; basis and without warranties of any kind, including without limitation satisfactory quality and conformity, merchantability, fitness for a particular purpose, accuracy or completeness, or absence of errors. Subject to any liability that may not be excluded or limited by law, New York University is not liable for, and expressly excludes all liability for, loss or damage however and whenever caused to anyone by any use of the SONYC-UST dataset or any part of it.\\u003c/p\\u003e\\n\\n\\u003cp\\u003e\\u0026nbsp;\\u003c/p\\u003e\\n\\n\\u003cp\\u003e\\u003cstrong\\u003eFeedback\\u003c/strong\\u003e\\u003c/p\\u003e\\n\\n\\u003cp\\u003ePlease help us improve SONYC-UST\\u0026nbsp;by sending your feedback to:\\u003c/p\\u003e\\n\\n\\u003cul\\u003e\\n\\t\\u003cli\\u003eMark Cartwright: \\u003ca href=\\\"mailto:mcartwright@gmail.com\\\"\\u003emcartwright@gmail.com\\u003c/a\\u003e\\u003c/li\\u003e\\n\\u003c/ul\\u003e\\n\\n\\u003cp\\u003eIn case of a problem, please include as many details as possible.\\u003c/p\\u003e\\n\\n\\u003cp\\u003e\\u0026nbsp;\\u003c/p\\u003e\\n\\n\\u003cp\\u003e\\u003cstrong\\u003eAcknowledgments\\u003c/strong\\u003e\\u003c/p\\u003e\\n\\n\\u003cp\\u003eWe would like to thank all the Zooniverse volunteers who continue to contribute to our project. This work is supported by \\u003ca href=\\\"https://www.nsf.gov/awardsearch/showAward?AWD_ID=1544753\\\"\\u003eNational Science Foundation award 1544753\\u003c/a\\u003e.\\u003c/p\\u003e\\n\\n\\u003cp\\u003e\\u0026nbsp;\\u003c/p\\u003e\\n\\n\\u003cp\\u003e\\u003cstrong\\u003eChange log\\u003c/strong\\u003e\\u003c/p\\u003e\\n\\n\\u003cul\\u003e\\n\\t\\u003cli\\u003e2.3 Added the ground truth annotations for the test set, and regrouped the audio files for upload to Zenodo.\\u003c/li\\u003e\\n\\t\\u003cli\\u003e2.2\\u0026nbsp;Added the audio for the test set (audio-eval.tar.gz).\\u003c/li\\u003e\\n\\t\\u003cli\\u003e2.1 The DCASE 2020 development dataset. 14778 new recordings added along with coarse spatiotemporal context information.\\u003c/li\\u003e\\n\\t\\u003cli\\u003e1.0 Data is the same as v0.4. Publication added to README.\\u003c/li\\u003e\\n\\t\\u003cli\\u003e0.4 Fixed error in annotations. Previously, the coarse class \\u0026quot;machinery-impact\\u0026quot; was accidentally indicated as present whenever \\u0026quot;non-machinery-impact\\u0026quot; was present regardless of the presence of \\u0026quot;machinery-impact\\u0026quot;. This error has been fixed.\\u003c/li\\u003e\\n\\t\\u003cli\\u003e0.3 Test set annotations added\\u003c/li\\u003e\\n\\t\\u003cli\\u003e0.2 Test set audio files added\\u003c/li\\u003e\\n\\u003c/ul\\u003e\", \"distribution\": [{\"@type\": \"DataDownload\", \"contentUrl\": \"https://zenodo.org/api/files/31fe1dae-031a-4bcf-8f1b-71b10dee72da/annotations.csv\", \"encodingFormat\": \"csv\"}, {\"@type\": \"DataDownload\", \"contentUrl\": \"https://zenodo.org/api/files/31fe1dae-031a-4bcf-8f1b-71b10dee72da/audio-0.tar.gz\", \"encodingFormat\": \"gz\"}, {\"@type\": \"DataDownload\", \"contentUrl\": \"https://zenodo.org/api/files/31fe1dae-031a-4bcf-8f1b-71b10dee72da/audio-10.tar.gz\", \"encodingFormat\": \"gz\"}, {\"@type\": \"DataDownload\", \"contentUrl\": \"https://zenodo.org/api/files/31fe1dae-031a-4bcf-8f1b-71b10dee72da/audio-11.tar.gz\", \"encodingFormat\": \"gz\"}, {\"@type\": \"DataDownload\", \"contentUrl\": \"https://zenodo.org/api/files/31fe1dae-031a-4bcf-8f1b-71b10dee72da/audio-12.tar.gz\", \"encodingFormat\": \"gz\"}, {\"@type\": \"DataDownload\", \"contentUrl\": \"https://zenodo.org/api/files/31fe1dae-031a-4bcf-8f1b-71b10dee72da/audio-13.tar.gz\", \"encodingFormat\": \"gz\"}, {\"@type\": \"DataDownload\", \"contentUrl\": \"https://zenodo.org/api/files/31fe1dae-031a-4bcf-8f1b-71b10dee72da/audio-14.tar.gz\", \"encodingFormat\": \"gz\"}, {\"@type\": \"DataDownload\", \"contentUrl\": \"https://zenodo.org/api/files/31fe1dae-031a-4bcf-8f1b-71b10dee72da/audio-15.tar.gz\", \"encodingFormat\": \"gz\"}, {\"@type\": \"DataDownload\", \"contentUrl\": \"https://zenodo.org/api/files/31fe1dae-031a-4bcf-8f1b-71b10dee72da/audio-16.tar.gz\", \"encodingFormat\": \"gz\"}, {\"@type\": \"DataDownload\", \"contentUrl\": \"https://zenodo.org/api/files/31fe1dae-031a-4bcf-8f1b-71b10dee72da/audio-17.tar.gz\", \"encodingFormat\": \"gz\"}, {\"@type\": \"DataDownload\", \"contentUrl\": \"https://zenodo.org/api/files/31fe1dae-031a-4bcf-8f1b-71b10dee72da/audio-18.tar.gz\", \"encodingFormat\": \"gz\"}, {\"@type\": \"DataDownload\", \"contentUrl\": \"https://zenodo.org/api/files/31fe1dae-031a-4bcf-8f1b-71b10dee72da/audio-1.tar.gz\", \"encodingFormat\": \"gz\"}, {\"@type\": \"DataDownload\", \"contentUrl\": \"https://zenodo.org/api/files/31fe1dae-031a-4bcf-8f1b-71b10dee72da/audio-2.tar.gz\", \"encodingFormat\": \"gz\"}, {\"@type\": \"DataDownload\", \"contentUrl\": \"https://zenodo.org/api/files/31fe1dae-031a-4bcf-8f1b-71b10dee72da/audio-3.tar.gz\", \"encodingFormat\": \"gz\"}, {\"@type\": \"DataDownload\", \"contentUrl\": \"https://zenodo.org/api/files/31fe1dae-031a-4bcf-8f1b-71b10dee72da/audio-4.tar.gz\", \"encodingFormat\": \"gz\"}, {\"@type\": \"DataDownload\", \"contentUrl\": \"https://zenodo.org/api/files/31fe1dae-031a-4bcf-8f1b-71b10dee72da/audio-5.tar.gz\", \"encodingFormat\": \"gz\"}, {\"@type\": \"DataDownload\", \"contentUrl\": \"https://zenodo.org/api/files/31fe1dae-031a-4bcf-8f1b-71b10dee72da/audio-6.tar.gz\", \"encodingFormat\": \"gz\"}, {\"@type\": \"DataDownload\", \"contentUrl\": \"https://zenodo.org/api/files/31fe1dae-031a-4bcf-8f1b-71b10dee72da/audio-7.tar.gz\", \"encodingFormat\": \"gz\"}, {\"@type\": \"DataDownload\", \"contentUrl\": \"https://zenodo.org/api/files/31fe1dae-031a-4bcf-8f1b-71b10dee72da/audio-8.tar.gz\", \"encodingFormat\": \"gz\"}, {\"@type\": \"DataDownload\", \"contentUrl\": \"https://zenodo.org/api/files/31fe1dae-031a-4bcf-8f1b-71b10dee72da/audio-9.tar.gz\", \"encodingFormat\": \"gz\"}, {\"@type\": \"DataDownload\", \"contentUrl\": \"https://zenodo.org/api/files/31fe1dae-031a-4bcf-8f1b-71b10dee72da/dcase-ust-taxonomy.yaml\", \"encodingFormat\": \"yaml\"}, {\"@type\": \"DataDownload\", \"contentUrl\": \"https://zenodo.org/api/files/31fe1dae-031a-4bcf-8f1b-71b10dee72da/README.md\", \"encodingFormat\": \"md\"}, {\"@type\": \"DataDownload\", \"contentUrl\": \"https://zenodo.org/api/files/31fe1dae-031a-4bcf-8f1b-71b10dee72da/unpack_audio.sh\", \"encodingFormat\": \"sh\"}], \"identifier\": \"https://doi.org/10.5281/zenodo.3966543\", \"keywords\": [\"urban sound\", \"noise pollution\", \"machine listening\", \"computer audition\", \"longterm spatiotemporal context\", \"sound tagging\"], \"license\": \"https://creativecommons.org/licenses/by/4.0/legalcode\", \"name\": \"SONYC Urban Sound Tagging (SONYC-UST): a multilabel dataset from an urban acoustic sensor network\", \"url\": \"https://zenodo.org/record/3966543\", \"version\": \"2.3.0\"}</script>\n",
       "<script src=\"/static/gen/zenodo.f843b7ec.js\"></script>\n",
       "<script src=\"//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML\" type=\"text/javascript\"></script>\n",
       "<script src=\"/static/gen/zenodo.search.ff7bb1ad.js\"></script>\n",
       "<script type=\"text/javascript\">var addthis_config = {\"data_track_addressbar\": true};</script>\n",
       "<script type=\"text/javascript\">\n",
       "  // Bootstrap the Invenio CSL Formatter and invenio-search-js\n",
       "  require([\n",
       "    \"jquery\",\n",
       "    'typeahead.js',\n",
       "    'bloodhound',\n",
       "    \"node_modules/angular/angular\",\n",
       "    \"node_modules/invenio-csl-js/dist/invenio-csl-js\",\n",
       "    \"node_modules/invenio-search-js/dist/invenio-search-js\",\n",
       "    \"js/zenodo/module\"\n",
       "    ], function(typeahead, Bloodhound) {\n",
       "      angular.element(document).ready(function() {\n",
       "\n",
       "        // FIXME: This is already defined in js/zenodo_deposit/filters.js.\n",
       "        // It should be moved to a common place...\n",
       "        angular.module('zenodo.filters').filter('limitToEllipsis', function () {\n",
       "          return function(text, n) {\n",
       "            return (text.length > n) ? text.substr(0, n-1) + '&hellip;' : text;\n",
       "          };\n",
       "        });\n",
       "\n",
       "        angular.bootstrap(document.getElementById(\"citations\"), [\n",
       "            'invenioSearch',\n",
       "            'zenodo.filters',\n",
       "            'mgcrea.ngStrap.tooltip',\n",
       "          ]\n",
       "        );\n",
       "\n",
       "        angular.bootstrap(document.getElementById(\"invenio-csl\"), [\n",
       "            'invenioCsl',\n",
       "          ]\n",
       "        );\n",
       "      });\n",
       "    }\n",
       "  );\n",
       "  require([\n",
       "    \"jquery\",\n",
       "    \"js/zenodo/functions\",\n",
       "    ], function($, recordCommunityCurate) {\n",
       "      $(function () {\n",
       "        $(\"#recordCommunityCuration .btn\").click(recordCommunityCurate);\n",
       "        $('.preview-link').on('click', function(event) {\n",
       "          $('#preview').show();\n",
       "          var filename = encodeURIComponent($(event.target).data('filename'));\n",
       "          $('#preview-iframe').attr(\"src\",\"/record/3966543/preview/\" + filename);\n",
       "        });\n",
       "      });\n",
       "    }\n",
       "  );\n",
       "  $(function () {\n",
       "    $('[data-toggle=\"tooltip\"]').tooltip();\n",
       "  });\n",
       "</script>\n",
       "<script src=\"//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5137aff66ad9c2a1\" type=\"text/javascript\"></script>\n",
       "<script type=\"text/javascript\">\n",
       "var _paq = _paq || [];\n",
       "_paq.push([\"setDomains\", [\"*.zenodo.org\",\"*.zenodo.cern.ch\",\"*.zenodo.eu\",\"*.zenodo.net\"]]);\n",
       "_paq.push([\"trackPageView\"]);\n",
       "_paq.push([\"enableLinkTracking\"]);\n",
       "\n",
       "(function() {\n",
       "  var u=((\"https:\" == document.location.protocol) ? \"https\" : \"http\") + \"://piwik.web.cern.ch/\";\n",
       "  _paq.push([\"setTrackerUrl\", u+\"piwik.php\"]);\n",
       "  _paq.push([\"setSiteId\", \"57\"]);\n",
       "  var d=document, g=d.createElement(\"script\"), s=d.getElementsByTagName(\"script\")[0]; g.type=\"text/javascript\";\n",
       "  g.defer=true; g.async=true; g.src=u+\"piwik.js\"; s.parentNode.insertBefore(g,s);\n",
       "})();\n",
       "</script>\n",
       "</body>\n",
       "</html>>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "\n",
    "page = requests.get('https://zenodo.org/record/3966543/').text\n",
    "soup = BeautifulSoup(page, 'html.parser')\n",
    "print(soup)\n",
    "soup.find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d506a133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://zenodo.org/record/3966543/files/README.md?download=1',\n",
       " 'https://zenodo.org/record/3966543/files/annotations.csv?download=1',\n",
       " 'https://zenodo.org/record/3966543/files/audio-0.tar.gz?download=1',\n",
       " 'https://zenodo.org/record/3966543/files/audio-1.tar.gz?download=1',\n",
       " 'https://zenodo.org/record/3966543/files/audio-10.tar.gz?download=1',\n",
       " 'https://zenodo.org/record/3966543/files/audio-11.tar.gz?download=1',\n",
       " 'https://zenodo.org/record/3966543/files/audio-12.tar.gz?download=1',\n",
       " 'https://zenodo.org/record/3966543/files/audio-13.tar.gz?download=1',\n",
       " 'https://zenodo.org/record/3966543/files/audio-14.tar.gz?download=1',\n",
       " 'https://zenodo.org/record/3966543/files/audio-15.tar.gz?download=1',\n",
       " 'https://zenodo.org/record/3966543/files/audio-16.tar.gz?download=1',\n",
       " 'https://zenodo.org/record/3966543/files/audio-17.tar.gz?download=1',\n",
       " 'https://zenodo.org/record/3966543/files/audio-18.tar.gz?download=1',\n",
       " 'https://zenodo.org/record/3966543/files/audio-2.tar.gz?download=1',\n",
       " 'https://zenodo.org/record/3966543/files/audio-3.tar.gz?download=1',\n",
       " 'https://zenodo.org/record/3966543/files/audio-4.tar.gz?download=1',\n",
       " 'https://zenodo.org/record/3966543/files/audio-5.tar.gz?download=1',\n",
       " 'https://zenodo.org/record/3966543/files/audio-6.tar.gz?download=1',\n",
       " 'https://zenodo.org/record/3966543/files/audio-7.tar.gz?download=1',\n",
       " 'https://zenodo.org/record/3966543/files/audio-8.tar.gz?download=1',\n",
       " 'https://zenodo.org/record/3966543/files/audio-9.tar.gz?download=1',\n",
       " 'https://zenodo.org/record/3966543/files/dcase-ust-taxonomy.yaml?download=1',\n",
       " 'https://zenodo.org/record/3966543/files/unpack_audio.sh?download=1']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls = []\n",
    "for link in soup.find_all('a'):\n",
    "    b = link.get('href')\n",
    "    if b is not None:\n",
    "        if '/record/3966543/files/' in b:\n",
    "            urls.append('https://zenodo.org'+b)\n",
    "\n",
    "urls = list(set(urls))\n",
    "urls.sort()\n",
    "urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7600c0fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading: downloading: downloading:  https://zenodo.org/record/3966543/files/annotations.csv?download=1\n",
      "downloading:  https://zenodo.org/record/3966543/files/audio-0.tar.gz?download=1 https://zenodo.org/record/3966543/files/audio-1.tar.gz?download=1\n",
      "downloading: \n",
      " https://zenodo.org/record/3966543/files/audio-10.tar.gz?download=1\n",
      " https://zenodo.org/record/3966543/files/README.md?download=1\n",
      "downloading:  https://zenodo.org/record/3966543/files/audio-11.tar.gz?download=1\n",
      "https://zenodo.org/record/3966543/files/README.md?download=1\n",
      "downloading: https://zenodo.org/record/3966543/files/annotations.csv?download=1 https://zenodo.org/record/3966543/files/audio-12.tar.gz?download=1\n",
      "\n",
      "downloading:  https://zenodo.org/record/3966543/files/audio-13.tar.gz?download=1\n",
      "https://zenodo.org/record/3966543/files/audio-11.tar.gz?download=1\n",
      "downloading:  https://zenodo.org/record/3966543/files/audio-14.tar.gz?download=1\n",
      "https://zenodo.org/record/3966543/files/audio-12.tar.gz?download=1\n",
      "downloading:  https://zenodo.org/record/3966543/files/audio-15.tar.gz?download=1\n",
      "https://zenodo.org/record/3966543/files/audio-10.tar.gz?download=1\n",
      "downloading: https://zenodo.org/record/3966543/files/audio-0.tar.gz?download=1\n",
      " https://zenodo.org/record/3966543/files/audio-16.tar.gz?download=1\n",
      "downloading:  https://zenodo.org/record/3966543/files/audio-17.tar.gz?download=1\n",
      "https://zenodo.org/record/3966543/files/audio-1.tar.gz?download=1\n",
      "downloading: https://zenodo.org/record/3966543/files/audio-15.tar.gz?download=1 https://zenodo.org/record/3966543/files/audio-18.tar.gz?download=1\n",
      "\n",
      "downloading:  https://zenodo.org/record/3966543/files/audio-2.tar.gz?download=1\n",
      "https://zenodo.org/record/3966543/files/audio-16.tar.gz?download=1\n",
      "downloading: https://zenodo.org/record/3966543/files/audio-17.tar.gz?download=1 https://zenodo.org/record/3966543/files/audio-3.tar.gz?download=1\n",
      "\n",
      "downloading: https://zenodo.org/record/3966543/files/audio-14.tar.gz?download=1\n",
      " https://zenodo.org/record/3966543/files/audio-4.tar.gz?download=1\n",
      "downloading: https://zenodo.org/record/3966543/files/audio-13.tar.gz?download=1 https://zenodo.org/record/3966543/files/audio-5.tar.gz?download=1\n",
      "\n",
      "downloading:  https://zenodo.org/record/3966543/files/audio-6.tar.gz?download=1\n",
      "https://zenodo.org/record/3966543/files/audio-18.tar.gz?download=1\n",
      "downloading:  https://zenodo.org/record/3966543/files/audio-7.tar.gz?download=1\n",
      "https://zenodo.org/record/3966543/files/audio-3.tar.gz?download=1\n",
      "downloading:  https://zenodo.org/record/3966543/files/audio-8.tar.gz?download=1\n",
      "https://zenodo.org/record/3966543/files/audio-5.tar.gz?download=1\n",
      "downloading: https://zenodo.org/record/3966543/files/audio-2.tar.gz?download=1 https://zenodo.org/record/3966543/files/audio-9.tar.gz?download=1\n",
      "\n",
      "downloading: https://zenodo.org/record/3966543/files/audio-4.tar.gz?download=1 https://zenodo.org/record/3966543/files/dcase-ust-taxonomy.yaml?download=1\n",
      "\n",
      "downloading: https://zenodo.org/record/3966543/files/dcase-ust-taxonomy.yaml?download=1 https://zenodo.org/record/3966543/files/unpack_audio.sh?download=1\n",
      "\n",
      "https://zenodo.org/record/3966543/files/unpack_audio.sh?download=1\n",
      "https://zenodo.org/record/3966543/files/audio-6.tar.gz?download=1\n",
      "https://zenodo.org/record/3966543/files/audio-7.tar.gz?download=1\n",
      "https://zenodo.org/record/3966543/files/audio-8.tar.gz?download=1\n",
      "https://zenodo.org/record/3966543/files/audio-9.tar.gz?download=1\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing.pool import ThreadPool\n",
    "import os\n",
    "\n",
    "# function revised from https://www.quickprogrammingtips.com/python/how-to-download-multiple-files-concurrently-in-python.html\n",
    "def download_url(url):\n",
    "    print(\"downloading: \",url)\n",
    "    # assumes that the last segment after the / represents the file name\n",
    "    # if url is abc/xyz/file.txt, the file name will be file.txt\n",
    "    file_name_start_pos = url.rfind(\"/\") + 1\n",
    "    file_name_end_pos = url.rfind(\"?\")\n",
    "    file_name = url[file_name_start_pos:file_name_end_pos]\n",
    "\n",
    "    r = requests.get(url, stream=True)\n",
    "    if r.status_code == requests.codes.ok:\n",
    "    with open('../data/raw/SONYC/'+file_name, 'wb') as f:\n",
    "        for data in r:\n",
    "            f.write(data)\n",
    "    return url\n",
    " \n",
    "# Run 5 multiple threads. Each call will take the next element in urls list\n",
    "os.mkdir('../data/raw/SONYC')\n",
    "results = ThreadPool(5).imap_unordered(download_url, urls)\n",
    "for r in results:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "948ebfc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio-0.tar.gz\n",
      "audio-1.tar.gz\n",
      "audio-10.tar.gz\n",
      "audio-11.tar.gz\n",
      "audio-12.tar.gz\n",
      "audio-13.tar.gz\n",
      "audio-14.tar.gz\n",
      "audio-15.tar.gz\n",
      "audio-16.tar.gz\n",
      "audio-17.tar.gz\n",
      "audio-18.tar.gz\n",
      "audio-2.tar.gz\n",
      "audio-3.tar.gz\n",
      "audio-4.tar.gz\n",
      "audio-5.tar.gz\n",
      "audio-6.tar.gz\n",
      "audio-7.tar.gz\n",
      "audio-8.tar.gz\n",
      "audio-9.tar.gz\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "import os\n",
    "\n",
    "for n_url in urls:\n",
    "    if 'tar.gz' in n_url:\n",
    "        file_name_start_pos = n_url.rfind(\"/\") + 1\n",
    "        file_name_end_pos = n_url.rfind(\"?\")\n",
    "        file_name = n_url[file_name_start_pos:file_name_end_pos]\n",
    "        print(file_name)\n",
    "        file = tarfile.open('../data/raw/SONYC/'+file_name)\n",
    "        file.extractall('../data/raw/SONYC/')\n",
    "        file.close()\n",
    "        os.remove('../data/raw/SONYC/'+file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43bdc50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
