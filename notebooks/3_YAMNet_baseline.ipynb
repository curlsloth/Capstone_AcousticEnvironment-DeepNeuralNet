{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e748249",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-21 18:20:11.537621: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-21 18:20:15.391144: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "yamnet_model_handle = 'https://tfhub.dev/google/yamnet/1'\n",
    "yamnet_model = hub.load(yamnet_model_handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1f92587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scan data directories\n",
    "import glob\n",
    "\n",
    "nature_file_list = []\n",
    "nature_file_list += glob.glob('../data/interim/AmbisonicSoundLibrary/nature/*')\n",
    "nature_file_list += glob.glob('../data/interim/GoogleAudioSet/Outside, rural or natural/*')\n",
    "nature_file_list += glob.glob('../data/interim/youtube/NatureSoundscapes/*')\n",
    "nature_file_list += glob.glob('../data/interim/youtube/NomadicAmbience_nature/*')\n",
    "nature_file_list += glob.glob('../data/interim/S2L_LULC/non_urban/*')\n",
    "nature_file_list += glob.glob('../data/interim/S2L_LULC/urban_0_25/*')\n",
    "\n",
    "city_file_list = []\n",
    "city_file_list += glob.glob('../data/interim/GoogleAudioSet/Outside, urban or manmade/*')\n",
    "city_file_list += glob.glob('../data/interim/youtube/NomadicAmbience_city/*')\n",
    "city_file_list += glob.glob('../data/interim/SONYC/**/*.pkl')\n",
    "city_file_list += glob.glob('../data/interim/S2L_LULC/urban_26_100/*')\n",
    "\n",
    "nature_source_list = ['nature_'+i.rsplit('/', 3)[1]+'/'+i.rsplit('/', 3)[2] for i in nature_file_list]\n",
    "city_source_list = ['city_'+i.rsplit('/', -1)[3] for i in city_file_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51ec37f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>source</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../data/interim/AmbisonicSoundLibrary/nature/W...</td>\n",
       "      <td>nature_AmbisonicSoundLibrary/nature</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../data/interim/AmbisonicSoundLibrary/nature/R...</td>\n",
       "      <td>nature_AmbisonicSoundLibrary/nature</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../data/interim/AmbisonicSoundLibrary/nature/A...</td>\n",
       "      <td>nature_AmbisonicSoundLibrary/nature</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../data/interim/AmbisonicSoundLibrary/nature/W...</td>\n",
       "      <td>nature_AmbisonicSoundLibrary/nature</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../data/interim/AmbisonicSoundLibrary/nature/L...</td>\n",
       "      <td>nature_AmbisonicSoundLibrary/nature</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1919</th>\n",
       "      <td>../data/interim/S2L_LULC/urban_26_100/s2lam111...</td>\n",
       "      <td>city_S2L_LULC</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1920</th>\n",
       "      <td>../data/interim/S2L_LULC/urban_26_100/s2lam006...</td>\n",
       "      <td>city_S2L_LULC</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1921</th>\n",
       "      <td>../data/interim/S2L_LULC/urban_26_100/s2lam083...</td>\n",
       "      <td>city_S2L_LULC</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1922</th>\n",
       "      <td>../data/interim/S2L_LULC/urban_26_100/s2lam052...</td>\n",
       "      <td>city_S2L_LULC</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1923</th>\n",
       "      <td>../data/interim/S2L_LULC/urban_26_100/s2lam098...</td>\n",
       "      <td>city_S2L_LULC</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1924 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   file  \\\n",
       "0     ../data/interim/AmbisonicSoundLibrary/nature/W...   \n",
       "1     ../data/interim/AmbisonicSoundLibrary/nature/R...   \n",
       "2     ../data/interim/AmbisonicSoundLibrary/nature/A...   \n",
       "3     ../data/interim/AmbisonicSoundLibrary/nature/W...   \n",
       "4     ../data/interim/AmbisonicSoundLibrary/nature/L...   \n",
       "...                                                 ...   \n",
       "1919  ../data/interim/S2L_LULC/urban_26_100/s2lam111...   \n",
       "1920  ../data/interim/S2L_LULC/urban_26_100/s2lam006...   \n",
       "1921  ../data/interim/S2L_LULC/urban_26_100/s2lam083...   \n",
       "1922  ../data/interim/S2L_LULC/urban_26_100/s2lam052...   \n",
       "1923  ../data/interim/S2L_LULC/urban_26_100/s2lam098...   \n",
       "\n",
       "                                   source  category  \n",
       "0     nature_AmbisonicSoundLibrary/nature         0  \n",
       "1     nature_AmbisonicSoundLibrary/nature         0  \n",
       "2     nature_AmbisonicSoundLibrary/nature         0  \n",
       "3     nature_AmbisonicSoundLibrary/nature         0  \n",
       "4     nature_AmbisonicSoundLibrary/nature         0  \n",
       "...                                   ...       ...  \n",
       "1919                        city_S2L_LULC         1  \n",
       "1920                        city_S2L_LULC         1  \n",
       "1921                        city_S2L_LULC         1  \n",
       "1922                        city_S2L_LULC         1  \n",
       "1923                        city_S2L_LULC         1  \n",
       "\n",
       "[1924 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nature_df = pd.DataFrame({'file': nature_file_list, 'source': nature_source_list, 'category': 0})\n",
    "city_df = pd.DataFrame({'file': city_file_list, 'source': city_source_list, 'category': 1})\n",
    "df_all = pd.concat([nature_df, city_df], ignore_index=True)\n",
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b26f5115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>source</th>\n",
       "      <th>category</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../data/interim/S2L_LULC/urban_0_25/s2llg003_1...</td>\n",
       "      <td>nature_S2L_LULC/urban_0_25</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../data/interim/GoogleAudioSet/Outside, urban ...</td>\n",
       "      <td>city_GoogleAudioSet</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../data/interim/SONYC/audio-12/34_018803.pkl</td>\n",
       "      <td>city_SONYC</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../data/interim/GoogleAudioSet/Outside, urban ...</td>\n",
       "      <td>city_GoogleAudioSet</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../data/interim/GoogleAudioSet/Outside, urban ...</td>\n",
       "      <td>city_GoogleAudioSet</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1919</th>\n",
       "      <td>../data/interim/SONYC/audio-4/22_004702.pkl</td>\n",
       "      <td>city_SONYC</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1920</th>\n",
       "      <td>../data/interim/S2L_LULC/urban_0_25/s2lam042_1...</td>\n",
       "      <td>nature_S2L_LULC/urban_0_25</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1921</th>\n",
       "      <td>../data/interim/GoogleAudioSet/Outside, urban ...</td>\n",
       "      <td>city_GoogleAudioSet</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1922</th>\n",
       "      <td>../data/interim/S2L_LULC/urban_0_25/s2llg001_1...</td>\n",
       "      <td>nature_S2L_LULC/urban_0_25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1923</th>\n",
       "      <td>../data/interim/S2L_LULC/non_urban/s2llg004_17...</td>\n",
       "      <td>nature_S2L_LULC/non_urban</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1924 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   file  \\\n",
       "0     ../data/interim/S2L_LULC/urban_0_25/s2llg003_1...   \n",
       "1     ../data/interim/GoogleAudioSet/Outside, urban ...   \n",
       "2          ../data/interim/SONYC/audio-12/34_018803.pkl   \n",
       "3     ../data/interim/GoogleAudioSet/Outside, urban ...   \n",
       "4     ../data/interim/GoogleAudioSet/Outside, urban ...   \n",
       "...                                                 ...   \n",
       "1919        ../data/interim/SONYC/audio-4/22_004702.pkl   \n",
       "1920  ../data/interim/S2L_LULC/urban_0_25/s2lam042_1...   \n",
       "1921  ../data/interim/GoogleAudioSet/Outside, urban ...   \n",
       "1922  ../data/interim/S2L_LULC/urban_0_25/s2llg001_1...   \n",
       "1923  ../data/interim/S2L_LULC/non_urban/s2llg004_17...   \n",
       "\n",
       "                          source  category  fold  \n",
       "0     nature_S2L_LULC/urban_0_25         0     9  \n",
       "1            city_GoogleAudioSet         1     8  \n",
       "2                     city_SONYC         1     7  \n",
       "3            city_GoogleAudioSet         1     4  \n",
       "4            city_GoogleAudioSet         1     0  \n",
       "...                          ...       ...   ...  \n",
       "1919                  city_SONYC         1     1  \n",
       "1920  nature_S2L_LULC/urban_0_25         0     2  \n",
       "1921         city_GoogleAudioSet         1     1  \n",
       "1922  nature_S2L_LULC/urban_0_25         0     0  \n",
       "1923   nature_S2L_LULC/non_urban         0     0  \n",
       "\n",
       "[1924 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Split the data into folds using StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=23)\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(df_all, df_all['source'])):\n",
    "    # Assign the fold number to each row in the DataFrame\n",
    "    df_all.loc[val_idx, 'fold'] = fold\n",
    "    \n",
    "df_all['fold'] = df_all['fold'].astype('int')\n",
    "df_all = df_all.sample(frac=1, random_state=23).reset_index(drop=True) # need to shuffle the rows before deep learning\n",
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71d579e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.to_csv('../data/interim/train_val_test_split_Feb21.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfc4905",
   "metadata": {},
   "source": [
    "# Convert data into TF format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "341a8a27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(), dtype=tf.string, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int64, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int64, name=None))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames = df_all['file']\n",
    "targets = df_all['category']\n",
    "folds = df_all['fold']\n",
    "\n",
    "main_ds = tf.data.Dataset.from_tensor_slices((filenames, targets, folds))\n",
    "main_ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a936059a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "short length: 159999\n",
      "short length: 159880\n",
      "short length: 146099\n",
      "short length: 159880\n",
      "short length: 159880\n",
      "short length: 151683\n",
      "short length: 159880\n",
      "short length: 153357\n",
      "short length: 153242\n",
      "short length: 157848\n",
      "short length: 159992\n",
      "short length: 151461\n",
      "short length: 148006\n",
      "short length: 159997\n",
      "short length: 156480\n",
      "short length: 159993\n"
     ]
    }
   ],
   "source": [
    "def load_wav_pkl(filename, wav_label='y'):\n",
    "    import pickle\n",
    "    # open a file, where you stored the pickled data\n",
    "    file = open(filename, 'rb')\n",
    "\n",
    "    # dump information to that file\n",
    "    output = pickle.load(file)\n",
    "    wav = output[wav_label]\n",
    "\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return wav\n",
    "\n",
    "wav_list = []\n",
    "for index, row in df_all.iterrows():\n",
    "    if len(load_wav_pkl(row['file'])) == 160000:\n",
    "        wav_list.append(load_wav_pkl(row['file']))\n",
    "    else: # if the waveform is shorter (for unknown reason)\n",
    "        temp_wav = load_wav_pkl(row['file'])\n",
    "        print('short length: '+str(len(temp_wav)))\n",
    "        wav_list.append(np.pad(temp_wav, (0,160000-len(temp_wav)),'mean')) # zero-padding at the end to 160000\n",
    "\n",
    "wav_bg_list = []\n",
    "for index, row in df_all.iterrows():\n",
    "    if len(load_wav_pkl(row['file'], 'bg_y')) == 160000:\n",
    "        wav_bg_list.append(load_wav_pkl(row['file'], 'bg_y'))\n",
    "    else: # if the waveform is shorter (for unknown reason)\n",
    "        temp_wav = load_wav_pkl(row['file'], 'bg_y')\n",
    "#         print('short length: '+str(len(temp_wav)))\n",
    "        wav_bg_list.append(np.pad(temp_wav, (0,160000-len(temp_wav)),'mean')) # zero-padding at the end to 160000\n",
    "\n",
    "wav_fg_list = []\n",
    "for index, row in df_all.iterrows():\n",
    "    if len(load_wav_pkl(row['file'], 'fg_y')) == 160000:\n",
    "        wav_fg_list.append(load_wav_pkl(row['file'], 'fg_y'))\n",
    "    else: # if the waveform is shorter (for unknown reason)\n",
    "        temp_wav = load_wav_pkl(row['file'], 'fg_y')\n",
    "#         print('short length: '+str(len(temp_wav)))\n",
    "        wav_fg_list.append(np.pad(temp_wav, (0,160000-len(temp_wav)),'mean')) # zero-padding at the end to 160000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53798c90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(160000,), dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int64, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int64, name=None))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_ds = tf.data.Dataset.from_tensor_slices((np.stack(wav_list, axis = 0), df_all['category'], df_all['fold']))\n",
    "main_ds_fg = tf.data.Dataset.from_tensor_slices((np.stack(wav_fg_list, axis = 0), df_all['category'], df_all['fold']))\n",
    "main_ds_bg = tf.data.Dataset.from_tensor_slices((np.stack(wav_bg_list, axis = 0), df_all['category'], df_all['fold']))\n",
    "\n",
    "main_ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3a63c0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(1024,), dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int64, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int64, name=None))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# applies the embedding extraction model to a wav data\n",
    "def extract_embedding(wav_data, label, fold):\n",
    "    # run YAMNet to extract embedding from the wav data\n",
    "    scores, embeddings, spectrogram = yamnet_model(wav_data)\n",
    "    num_embeddings = tf.shape(embeddings)[0]\n",
    "    return (embeddings,\n",
    "            tf.repeat(label, num_embeddings),\n",
    "            tf.repeat(fold, num_embeddings))\n",
    "\n",
    "# extract embedding\n",
    "main_ds = main_ds.map(extract_embedding).unbatch()\n",
    "main_ds_fg = main_ds_fg.map(extract_embedding).unbatch()\n",
    "main_ds_bg = main_ds_bg.map(extract_embedding).unbatch()\n",
    "\n",
    "main_ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb24df14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(160000,), dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(160000,), dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(160000,), dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int64, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int64, name=None))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_ds_all3 = tf.data.Dataset.from_tensor_slices((np.stack(wav_list, axis = 0), np.stack(wav_bg_list, axis = 0), np.stack(wav_fg_list, axis = 0), df_all['category'], df_all['fold']))\n",
    "main_ds_all3.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1425beee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(3072,), dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int64, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int64, name=None))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# applies the embedding extraction model to a wav data\n",
    "def extract_embedding_3(wav_data_raw, wav_data_bg, wav_data_fg, label, fold):\n",
    "    # run YAMNet to extract embedding from the wav data\n",
    "    scores, embeddings_raw, spectrogram = yamnet_model(wav_data_raw)\n",
    "    scores, embeddings_bg, spectrogram = yamnet_model(wav_data_bg)\n",
    "    scores, embeddings_fg, spectrogram = yamnet_model(wav_data_fg)\n",
    "    num_embeddings_raw = tf.shape(embeddings_raw)[0]\n",
    "    return (tf.concat([embeddings_raw, embeddings_bg, embeddings_fg],1),\n",
    "            tf.repeat(label, num_embeddings_raw),\n",
    "            tf.repeat(fold, num_embeddings_raw))\n",
    "\n",
    "# extract embedding\n",
    "main_ds_3 = main_ds_all3.map(extract_embedding_3).unbatch()\n",
    "\n",
    "main_ds_3.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "439303be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/andrewchang/opt/anaconda3/envs/AcousticEnv/lib/python3.8/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/andrewchang/opt/anaconda3/envs/AcousticEnv/lib/python3.8/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    }
   ],
   "source": [
    "## raw signal\n",
    "cached_ds = main_ds.cache()\n",
    "train_ds = cached_ds.filter(lambda embedding, label, fold: fold < 8)\n",
    "val_ds = cached_ds.filter(lambda embedding, label, fold: fold == 8)\n",
    "test_ds = cached_ds.filter(lambda embedding, label, fold: fold == 9)\n",
    "\n",
    "# remove the folds column now that it's not needed anymore\n",
    "remove_fold_column = lambda embedding, label, fold: (embedding, label)\n",
    "\n",
    "train_ds = train_ds.map(remove_fold_column)\n",
    "val_ds = val_ds.map(remove_fold_column)\n",
    "test_ds = test_ds.map(remove_fold_column)\n",
    "\n",
    "train_ds = train_ds.cache().shuffle(1000).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.cache().batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "test_ds = test_ds.cache().batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55f2c04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## background sound\n",
    "cached_ds_bg = main_ds_bg.cache()\n",
    "train_ds_bg = cached_ds_bg.filter(lambda embedding, label, fold: fold < 8)\n",
    "val_ds_bg = cached_ds_bg.filter(lambda embedding, label, fold: fold == 8)\n",
    "test_ds_bg = cached_ds_bg.filter(lambda embedding, label, fold: fold == 9)\n",
    "\n",
    "# remove the folds column now that it's not needed anymore\n",
    "remove_fold_column = lambda embedding, label, fold: (embedding, label)\n",
    "\n",
    "train_ds_bg = train_ds_bg.map(remove_fold_column)\n",
    "val_ds_bg = val_ds_bg.map(remove_fold_column)\n",
    "test_ds_bg = test_ds_bg.map(remove_fold_column)\n",
    "\n",
    "train_ds_bg = train_ds_bg.cache().shuffle(1000).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "val_ds_bg = val_ds_bg.cache().batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "test_ds_bg = test_ds_bg.cache().batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "662d18f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## foreground sound\n",
    "cached_ds_fg = main_ds_fg.cache()\n",
    "train_ds_fg = cached_ds_fg.filter(lambda embedding, label, fold: fold < 8)\n",
    "val_ds_fg = cached_ds_fg.filter(lambda embedding, label, fold: fold == 8)\n",
    "test_ds_fg = cached_ds_fg.filter(lambda embedding, label, fold: fold == 9)\n",
    "\n",
    "# remove the folds column now that it's not needed anymore\n",
    "remove_fold_column = lambda embedding, label, fold: (embedding, label)\n",
    "\n",
    "train_ds_fg = train_ds_fg.map(remove_fold_column)\n",
    "val_ds_fg = val_ds_fg.map(remove_fold_column)\n",
    "test_ds_fg = test_ds_fg.map(remove_fold_column)\n",
    "\n",
    "train_ds_fg = train_ds_fg.cache().shuffle(1000).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "val_ds_fg = val_ds_fg.cache().batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "test_ds_fg = test_ds_fg.cache().batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aeaf0853",
   "metadata": {},
   "outputs": [],
   "source": [
    "## all 3 signals\n",
    "cached_ds_3 = main_ds_3.cache()\n",
    "train_ds_3 = cached_ds_3.filter(lambda embedding, label, fold: fold < 8)\n",
    "val_ds_3 = cached_ds_3.filter(lambda embedding, label, fold: fold == 8)\n",
    "test_ds_3 = cached_ds_3.filter(lambda embedding, label, fold: fold == 9)\n",
    "\n",
    "# remove the folds column now that it's not needed anymore\n",
    "remove_fold_column = lambda embedding, label, fold: (embedding, label)\n",
    "\n",
    "train_ds_3 = train_ds_3.map(remove_fold_column)\n",
    "val_ds_3 = val_ds_3.map(remove_fold_column)\n",
    "test_ds_3 = test_ds_3.map(remove_fold_column)\n",
    "\n",
    "train_ds_3 = train_ds_3.cache().shuffle(1000).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "val_ds_3 = val_ds_3.cache().batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "test_ds_3 = test_ds_3.cache().batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe243be",
   "metadata": {},
   "source": [
    "# Model of raw signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21e40085",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "efd43949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"raw_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 512)               524800    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 1026      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 525,826\n",
      "Trainable params: 525,826\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "my_classes = ['city', 'nature']\n",
    "raw_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(1024), dtype=tf.float32,\n",
    "                          name='input_embedding'),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(len(my_classes))\n",
    "], name='raw_model')\n",
    "\n",
    "raw_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f392d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                    optimizer=\"adam\",\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "# callback will be used in the other models below too\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss',\n",
    "                                            patience=3,\n",
    "                                            restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0259ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "963/963 [==============================] - 153s 152ms/step - loss: 0.4380 - accuracy: 0.8110 - val_loss: 0.4690 - val_accuracy: 0.7935\n",
      "Epoch 2/20\n",
      "963/963 [==============================] - 4s 4ms/step - loss: 0.3775 - accuracy: 0.8353 - val_loss: 0.4902 - val_accuracy: 0.7927\n",
      "Epoch 3/20\n",
      "963/963 [==============================] - 3s 4ms/step - loss: 0.3640 - accuracy: 0.8533 - val_loss: 0.5136 - val_accuracy: 0.8023\n",
      "Epoch 4/20\n",
      "963/963 [==============================] - 4s 4ms/step - loss: 0.3274 - accuracy: 0.8700 - val_loss: 0.5310 - val_accuracy: 0.7940\n",
      "Epoch 5/20\n",
      "963/963 [==============================] - 4s 4ms/step - loss: 0.2867 - accuracy: 0.8829 - val_loss: 0.5279 - val_accuracy: 0.7984\n",
      "Epoch 6/20\n",
      "963/963 [==============================] - 4s 4ms/step - loss: 0.2647 - accuracy: 0.8914 - val_loss: 0.5594 - val_accuracy: 0.7951\n",
      "Epoch 7/20\n",
      "963/963 [==============================] - 5s 5ms/step - loss: 0.2372 - accuracy: 0.9005 - val_loss: 0.5522 - val_accuracy: 0.7937\n",
      "Epoch 8/20\n",
      "963/963 [==============================] - 4s 5ms/step - loss: 0.2445 - accuracy: 0.9082 - val_loss: 0.5925 - val_accuracy: 0.8008\n",
      "Epoch 9/20\n",
      "963/963 [==============================] - 7s 7ms/step - loss: 0.2218 - accuracy: 0.9163 - val_loss: 0.6052 - val_accuracy: 0.8036\n",
      "Epoch 10/20\n",
      "963/963 [==============================] - 6s 7ms/step - loss: 0.1978 - accuracy: 0.9251 - val_loss: 0.6413 - val_accuracy: 0.7922\n",
      "Epoch 11/20\n",
      "963/963 [==============================] - 5s 5ms/step - loss: 0.1787 - accuracy: 0.9320 - val_loss: 0.6759 - val_accuracy: 0.7922\n",
      "Epoch 12/20\n",
      "963/963 [==============================] - 4s 4ms/step - loss: 0.1754 - accuracy: 0.9358 - val_loss: 0.6837 - val_accuracy: 0.7961\n",
      "Epoch 13/20\n",
      "963/963 [==============================] - 5s 5ms/step - loss: 0.1574 - accuracy: 0.9400 - val_loss: 0.7374 - val_accuracy: 0.7854\n",
      "Epoch 14/20\n",
      "963/963 [==============================] - 4s 4ms/step - loss: 0.1598 - accuracy: 0.9441 - val_loss: 0.7703 - val_accuracy: 0.7880\n",
      "Epoch 15/20\n",
      "963/963 [==============================] - 4s 5ms/step - loss: 0.1465 - accuracy: 0.9493 - val_loss: 0.7920 - val_accuracy: 0.7893\n",
      "Epoch 16/20\n",
      "963/963 [==============================] - 4s 5ms/step - loss: 0.1385 - accuracy: 0.9528 - val_loss: 0.8270 - val_accuracy: 0.7854\n",
      "Epoch 17/20\n",
      "963/963 [==============================] - 5s 5ms/step - loss: 0.1364 - accuracy: 0.9550 - val_loss: 0.8678 - val_accuracy: 0.7885\n",
      "Epoch 18/20\n",
      "963/963 [==============================] - 5s 5ms/step - loss: 0.1295 - accuracy: 0.9591 - val_loss: 0.8825 - val_accuracy: 0.7990\n",
      "Epoch 19/20\n",
      "963/963 [==============================] - 4s 4ms/step - loss: 0.1469 - accuracy: 0.9623 - val_loss: 0.9195 - val_accuracy: 0.7911\n",
      "Epoch 20/20\n",
      "963/963 [==============================] - 4s 4ms/step - loss: 0.1244 - accuracy: 0.9642 - val_loss: 0.9579 - val_accuracy: 0.7893\n"
     ]
    }
   ],
   "source": [
    "history = raw_model.fit(train_ds,\n",
    "                       epochs=20,\n",
    "                       validation_data=val_ds,\n",
    "                       callbacks=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d05995ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 3s 6ms/step - loss: 1.2102 - accuracy: 0.7651\n",
      "Loss:  1.2102042436599731\n",
      "Accuracy:  0.7651041746139526\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = raw_model.evaluate(test_ds)\n",
    "\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7240d839",
   "metadata": {},
   "source": [
    "# Model of background signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "40f53bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"bg_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_2 (Dense)             (None, 512)               524800    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 2)                 1026      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 525,826\n",
      "Trainable params: 525,826\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "my_classes = ['city', 'nature']\n",
    "bg_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(1024), dtype=tf.float32,\n",
    "                          name='input_embedding'),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(len(my_classes))\n",
    "], name='bg_model')\n",
    "\n",
    "bg_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "acc05654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "963/963 [==============================] - 132s 131ms/step - loss: 0.4233 - accuracy: 0.8087 - val_loss: 0.4762 - val_accuracy: 0.7763\n",
      "Epoch 2/20\n",
      "963/963 [==============================] - 3s 4ms/step - loss: 0.3640 - accuracy: 0.8369 - val_loss: 0.4971 - val_accuracy: 0.7919\n",
      "Epoch 3/20\n",
      "963/963 [==============================] - 3s 4ms/step - loss: 0.3321 - accuracy: 0.8556 - val_loss: 0.4813 - val_accuracy: 0.7937\n",
      "Epoch 4/20\n",
      "963/963 [==============================] - 4s 4ms/step - loss: 0.3052 - accuracy: 0.8718 - val_loss: 0.5328 - val_accuracy: 0.7953\n",
      "Epoch 5/20\n",
      "963/963 [==============================] - 3s 4ms/step - loss: 0.2833 - accuracy: 0.8844 - val_loss: 0.5352 - val_accuracy: 0.7906\n",
      "Epoch 6/20\n",
      "963/963 [==============================] - 3s 4ms/step - loss: 0.2536 - accuracy: 0.8958 - val_loss: 0.5544 - val_accuracy: 0.7859\n",
      "Epoch 7/20\n",
      "963/963 [==============================] - 3s 4ms/step - loss: 0.2514 - accuracy: 0.9038 - val_loss: 0.6081 - val_accuracy: 0.7839\n",
      "Epoch 8/20\n",
      "963/963 [==============================] - 4s 4ms/step - loss: 0.2233 - accuracy: 0.9098 - val_loss: 0.6019 - val_accuracy: 0.7888\n",
      "Epoch 9/20\n",
      "963/963 [==============================] - 3s 4ms/step - loss: 0.2121 - accuracy: 0.9183 - val_loss: 0.6079 - val_accuracy: 0.7927\n",
      "Epoch 10/20\n",
      "963/963 [==============================] - 4s 4ms/step - loss: 0.1876 - accuracy: 0.9242 - val_loss: 0.6221 - val_accuracy: 0.7919\n",
      "Epoch 11/20\n",
      "963/963 [==============================] - 4s 4ms/step - loss: 0.1858 - accuracy: 0.9318 - val_loss: 0.6652 - val_accuracy: 0.7896\n",
      "Epoch 12/20\n",
      "963/963 [==============================] - 3s 4ms/step - loss: 0.1605 - accuracy: 0.9357 - val_loss: 0.6896 - val_accuracy: 0.7852\n",
      "Epoch 13/20\n",
      "963/963 [==============================] - 3s 4ms/step - loss: 0.1484 - accuracy: 0.9424 - val_loss: 0.7006 - val_accuracy: 0.7880\n",
      "Epoch 14/20\n",
      "963/963 [==============================] - 4s 4ms/step - loss: 0.1433 - accuracy: 0.9460 - val_loss: 0.7520 - val_accuracy: 0.7888\n",
      "Epoch 15/20\n",
      "963/963 [==============================] - 3s 4ms/step - loss: 0.1282 - accuracy: 0.9496 - val_loss: 0.8074 - val_accuracy: 0.7794\n",
      "Epoch 16/20\n",
      "963/963 [==============================] - 3s 4ms/step - loss: 0.1215 - accuracy: 0.9534 - val_loss: 0.8107 - val_accuracy: 0.7849\n",
      "Epoch 17/20\n",
      "963/963 [==============================] - 3s 4ms/step - loss: 0.1146 - accuracy: 0.9531 - val_loss: 0.8356 - val_accuracy: 0.7828\n",
      "Epoch 18/20\n",
      "963/963 [==============================] - 3s 4ms/step - loss: 0.1097 - accuracy: 0.9578 - val_loss: 0.8944 - val_accuracy: 0.7792\n",
      "Epoch 19/20\n",
      "963/963 [==============================] - 3s 4ms/step - loss: 0.1023 - accuracy: 0.9611 - val_loss: 0.9445 - val_accuracy: 0.7760\n",
      "Epoch 20/20\n",
      "963/963 [==============================] - 4s 4ms/step - loss: 0.0944 - accuracy: 0.9633 - val_loss: 0.9256 - val_accuracy: 0.7841\n"
     ]
    }
   ],
   "source": [
    "bg_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                    optimizer=\"adam\",\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "history = bg_model.fit(train_ds_bg,\n",
    "                       epochs=20,\n",
    "                       validation_data=val_ds_bg,\n",
    "                       callbacks=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "298a1656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 2s 6ms/step - loss: 1.1862 - accuracy: 0.7526\n",
      "Loss:  1.1862424612045288\n",
      "Accuracy:  0.7526041865348816\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = bg_model.evaluate(test_ds_bg)\n",
    "\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb5dd27",
   "metadata": {},
   "source": [
    "# Model of foreground signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9c377256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"fg_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_4 (Dense)             (None, 512)               524800    \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 2)                 1026      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 525,826\n",
      "Trainable params: 525,826\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "my_classes = ['city', 'nature']\n",
    "fg_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(1024), dtype=tf.float32,\n",
    "                          name='input_embedding'),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(len(my_classes))\n",
    "], name='fg_model')\n",
    "\n",
    "fg_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "57973000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "963/963 [==============================] - 143s 144ms/step - loss: 0.6157 - accuracy: 0.6699 - val_loss: 0.6195 - val_accuracy: 0.6536\n",
      "Epoch 2/20\n",
      "963/963 [==============================] - 5s 5ms/step - loss: 0.5807 - accuracy: 0.6900 - val_loss: 0.6192 - val_accuracy: 0.6557\n",
      "Epoch 3/20\n",
      "963/963 [==============================] - 5s 6ms/step - loss: 0.5681 - accuracy: 0.7011 - val_loss: 0.6327 - val_accuracy: 0.6654\n",
      "Epoch 4/20\n",
      "963/963 [==============================] - 5s 5ms/step - loss: 0.5564 - accuracy: 0.7100 - val_loss: 0.6284 - val_accuracy: 0.6581\n",
      "Epoch 5/20\n",
      "963/963 [==============================] - 5s 5ms/step - loss: 0.5417 - accuracy: 0.7188 - val_loss: 0.6407 - val_accuracy: 0.6607\n",
      "Epoch 6/20\n",
      "963/963 [==============================] - 5s 5ms/step - loss: 0.5277 - accuracy: 0.7296 - val_loss: 0.6312 - val_accuracy: 0.6672\n",
      "Epoch 7/20\n",
      "963/963 [==============================] - 5s 6ms/step - loss: 0.5121 - accuracy: 0.7398 - val_loss: 0.6666 - val_accuracy: 0.6625\n",
      "Epoch 8/20\n",
      "963/963 [==============================] - 5s 5ms/step - loss: 0.4970 - accuracy: 0.7491 - val_loss: 0.6532 - val_accuracy: 0.6682\n",
      "Epoch 9/20\n",
      "963/963 [==============================] - 5s 5ms/step - loss: 0.4809 - accuracy: 0.7613 - val_loss: 0.6816 - val_accuracy: 0.6565\n",
      "Epoch 10/20\n",
      "963/963 [==============================] - 5s 5ms/step - loss: 0.4681 - accuracy: 0.7697 - val_loss: 0.6730 - val_accuracy: 0.6690\n",
      "Epoch 11/20\n",
      "963/963 [==============================] - 6s 6ms/step - loss: 0.4496 - accuracy: 0.7777 - val_loss: 0.7040 - val_accuracy: 0.6622\n",
      "Epoch 12/20\n",
      "963/963 [==============================] - 5s 5ms/step - loss: 0.4356 - accuracy: 0.7876 - val_loss: 0.7069 - val_accuracy: 0.6589\n",
      "Epoch 13/20\n",
      "963/963 [==============================] - 6s 7ms/step - loss: 0.4195 - accuracy: 0.8008 - val_loss: 0.7526 - val_accuracy: 0.6562\n",
      "Epoch 14/20\n",
      "963/963 [==============================] - 6s 6ms/step - loss: 0.4029 - accuracy: 0.8065 - val_loss: 0.7791 - val_accuracy: 0.6549\n",
      "Epoch 15/20\n",
      "963/963 [==============================] - 6s 6ms/step - loss: 0.3872 - accuracy: 0.8163 - val_loss: 0.8037 - val_accuracy: 0.6573\n",
      "Epoch 16/20\n",
      "963/963 [==============================] - 6s 6ms/step - loss: 0.3738 - accuracy: 0.8225 - val_loss: 0.8196 - val_accuracy: 0.6589\n",
      "Epoch 17/20\n",
      "963/963 [==============================] - 6s 6ms/step - loss: 0.3594 - accuracy: 0.8320 - val_loss: 0.8991 - val_accuracy: 0.6570\n",
      "Epoch 18/20\n",
      "963/963 [==============================] - 6s 6ms/step - loss: 0.3414 - accuracy: 0.8416 - val_loss: 0.8859 - val_accuracy: 0.6456\n",
      "Epoch 19/20\n",
      "963/963 [==============================] - 5s 5ms/step - loss: 0.3304 - accuracy: 0.8476 - val_loss: 0.9346 - val_accuracy: 0.6604\n",
      "Epoch 20/20\n",
      "963/963 [==============================] - 5s 5ms/step - loss: 0.3189 - accuracy: 0.8554 - val_loss: 0.9628 - val_accuracy: 0.6513\n"
     ]
    }
   ],
   "source": [
    "fg_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                    optimizer=\"adam\",\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "history = fg_model.fit(train_ds_fg,\n",
    "                       epochs=20,\n",
    "                       validation_data=val_ds_fg,\n",
    "                       callbacks=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9fc48ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 2s 8ms/step - loss: 1.0114 - accuracy: 0.6523\n",
      "Loss:  1.0114282369613647\n",
      "Accuracy:  0.65234375\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = fg_model.evaluate(test_ds_fg)\n",
    "\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c936b5",
   "metadata": {},
   "source": [
    "# Model of 3 signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "63557027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"all3_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 1024)              3146752   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 512)               524800    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 1026      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,672,578\n",
      "Trainable params: 3,672,578\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "my_classes = ['city', 'nature']\n",
    "tf.keras.backend.clear_session()\n",
    "all3_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(3072), dtype=tf.float32,\n",
    "                          name='input_embedding'),\n",
    "    tf.keras.layers.Dense(1024, activation='relu'),\n",
    "#     tf.keras.layers.Dense(1024, activation='relu'),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(len(my_classes))\n",
    "], name='all3_model')\n",
    "\n",
    "all3_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f4d329d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "963/963 [==============================] - 204s 204ms/step - loss: 0.4313 - accuracy: 0.8043 - val_loss: 0.5450 - val_accuracy: 0.7888\n",
      "Epoch 2/20\n",
      "963/963 [==============================] - 17s 18ms/step - loss: 0.3658 - accuracy: 0.8373 - val_loss: 0.5898 - val_accuracy: 0.7812\n",
      "Epoch 3/20\n",
      "963/963 [==============================] - 17s 17ms/step - loss: 0.3193 - accuracy: 0.8581 - val_loss: 0.6143 - val_accuracy: 0.7724\n",
      "Epoch 4/20\n",
      "963/963 [==============================] - 17s 18ms/step - loss: 0.2746 - accuracy: 0.8808 - val_loss: 0.6786 - val_accuracy: 0.7784\n",
      "Epoch 5/20\n",
      "963/963 [==============================] - 17s 18ms/step - loss: 0.2393 - accuracy: 0.8967 - val_loss: 0.7757 - val_accuracy: 0.7628\n",
      "Epoch 6/20\n",
      "963/963 [==============================] - 17s 18ms/step - loss: 0.2145 - accuracy: 0.9101 - val_loss: 0.8834 - val_accuracy: 0.7672\n",
      "Epoch 7/20\n",
      "963/963 [==============================] - 17s 18ms/step - loss: 0.1908 - accuracy: 0.9183 - val_loss: 0.8495 - val_accuracy: 0.7794\n",
      "Epoch 8/20\n",
      "963/963 [==============================] - 17s 18ms/step - loss: 0.1678 - accuracy: 0.9301 - val_loss: 0.9893 - val_accuracy: 0.7919\n",
      "Epoch 9/20\n",
      "963/963 [==============================] - 17s 18ms/step - loss: 0.1569 - accuracy: 0.9347 - val_loss: 1.0380 - val_accuracy: 0.7799\n",
      "Epoch 10/20\n",
      "963/963 [==============================] - 17s 18ms/step - loss: 0.1446 - accuracy: 0.9408 - val_loss: 1.0553 - val_accuracy: 0.7742\n",
      "Epoch 11/20\n",
      "963/963 [==============================] - 17s 18ms/step - loss: 0.1286 - accuracy: 0.9460 - val_loss: 1.2619 - val_accuracy: 0.7792\n",
      "Epoch 12/20\n",
      "963/963 [==============================] - 17s 18ms/step - loss: 0.1257 - accuracy: 0.9508 - val_loss: 1.1238 - val_accuracy: 0.7747\n",
      "Epoch 13/20\n",
      "963/963 [==============================] - 17s 18ms/step - loss: 0.1188 - accuracy: 0.9524 - val_loss: 1.3162 - val_accuracy: 0.7758\n",
      "Epoch 14/20\n",
      "963/963 [==============================] - 17s 17ms/step - loss: 0.1082 - accuracy: 0.9567 - val_loss: 1.0470 - val_accuracy: 0.7862\n",
      "Epoch 15/20\n",
      "963/963 [==============================] - 17s 18ms/step - loss: 0.1015 - accuracy: 0.9589 - val_loss: 1.2681 - val_accuracy: 0.7628\n",
      "Epoch 16/20\n",
      "963/963 [==============================] - 17s 18ms/step - loss: 0.0951 - accuracy: 0.9618 - val_loss: 1.4499 - val_accuracy: 0.7760\n",
      "Epoch 17/20\n",
      "963/963 [==============================] - 17s 18ms/step - loss: 0.0969 - accuracy: 0.9626 - val_loss: 1.3489 - val_accuracy: 0.7732\n",
      "Epoch 18/20\n",
      "963/963 [==============================] - 17s 18ms/step - loss: 0.0978 - accuracy: 0.9622 - val_loss: 1.3293 - val_accuracy: 0.7820\n",
      "Epoch 19/20\n",
      "963/963 [==============================] - 17s 18ms/step - loss: 0.0859 - accuracy: 0.9659 - val_loss: 1.3628 - val_accuracy: 0.7917\n",
      "Epoch 20/20\n",
      "963/963 [==============================] - 17s 18ms/step - loss: 0.0868 - accuracy: 0.9646 - val_loss: 1.4927 - val_accuracy: 0.7781\n"
     ]
    }
   ],
   "source": [
    "all3_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                    optimizer=\"adam\",\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "# callback will be used in the other models below too\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss',\n",
    "                                            patience=3,\n",
    "                                            restore_best_weights=True)\n",
    "\n",
    "\n",
    "history = all3_model.fit(train_ds_3,\n",
    "                       epochs=20,\n",
    "                       validation_data=val_ds_3,\n",
    "                       callbacks=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "06e19c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 3s 7ms/step - loss: 1.7952 - accuracy: 0.7487\n",
      "Loss:  1.7951960563659668\n",
      "Accuracy:  0.7486979365348816\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = all3_model.evaluate(test_ds_3)\n",
    "\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67276947",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
