{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Lf9wooRbilYQ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-15 15:06:03.502859: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as python_random\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras import regularizers\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "2Fne5qV1iR5v"
   },
   "outputs": [],
   "source": [
    "aggregated_data = pickle.load( open( '../data/processed/20230304/aggregated_data', 'rb' ) )\n",
    "\n",
    "[train_mps_raw_pca, valid_mps_raw_pca, test_mps_raw_pca,\n",
    "    train_mps_bg_pca, valid_mps_bg_pca, test_mps_bg_pca,\n",
    "    train_mps_fg_pca, valid_mps_fg_pca, test_mps_fg_pca,\n",
    "    train_indices_raw_pca, valid_indices_raw_pca, test_indices_raw_pca,\n",
    "    train_indices_bg_pca, valid_indices_bg_pca, test_indices_bg_pca,\n",
    "    train_indices_fg_pca, valid_indices_fg_pca, test_indices_fg_pca,\n",
    "    train_embedding_raw_pca, valid_embedding_raw_pca, test_embedding_raw_pca,\n",
    "    train_embedding_bg_pca, valid_embedding_bg_pca, test_embedding_bg_pca,\n",
    "    train_embedding_fg_pca, valid_embedding_fg_pca, test_embedding_fg_pca,\n",
    "    train_vgg_raw_pca, valid_vgg_raw_pca, test_vgg_raw_pca,\n",
    "    train_vgg_bg_pca, valid_vgg_bg_pca, test_vgg_bg_pca,\n",
    "    train_vgg_fg_pca, valid_vgg_fg_pca, test_vgg_fg_pca,\n",
    "    train_panns_clip_raw_pca, valid_panns_clip_raw_pca, test_panns_clip_raw_pca,\n",
    "    train_panns_clip_bg_pca, valid_panns_clip_bg_pca, test_panns_clip_bg_pca,\n",
    "    train_panns_clip_fg_pca, valid_panns_clip_fg_pca, test_panns_clip_fg_pca,\n",
    "    train_panns_embedding_raw_pca, valid_panns_embedding_raw_pca, test_panns_embedding_raw_pca,\n",
    "    train_panns_embedding_bg_pca, valid_panns_embedding_bg_pca, test_panns_embedding_bg_pca,\n",
    "    train_panns_embedding_fg_pca, valid_panns_embedding_fg_pca, test_panns_embedding_fg_pca,\n",
    "    y_train, y_valid, y_test] = aggregated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "hYt1UQ3kiyYQ"
   },
   "outputs": [],
   "source": [
    "# # YAMNet\n",
    "# train_data_raw = np.concatenate((train_mps_raw_pca, train_indices_raw_pca, train_embedding_raw_pca, train_vgg_raw_pca), axis = 1)\n",
    "# valid_data_raw = np.concatenate((valid_mps_raw_pca, valid_indices_raw_pca, valid_embedding_raw_pca, valid_vgg_raw_pca), axis = 1)\n",
    "# test_data_raw = np.concatenate((test_mps_raw_pca, test_indices_raw_pca, test_embedding_raw_pca, test_vgg_raw_pca), axis = 1)\n",
    "\n",
    "# train_data_bg = np.concatenate((train_mps_bg_pca, train_indices_bg_pca, train_embedding_bg_pca, train_vgg_bg_pca), axis = 1)\n",
    "# valid_data_bg = np.concatenate((valid_mps_bg_pca, valid_indices_bg_pca, valid_embedding_bg_pca, valid_vgg_bg_pca), axis = 1)\n",
    "# test_data_bg = np.concatenate((test_mps_bg_pca, test_indices_bg_pca, test_embedding_bg_pca, test_vgg_bg_pca), axis = 1)\n",
    "\n",
    "# train_data_fg = np.concatenate((train_mps_fg_pca, train_indices_fg_pca, train_embedding_fg_pca, train_vgg_fg_pca), axis = 1)\n",
    "# valid_data_fg = np.concatenate((valid_mps_fg_pca, valid_indices_fg_pca, valid_embedding_fg_pca, valid_vgg_fg_pca), axis = 1)\n",
    "# test_data_fg = np.concatenate((test_mps_fg_pca, test_indices_fg_pca, test_embedding_fg_pca, test_vgg_fg_pca), axis = 1)\n",
    "\n",
    "# train_data_all = np.concatenate((train_data_raw, train_data_bg, train_data_fg), axis = 1)\n",
    "# valid_data_all = np.concatenate((valid_data_raw, valid_data_bg, valid_data_fg), axis = 1)\n",
    "# test_data_all = np.concatenate((test_data_raw, test_data_bg, test_data_fg), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # VGG\n",
    "# train_data_raw = np.concatenate((train_mps_raw_pca, train_indices_raw_pca, train_vgg_raw_pca), axis = 1)\n",
    "# valid_data_raw = np.concatenate((valid_mps_raw_pca, valid_indices_raw_pca, valid_vgg_raw_pca), axis = 1)\n",
    "# test_data_raw = np.concatenate((test_mps_raw_pca, test_indices_raw_pca, test_vgg_raw_pca), axis = 1)\n",
    "\n",
    "# train_data_bg = np.concatenate((train_mps_bg_pca, train_indices_bg_pca, train_vgg_bg_pca), axis = 1)\n",
    "# valid_data_bg = np.concatenate((valid_mps_bg_pca, valid_indices_bg_pca, valid_vgg_bg_pca), axis = 1)\n",
    "# test_data_bg = np.concatenate((test_mps_bg_pca, test_indices_bg_pca, test_vgg_bg_pca), axis = 1)\n",
    "\n",
    "# train_data_fg = np.concatenate((train_mps_fg_pca, train_indices_fg_pca, train_vgg_fg_pca), axis = 1)\n",
    "# valid_data_fg = np.concatenate((valid_mps_fg_pca, valid_indices_fg_pca, valid_vgg_fg_pca), axis = 1)\n",
    "# test_data_fg = np.concatenate((test_mps_fg_pca, test_indices_fg_pca, test_vgg_fg_pca), axis = 1)\n",
    "\n",
    "# train_data_all = np.concatenate((train_data_raw, train_data_bg, train_data_fg), axis = 1)\n",
    "# valid_data_all = np.concatenate((valid_data_raw, valid_data_bg, valid_data_fg), axis = 1)\n",
    "# test_data_all = np.concatenate((test_data_raw, test_data_bg, test_data_fg), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # including panns embeddings\n",
    "# train_data_raw = np.concatenate((train_mps_raw_pca, train_indices_raw_pca, train_panns_embedding_raw_pca), axis = 1)\n",
    "# valid_data_raw = np.concatenate((valid_mps_raw_pca, valid_indices_raw_pca, valid_panns_embedding_raw_pca), axis = 1)\n",
    "# test_data_raw = np.concatenate((test_mps_raw_pca, test_indices_raw_pca, test_panns_embedding_raw_pca), axis = 1)\n",
    "\n",
    "# train_data_bg = np.concatenate((train_mps_bg_pca, train_indices_bg_pca, train_panns_embedding_bg_pca), axis = 1)\n",
    "# valid_data_bg = np.concatenate((valid_mps_bg_pca, valid_indices_bg_pca, valid_panns_embedding_bg_pca), axis = 1)\n",
    "# test_data_bg = np.concatenate((test_mps_bg_pca, test_indices_bg_pca, test_panns_embedding_bg_pca), axis = 1)\n",
    "\n",
    "# train_data_fg = np.concatenate((train_mps_fg_pca, train_indices_fg_pca, train_panns_embedding_fg_pca), axis = 1)\n",
    "# valid_data_fg = np.concatenate((valid_mps_fg_pca, valid_indices_fg_pca, valid_panns_embedding_fg_pca), axis = 1)\n",
    "# test_data_fg = np.concatenate((test_mps_fg_pca, test_indices_fg_pca, test_panns_embedding_fg_pca), axis = 1)\n",
    "\n",
    "# train_data_all = np.concatenate((train_data_raw, train_data_bg, train_data_fg), axis = 1)\n",
    "# valid_data_all = np.concatenate((valid_data_raw, valid_data_bg, valid_data_fg), axis = 1)\n",
    "# test_data_all = np.concatenate((test_data_raw, test_data_bg, test_data_fg), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lazy predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lazypredict.Supervised import LazyClassifier\n",
    "# clf = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)\n",
    "# models,predictions = clf.fit(train_vgg_bg_pca, valid_vgg_bg_pca, y_train, y_valid)\n",
    "# models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XdHjNS8fnvej"
   },
   "source": [
    "# Tensorflow parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "NAbJ5gLwnutk"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import keras_tuner as kt\n",
    "\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "TNtZTp_wn6Zc"
   },
   "outputs": [],
   "source": [
    "def build_model_narrow(hp):\n",
    "\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Input(shape=(train_data_raw.shape[1]), dtype=tf.float32))\n",
    "    drop_rate = hp.Float(\"dr\", min_value=0.2, max_value=0.5, sampling=\"linear\")\n",
    "#         if hp.Boolean(\"dropout\"):\n",
    "    model.add(layers.Dropout(rate=drop_rate))\n",
    "    # Tune the number of layers.\n",
    "    for i in range(hp.Int(\"num_layers\", 1, 10)):\n",
    "        model.add(\n",
    "            layers.Dense(\n",
    "                # Tune number of units separately.\n",
    "                units=hp.Int(f\"units_{i}\", min_value=10, max_value=200, step=10),\n",
    "                activation=hp.Choice(\"activation\", [\"relu\", \"tanh\"]),\n",
    "            )\n",
    "        )\n",
    "        model.add(layers.Dropout(rate=drop_rate))\n",
    "    model.add(layers.Dense(2))\n",
    "    learning_rate = hp.Float(\"lr\", min_value=1e-4, max_value=1e-2, sampling=\"log\")\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "def build_model_wide(hp):\n",
    "\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Input(shape=(train_data_all.shape[1]), dtype=tf.float32))\n",
    "    drop_rate = hp.Float(\"dr\", min_value=0.2, max_value=0.5, sampling=\"linear\")\n",
    "#         if hp.Boolean(\"dropout\"):\n",
    "    model.add(layers.Dropout(rate=drop_rate))\n",
    "    # Tune the number of layers.\n",
    "    for i in range(hp.Int(\"num_layers\", 1, 10)):\n",
    "        model.add(\n",
    "            layers.Dense(\n",
    "                # Tune number of units separately.\n",
    "                units=hp.Int(f\"units_{i}\", min_value=10, max_value=500, step=50),\n",
    "                activation=hp.Choice(\"activation\", [\"relu\", \"tanh\"]),\n",
    "            )\n",
    "        )\n",
    "        model.add(layers.Dropout(rate=drop_rate))\n",
    "    model.add(layers.Dense(2))\n",
    "    learning_rate = hp.Float(\"lr\", min_value=1e-4, max_value=1e-2, sampling=\"log\")\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_AUC_narrow(hp):\n",
    "\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Input(shape=(train_data_raw.shape[1]), dtype=tf.float32))\n",
    "    drop_rate = hp.Float(\"dr\", min_value=0.0, max_value=0.5, sampling=\"linear\")\n",
    "#         if hp.Boolean(\"dropout\"):\n",
    "    model.add(layers.Dropout(rate=drop_rate))\n",
    "    # Tune the number of layers.\n",
    "    for i in range(hp.Int(\"num_layers\", 1, 5)):\n",
    "        model.add(\n",
    "            layers.Dense(\n",
    "                # Tune number of units separately.\n",
    "                units=hp.Int(f\"units_{i}\", min_value=16, max_value=256, step=16),\n",
    "                activation=hp.Choice(\"activation\", [\"relu\", \"tanh\"]),\n",
    "            )\n",
    "        )\n",
    "        model.add(layers.Dropout(rate=drop_rate))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    learning_rate = hp.Float(\"lr\", min_value=1e-4, max_value=1e-2, sampling=\"log\")\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "#         loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[tf.keras.metrics.AUC(name='AUC')],\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "def build_model_AUC_wide(hp):\n",
    "\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Input(shape=(train_data_all.shape[1]), dtype=tf.float32))\n",
    "    drop_rate = hp.Float(\"dr\", min_value=0.0, max_value=0.5, sampling=\"linear\")\n",
    "#         if hp.Boolean(\"dropout\"):\n",
    "    model.add(layers.Dropout(rate=drop_rate))\n",
    "    # Tune the number of layers.\n",
    "    for i in range(hp.Int(\"num_layers\", 1, 5)):\n",
    "        model.add(\n",
    "            layers.Dense(\n",
    "                # Tune number of units separately.\n",
    "                units=hp.Int(f\"units_{i}\", min_value=16, max_value=512, step=16),\n",
    "                activation=hp.Choice(\"activation\", [\"relu\", \"tanh\"]),\n",
    "            )\n",
    "        )\n",
    "        model.add(layers.Dropout(rate=drop_rate))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    learning_rate = hp.Float(\"lr\", min_value=1e-4, max_value=1e-2, sampling=\"log\")\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "        metrics=[tf.keras.metrics.AUC(name='AUC')],\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_bACC_narrow(hp):\n",
    "\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Input(shape=(train_data_raw.shape[1]), dtype=tf.float32))\n",
    "    drop_rate = hp.Float(\"dr\", min_value=0.0, max_value=0.5, sampling=\"linear\")\n",
    "    model.add(layers.Dropout(rate=drop_rate))\n",
    "    # Tune the number of layers.\n",
    "    for i in range(hp.Int(\"num_layers\", 1, 5)):\n",
    "        model.add(\n",
    "            layers.Dense(\n",
    "                # Tune number of units separately.\n",
    "                units=hp.Int(f\"units_{i}\", min_value=16, max_value=256, step=16),\n",
    "                activation=hp.Choice(\"activation\", [\"relu\", \"tanh\"]),\n",
    "            )\n",
    "        )\n",
    "        model.add(layers.Dropout(rate=drop_rate))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    learning_rate = hp.Float(\"lr\", min_value=1e-4, max_value=1e-2, sampling=\"log\")\n",
    "    threshold = hp.Float(\"threshold\", min_value=0.3, max_value=0.7, sampling=\"linear\")\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "        metrics=[keras.metrics.BinaryAccuracy(name='binary_accuracy', threshold=threshold)],\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "def build_model_bACC_wide(hp):\n",
    "\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Input(shape=(train_data_all.shape[1]), dtype=tf.float32))\n",
    "    drop_rate = hp.Float(\"dr\", min_value=0.0, max_value=0.5, sampling=\"linear\")\n",
    "    model.add(layers.Dropout(rate=drop_rate))\n",
    "    # Tune the number of layers.\n",
    "    for i in range(hp.Int(\"num_layers\", 1, 5)):\n",
    "        model.add(\n",
    "            layers.Dense(\n",
    "                # Tune number of units separately.\n",
    "                units=hp.Int(f\"units_{i}\", min_value=16, max_value=512, step=16),\n",
    "                activation=hp.Choice(\"activation\", [\"relu\", \"tanh\"]),\n",
    "            )\n",
    "        )\n",
    "        model.add(layers.Dropout(rate=drop_rate))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    learning_rate = hp.Float(\"lr\", min_value=1e-4, max_value=1e-2, sampling=\"log\")\n",
    "    threshold = hp.Float(\"threshold\", min_value=0.3, max_value=0.7, sampling=\"linear\")\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "        metrics=[keras.metrics.BinaryAccuracy(name='binary_accuracy', threshold=threshold)],\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "x_Gu5t5w5BkJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Tuner from ../model/BinaryCrossentropy_bACC/kt_Bayesian_raw_0315_pannsEmbedding/tuner0.json\n",
      "INFO:tensorflow:Reloading Tuner from ../model/BinaryCrossentropy_bACC/kt_Bayesian_bg_0315_pannsEmbedding/tuner0.json\n",
      "INFO:tensorflow:Reloading Tuner from ../model/BinaryCrossentropy_bACC/kt_Bayesian_fg_0315_pannsEmbedding/tuner0.json\n",
      "INFO:tensorflow:Reloading Tuner from ../model/BinaryCrossentropy_bACC/kt_Bayesian_all_0315_pannsEmbedding/tuner0.json\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "# panns\n",
    "\n",
    "directory = '../model/BinaryCrossentropy_bACC'\n",
    "feat_name = '0315_pannsEmbedding'\n",
    "objective = 'val_binary_accuracy'\n",
    "\n",
    "\n",
    "# including panns embeddings\n",
    "train_data_raw = np.concatenate((train_mps_raw_pca, train_indices_raw_pca, train_panns_embedding_raw_pca), axis = 1)\n",
    "valid_data_raw = np.concatenate((valid_mps_raw_pca, valid_indices_raw_pca, valid_panns_embedding_raw_pca), axis = 1)\n",
    "test_data_raw = np.concatenate((test_mps_raw_pca, test_indices_raw_pca, test_panns_embedding_raw_pca), axis = 1)\n",
    "\n",
    "train_data_bg = np.concatenate((train_mps_bg_pca, train_indices_bg_pca, train_panns_embedding_bg_pca), axis = 1)\n",
    "valid_data_bg = np.concatenate((valid_mps_bg_pca, valid_indices_bg_pca, valid_panns_embedding_bg_pca), axis = 1)\n",
    "test_data_bg = np.concatenate((test_mps_bg_pca, test_indices_bg_pca, test_panns_embedding_bg_pca), axis = 1)\n",
    "\n",
    "train_data_fg = np.concatenate((train_mps_fg_pca, train_indices_fg_pca, train_panns_embedding_fg_pca), axis = 1)\n",
    "valid_data_fg = np.concatenate((valid_mps_fg_pca, valid_indices_fg_pca, valid_panns_embedding_fg_pca), axis = 1)\n",
    "test_data_fg = np.concatenate((test_mps_fg_pca, test_indices_fg_pca, test_panns_embedding_fg_pca), axis = 1)\n",
    "\n",
    "train_data_all = np.concatenate((train_data_raw, train_data_bg, train_data_fg), axis = 1)\n",
    "valid_data_all = np.concatenate((valid_data_raw, valid_data_bg, valid_data_fg), axis = 1)\n",
    "test_data_all = np.concatenate((test_data_raw, test_data_bg, test_data_fg), axis = 1)\n",
    "\n",
    "\n",
    "tuner_raw = kt.BayesianOptimization(build_model_bACC_narrow,\n",
    "                     objective=objective,\n",
    "                     max_trials=100,\n",
    "                     num_initial_points=25,\n",
    "                     seed=23,\n",
    "                     directory=directory,\n",
    "                     project_name='kt_Bayesian_raw_'+feat_name)\n",
    "\n",
    "tuner_bg = kt.BayesianOptimization(build_model_bACC_narrow,\n",
    "                     objective=objective,\n",
    "                     max_trials=100,\n",
    "                     num_initial_points=25,\n",
    "                     seed=23,\n",
    "                     directory=directory,\n",
    "                     project_name='kt_Bayesian_bg_'+feat_name)\n",
    "\n",
    "tuner_fg = kt.BayesianOptimization(build_model_bACC_narrow,\n",
    "                     objective=objective,\n",
    "                     max_trials=100,\n",
    "                     num_initial_points=25,\n",
    "                     seed=23,\n",
    "                     directory=directory,\n",
    "                     project_name='kt_Bayesian_fg_'+feat_name)\n",
    "\n",
    "tuner_all = kt.BayesianOptimization(build_model_bACC_wide,\n",
    "                     objective=objective,\n",
    "                     max_trials=100,\n",
    "                     num_initial_points=25,\n",
    "                     seed=23,\n",
    "                     directory=directory,\n",
    "                     project_name='kt_Bayesian_all_'+feat_name)\n",
    "\n",
    "\n",
    "tuner_all.search(train_data_all, y_train, epochs=50, validation_data=(valid_data_all, y_valid), callbacks=[stop_early], use_multiprocessing=True)\n",
    "tuner_bg.search(train_data_bg, y_train, epochs=50, validation_data=(valid_data_bg, y_valid), callbacks=[stop_early], use_multiprocessing=True)\n",
    "tuner_raw.search(train_data_raw, y_train, epochs=50, validation_data=(valid_data_raw, y_valid), callbacks=[stop_early], use_multiprocessing=True)\n",
    "tuner_fg.search(train_data_fg, y_train, epochs=50, validation_data=(valid_data_fg, y_valid), callbacks=[stop_early], use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 100 Complete [00h 00m 08s]\n",
      "val_binary_accuracy: 0.6586183905601501\n",
      "\n",
      "Best val_binary_accuracy So Far: 0.6753856539726257\n",
      "Total elapsed time: 00h 25m 32s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "# YAMNet\n",
    "\n",
    "directory = '../model/BinaryCrossentropy_bACC'\n",
    "feat_name = '0315_YAMNetEmbedding'\n",
    "objective = 'val_binary_accuracy'\n",
    "\n",
    "\n",
    "# YAMNet\n",
    "train_data_raw = np.concatenate((train_mps_raw_pca, train_indices_raw_pca, train_embedding_raw_pca, train_vgg_raw_pca), axis = 1)\n",
    "valid_data_raw = np.concatenate((valid_mps_raw_pca, valid_indices_raw_pca, valid_embedding_raw_pca, valid_vgg_raw_pca), axis = 1)\n",
    "test_data_raw = np.concatenate((test_mps_raw_pca, test_indices_raw_pca, test_embedding_raw_pca, test_vgg_raw_pca), axis = 1)\n",
    "\n",
    "train_data_bg = np.concatenate((train_mps_bg_pca, train_indices_bg_pca, train_embedding_bg_pca, train_vgg_bg_pca), axis = 1)\n",
    "valid_data_bg = np.concatenate((valid_mps_bg_pca, valid_indices_bg_pca, valid_embedding_bg_pca, valid_vgg_bg_pca), axis = 1)\n",
    "test_data_bg = np.concatenate((test_mps_bg_pca, test_indices_bg_pca, test_embedding_bg_pca, test_vgg_bg_pca), axis = 1)\n",
    "\n",
    "train_data_fg = np.concatenate((train_mps_fg_pca, train_indices_fg_pca, train_embedding_fg_pca, train_vgg_fg_pca), axis = 1)\n",
    "valid_data_fg = np.concatenate((valid_mps_fg_pca, valid_indices_fg_pca, valid_embedding_fg_pca, valid_vgg_fg_pca), axis = 1)\n",
    "test_data_fg = np.concatenate((test_mps_fg_pca, test_indices_fg_pca, test_embedding_fg_pca, test_vgg_fg_pca), axis = 1)\n",
    "\n",
    "train_data_all = np.concatenate((train_data_raw, train_data_bg, train_data_fg), axis = 1)\n",
    "valid_data_all = np.concatenate((valid_data_raw, valid_data_bg, valid_data_fg), axis = 1)\n",
    "test_data_all = np.concatenate((test_data_raw, test_data_bg, test_data_fg), axis = 1)\n",
    "\n",
    "\n",
    "tuner_raw = kt.BayesianOptimization(build_model_bACC_narrow,\n",
    "                     objective=objective,\n",
    "                     max_trials=100,\n",
    "                     num_initial_points=25,\n",
    "                     seed=23,\n",
    "                     directory=directory,\n",
    "                     project_name='kt_Bayesian_raw_'+feat_name)\n",
    "\n",
    "tuner_bg = kt.BayesianOptimization(build_model_bACC_narrow,\n",
    "                     objective=objective,\n",
    "                     max_trials=100,\n",
    "                     num_initial_points=25,\n",
    "                     seed=23,\n",
    "                     directory=directory,\n",
    "                     project_name='kt_Bayesian_bg_'+feat_name)\n",
    "\n",
    "tuner_fg = kt.BayesianOptimization(build_model_bACC_narrow,\n",
    "                     objective=objective,\n",
    "                     max_trials=100,\n",
    "                     num_initial_points=25,\n",
    "                     seed=23,\n",
    "                     directory=directory,\n",
    "                     project_name='kt_Bayesian_fg_'+feat_name)\n",
    "\n",
    "tuner_all = kt.BayesianOptimization(build_model_bACC_wide,\n",
    "                     objective=objective,\n",
    "                     max_trials=100,\n",
    "                     num_initial_points=25,\n",
    "                     seed=23,\n",
    "                     directory=directory,\n",
    "                     project_name='kt_Bayesian_all_'+feat_name)\n",
    "\n",
    "tuner_all.search(train_data_all, y_train, epochs=50, validation_data=(valid_data_all, y_valid), callbacks=[stop_early], use_multiprocessing=True)\n",
    "tuner_bg.search(train_data_bg, y_train, epochs=50, validation_data=(valid_data_bg, y_valid), callbacks=[stop_early], use_multiprocessing=True)\n",
    "tuner_raw.search(train_data_raw, y_train, epochs=50, validation_data=(valid_data_raw, y_valid), callbacks=[stop_early], use_multiprocessing=True)\n",
    "tuner_fg.search(train_data_fg, y_train, epochs=50, validation_data=(valid_data_fg, y_valid), callbacks=[stop_early], use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 4 Complete [00h 00m 16s]\n",
      "val_binary_accuracy: 0.6398390531539917\n",
      "\n",
      "Best val_binary_accuracy So Far: 0.7149564027786255\n",
      "Total elapsed time: 00h 00m 47s\n",
      "\n",
      "Search: Running Trial #5\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "0.48296           |0.27406           |dr\n",
      "1                 |3                 |num_layers\n",
      "160               |16                |units_0\n",
      "relu              |relu              |activation\n",
      "0.00041268        |0.0013825         |lr\n",
      "0.49779           |0.51157           |threshold\n",
      "240               |144               |units_1\n",
      "128               |96                |units_2\n",
      "128               |176               |units_3\n",
      "\n",
      "Epoch 1/50\n",
      "373/373 [==============================] - 1s 2ms/step - loss: 0.8158 - binary_accuracy: 0.5667 - val_loss: 0.6022 - val_binary_accuracy: 0.6794\n",
      "Epoch 2/50\n",
      "373/373 [==============================] - 1s 1ms/step - loss: 0.7100 - binary_accuracy: 0.6133 - val_loss: 0.5859 - val_binary_accuracy: 0.6875\n",
      "Epoch 3/50\n",
      "373/373 [==============================] - 1s 1ms/step - loss: 0.6799 - binary_accuracy: 0.6250 - val_loss: 0.5845 - val_binary_accuracy: 0.6948\n",
      "Epoch 4/50\n",
      "373/373 [==============================] - 1s 1ms/step - loss: 0.6488 - binary_accuracy: 0.6383 - val_loss: 0.5802 - val_binary_accuracy: 0.7036\n",
      "Epoch 5/50\n",
      "373/373 [==============================] - 1s 1ms/step - loss: 0.6425 - binary_accuracy: 0.6420 - val_loss: 0.5806 - val_binary_accuracy: 0.6968\n",
      "Epoch 6/50\n",
      "373/373 [==============================] - 0s 1ms/step - loss: 0.6310 - binary_accuracy: 0.6449 - val_loss: 0.5818 - val_binary_accuracy: 0.7002\n",
      "Epoch 7/50\n",
      "373/373 [==============================] - 1s 1ms/step - loss: 0.6207 - binary_accuracy: 0.6517 - val_loss: 0.5820 - val_binary_accuracy: 0.7009\n",
      "Epoch 8/50\n",
      "373/373 [==============================] - 1s 1ms/step - loss: 0.6164 - binary_accuracy: 0.6552 - val_loss: 0.5786 - val_binary_accuracy: 0.7029\n",
      "Epoch 9/50\n",
      "373/373 [==============================] - 1s 1ms/step - loss: 0.6168 - binary_accuracy: 0.6588 - val_loss: 0.5776 - val_binary_accuracy: 0.7015\n",
      "Epoch 10/50\n",
      "373/373 [==============================] - 1s 1ms/step - loss: 0.6118 - binary_accuracy: 0.6606 - val_loss: 0.5764 - val_binary_accuracy: 0.7002\n",
      "Epoch 11/50\n",
      "124/373 [========>.....................] - ETA: 0s - loss: 0.6133 - binary_accuracy: 0.6651"
     ]
    }
   ],
   "source": [
    "# VGG\n",
    "\n",
    "directory = '../model/BinaryCrossentropy_bACC'\n",
    "feat_name = '0315_vggEmbedding'\n",
    "objective = 'val_binary_accuracy'\n",
    "\n",
    "\n",
    "# VGG\n",
    "train_data_raw = np.concatenate((train_mps_raw_pca, train_indices_raw_pca, train_vgg_raw_pca), axis = 1)\n",
    "valid_data_raw = np.concatenate((valid_mps_raw_pca, valid_indices_raw_pca, valid_vgg_raw_pca), axis = 1)\n",
    "test_data_raw = np.concatenate((test_mps_raw_pca, test_indices_raw_pca, test_vgg_raw_pca), axis = 1)\n",
    "\n",
    "train_data_bg = np.concatenate((train_mps_bg_pca, train_indices_bg_pca, train_vgg_bg_pca), axis = 1)\n",
    "valid_data_bg = np.concatenate((valid_mps_bg_pca, valid_indices_bg_pca, valid_vgg_bg_pca), axis = 1)\n",
    "test_data_bg = np.concatenate((test_mps_bg_pca, test_indices_bg_pca, test_vgg_bg_pca), axis = 1)\n",
    "\n",
    "train_data_fg = np.concatenate((train_mps_fg_pca, train_indices_fg_pca, train_vgg_fg_pca), axis = 1)\n",
    "valid_data_fg = np.concatenate((valid_mps_fg_pca, valid_indices_fg_pca, valid_vgg_fg_pca), axis = 1)\n",
    "test_data_fg = np.concatenate((test_mps_fg_pca, test_indices_fg_pca, test_vgg_fg_pca), axis = 1)\n",
    "\n",
    "train_data_all = np.concatenate((train_data_raw, train_data_bg, train_data_fg), axis = 1)\n",
    "valid_data_all = np.concatenate((valid_data_raw, valid_data_bg, valid_data_fg), axis = 1)\n",
    "test_data_all = np.concatenate((test_data_raw, test_data_bg, test_data_fg), axis = 1)\n",
    "\n",
    "\n",
    "tuner_raw = kt.BayesianOptimization(build_model_bACC_narrow,\n",
    "                     objective=objective,\n",
    "                     max_trials=100,\n",
    "                     num_initial_points=25,\n",
    "                     seed=23,\n",
    "                     directory=directory,\n",
    "                     project_name='kt_Bayesian_raw_'+feat_name)\n",
    "\n",
    "tuner_bg = kt.BayesianOptimization(build_model_bACC_narrow,\n",
    "                     objective=objective,\n",
    "                     max_trials=100,\n",
    "                     num_initial_points=25,\n",
    "                     seed=23,\n",
    "                     directory=directory,\n",
    "                     project_name='kt_Bayesian_bg_'+feat_name)\n",
    "\n",
    "tuner_fg = kt.BayesianOptimization(build_model_bACC_narrow,\n",
    "                     objective=objective,\n",
    "                     max_trials=100,\n",
    "                     num_initial_points=25,\n",
    "                     seed=23,\n",
    "                     directory=directory,\n",
    "                     project_name='kt_Bayesian_fg_'+feat_name)\n",
    "\n",
    "tuner_all = kt.BayesianOptimization(build_model_bACC_wide,\n",
    "                     objective=objective,\n",
    "                     max_trials=100,\n",
    "                     num_initial_points=25,\n",
    "                     seed=23,\n",
    "                     directory=directory,\n",
    "                     project_name='kt_Bayesian_all_'+feat_name)\n",
    "\n",
    "tuner_all.search(train_data_all, y_train, epochs=50, validation_data=(valid_data_all, y_valid), callbacks=[stop_early], use_multiprocessing=True)\n",
    "tuner_bg.search(train_data_bg, y_train, epochs=50, validation_data=(valid_data_bg, y_valid), callbacks=[stop_early], use_multiprocessing=True)\n",
    "tuner_raw.search(train_data_raw, y_train, epochs=50, validation_data=(valid_data_raw, y_valid), callbacks=[stop_early], use_multiprocessing=True)\n",
    "tuner_fg.search(train_data_fg, y_train, epochs=50, validation_data=(valid_data_fg, y_valid), callbacks=[stop_early], use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1mjOQ8bdqpZp"
   },
   "outputs": [],
   "source": [
    "# stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuner_all.search(train_data_all, y_train, epochs=50, validation_data=(valid_data_all, y_valid), callbacks=[stop_early], use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "WB0ar_NJNvTo",
    "outputId": "8031003a-9996-4735-e672-507666562056",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tuner_bg.search(train_data_bg, y_train, epochs=50, validation_data=(valid_data_bg, y_valid), callbacks=[stop_early], use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7JMqwp6fqvUi",
    "outputId": "3c1079c0-1f3f-4917-8363-cb1b269c26ec"
   },
   "outputs": [],
   "source": [
    "# tuner_raw.search(train_data_raw, y_train, epochs=50, validation_data=(valid_data_raw, y_valid), callbacks=[stop_early], use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hY6yptDENxeQ"
   },
   "outputs": [],
   "source": [
    "# tuner_fg.search(train_data_fg, y_train, epochs=50, validation_data=(valid_data_fg, y_valid), callbacks=[stop_early], use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all\n",
    "tuner_YAMNet_all = kt.BayesianOptimization(build_model_AUC_wide,\n",
    "                     objective='AUC',\n",
    "                     max_trials=100,\n",
    "                     num_initial_points=25,\n",
    "                     seed=23,\n",
    "                     directory='../model/BinaryCrossentropy_valAUC',\n",
    "                     project_name='kt_Bayesian_all_0314_YAMNetEmbedding')\n",
    "\n",
    "tuner_VGG_all = kt.BayesianOptimization(build_model_AUC_wide,\n",
    "                     objective='AUC',\n",
    "                     max_trials=100,\n",
    "                     num_initial_points=25,\n",
    "                     seed=23,\n",
    "                     directory='../model/BinaryCrossentropy_valAUC',\n",
    "                     project_name='kt_Bayesian_all_0314_vggEmbedding')\n",
    "\n",
    "tuner_panns_all = kt.BayesianOptimization(build_model_AUC_wide,\n",
    "                     objective='AUC',\n",
    "                     max_trials=100,\n",
    "                     num_initial_points=25,\n",
    "                     seed=23,\n",
    "                     directory='../model/BinaryCrossentropy_valAUC',\n",
    "                     project_name='kt_Bayesian_all_0314_pannsEmbedding')\n",
    "\n",
    "# raw\n",
    "tuner_YAMNet_raw = kt.BayesianOptimization(build_model_AUC_narrow,\n",
    "                     objective='AUC',\n",
    "                     max_trials=100,\n",
    "                     num_initial_points=25,\n",
    "                     seed=23,\n",
    "                     directory='../model/BinaryCrossentropy_valAUC',\n",
    "                     project_name='kt_Bayesian_raw_0314_YAMNetEmbedding')\n",
    "\n",
    "tuner_VGG_raw = kt.BayesianOptimization(build_model_AUC_narrow,\n",
    "                     objective='AUC',\n",
    "                     max_trials=100,\n",
    "                     num_initial_points=25,\n",
    "                     seed=23,\n",
    "                     directory='../model/BinaryCrossentropy_valAUC',\n",
    "                     project_name='kt_Bayesian_raw_0314_vggEmbedding')\n",
    "\n",
    "tuner_panns_raw = kt.BayesianOptimization(build_model_AUC_narrow,\n",
    "                     objective='AUC',\n",
    "                     max_trials=100,\n",
    "                     num_initial_points=25,\n",
    "                     seed=23,\n",
    "                     directory='../model/BinaryCrossentropy_valAUC',\n",
    "                     project_name='kt_Bayesian_raw_0314_pannsEmbedding')\n",
    "\n",
    "\n",
    "# bg\n",
    "tuner_YAMNet_bg = kt.BayesianOptimization(build_model_AUC_narrow,\n",
    "                     objective='AUC',\n",
    "                     max_trials=100,\n",
    "                     num_initial_points=25,\n",
    "                     seed=23,\n",
    "                     directory='../model/BinaryCrossentropy_valAUC',\n",
    "                     project_name='kt_Bayesian_bg_0314_YAMNetEmbedding')\n",
    "\n",
    "tuner_VGG_bg = kt.BayesianOptimization(build_model_AUC_narrow,\n",
    "                     objective='AUC',\n",
    "                     max_trials=100,\n",
    "                     num_initial_points=25,\n",
    "                     seed=23,\n",
    "                     directory='../model/BinaryCrossentropy_valAUC',\n",
    "                     project_name='kt_Bayesian_bg_0314_vggEmbedding')\n",
    "\n",
    "tuner_panns_bg = kt.BayesianOptimization(build_model_AUC_narrow,\n",
    "                     objective='AUC',\n",
    "                     max_trials=100,\n",
    "                     num_initial_points=25,\n",
    "                     seed=23,\n",
    "                     directory='../model/BinaryCrossentropy_valAUC',\n",
    "                     project_name='kt_Bayesian_bg_0314_pannsEmbedding')\n",
    "\n",
    "\n",
    "# fg\n",
    "tuner_YAMNet_fg = kt.BayesianOptimization(build_model_AUC_narrow,\n",
    "                     objective='AUC',\n",
    "                     max_trials=100,\n",
    "                     num_initial_points=25,\n",
    "                     seed=23,\n",
    "                     directory='../model/BinaryCrossentropy_valAUC',\n",
    "                     project_name='kt_Bayesian_fg_0314_YAMNetEmbedding')\n",
    "\n",
    "tuner_VGG_fg = kt.BayesianOptimization(build_model_AUC_narrow,\n",
    "                     objective='AUC',\n",
    "                     max_trials=100,\n",
    "                     num_initial_points=25,\n",
    "                     seed=23,\n",
    "                     directory='../model/BinaryCrossentropy_valAUC',\n",
    "                     project_name='kt_Bayesian_fg_0314_vggEmbedding')\n",
    "\n",
    "tuner_panns_fg = kt.BayesianOptimization(build_model_AUC_narrow,\n",
    "                     objective='AUC',\n",
    "                     max_trials=100,\n",
    "                     num_initial_points=25,\n",
    "                     seed=23,\n",
    "                     directory='../model/BinaryCrossentropy_valAUC',\n",
    "                     project_name='kt_Bayesian_fg_0314_pannsEmbedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner_YAMNet_all.results_summary(1)\n",
    "print('--------------------------------------------------')\n",
    "tuner_VGG_all.results_summary(1)\n",
    "print('--------------------------------------------------')\n",
    "tuner_panns_all.results_summary(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner_YAMNet_raw.results_summary(1)\n",
    "print('--------------------------------------------------')\n",
    "tuner_VGG_raw.results_summary(1)\n",
    "print('--------------------------------------------------')\n",
    "tuner_panns_raw.results_summary(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner_YAMNet_bg.results_summary(1)\n",
    "print('--------------------------------------------------')\n",
    "tuner_VGG_bg.results_summary(1)\n",
    "print('--------------------------------------------------')\n",
    "tuner_panns_bg.results_summary(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner_YAMNet_fg.results_summary(1)\n",
    "print('--------------------------------------------------')\n",
    "tuner_VGG_fg.results_summary(1)\n",
    "print('--------------------------------------------------')\n",
    "tuner_panns_fg.results_summary(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the best parameters to build a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hps=tuner_panns_raw.get_best_hyperparameters(num_trials=1)[0]\n",
    "model = tuner_panns_raw.hypermodel.build(best_hps)\n",
    "history = model.fit(train_data_raw, y_train, epochs=50, validation_data=(valid_data_raw, y_valid), callbacks=[stop_early], use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_AUC_per_epoch = history.history['val_AUC']\n",
    "best_epoch = val_AUC_per_epoch.index(max(val_AUC_per_epoch)) + 1\n",
    "print('Best epoch: %d' % (best_epoch,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypermodel = tuner_panns_raw.hypermodel.build(best_hps)\n",
    "\n",
    "# Retrain the model\n",
    "hypermodel.fit(train_data_raw, y_train, epochs=best_epoch, validation_data=(valid_data_raw, y_valid), callbacks=[stop_early], use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result = hypermodel.evaluate(test_data_raw, y_test)\n",
    "print(\"[test loss, test accuracy]:\", eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = hypermodel.predict(test_data_raw).ravel() \n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "# plot the ROC curve\n",
    "plt.plot(fpr, tpr, label=f'AUC = {auc:.2f}')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(tpr+1-fpr)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
